{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybbA7gxygDHm",
        "outputId": "f36caf38-857c-4675-bfec-156044db110d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn mlflow pytorch_lightning pytorch_forecasting\n",
        "\n",
        "# Set up Kaggle API\n",
        "!pip install -q kaggle pytorch_forecasting pytorch_lightning dagshub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8n9w-H2gFpK",
        "outputId": "b5dbb3db-fa4c-4d7d-ba73-255f5f3fa0a7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m733.8/733.8 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload your kaggle.json to Colab and run:\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "cpCE_wwigPl6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqPcDgRCgQPC",
        "outputId": "eca31d89-d028-4049-c289-f1526b73310c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 669MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q train.csv.zip\n",
        "!unzip -q stores.csv.zip\n",
        "!unzip -q test.csv.zip\n",
        "!unzip -q features.csv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naG0PTJsgSz_",
        "outputId": "8222053b-3521-4286-e0f8-5af98cc3311c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open stores.csv.zip, stores.csv.zip.zip or stores.csv.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "# Prophet Model Experiment Notebook\n",
        "\n",
        "ეს ნოუთბუქი განკუთვნილია Walmart Store Sales Forecasting Kaggle-ის კონკურსისთვის Prophet მოდელის ექსპერიმენტებისთვის.\n",
        "იგი მოიცავს მონაცემთა ჩატვირთვას, წინასწარ დამუშავებას, Prophet მოდელის ვარჯიშს, კროს-ვალიდაციას\n",
        "და MLflow-ზე ექსპერიმენტების ლოგირებას.\n",
        "\"\"\"\n",
        "import mlflow\n",
        "# --- 1. ბიბლიოთეკების იმპორტი ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from prophet import Prophet\n",
        "from prophet.diagnostics import cross_validation, performance_metrics\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import mlflow\n",
        "import mlflow.pyfunc\n",
        "import warnings\n",
        "import dagshub\n",
        "import joblib # დავამატე joblib ბიბლიოთეკა სერიალიზაციისთვის\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"საჭირო ბიბლიოთეკები წარმატებით იმპორტირებულია.\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "საჭირო ბიბლიოთეკები წარმატებით იმპორტირებულია.\n"
          ]
        }
      ],
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Oc_i5mHef1u",
        "outputId": "074b7efa-7328-4976-cb4b-22762b4240a6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 2. MLflow კონფიგურაცია ---\n",
        "# MLflow Tracking URI-ის დაყენება. თუ იყენებთ ლოკალურ MLflow სერვერს, შეცვალეთ შესაბამისად.\n",
        "# მაგალითად: \"http://localhost:5000\"\n",
        "import dagshub\n",
        "dagshub.init(repo_owner='ekvirika', repo_name='WalmartRecruiting', mlflow=True)\n",
        "mlflow.autolog()\n",
        "\n",
        "\n",
        "print(f\"MLflow Tracking URI დაყენებულია: {mlflow.get_tracking_uri()}\")\n",
        "\n",
        "# ექსპერიმენტის სახელის დაყენება\n",
        "experiment_name = \"Prophet_Training\"\n",
        "mlflow.set_experiment(experiment_name)\n",
        "print(f\"MLflow ექსპერიმენტი დაყენებულია: {experiment_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319,
          "referenced_widgets": [
            "9ca35ad8a798492186fa18149b612d57",
            "d88cd209fd6e473fb61429496d14899b"
          ]
        },
        "id": "pqqUq0f_iNO7",
        "outputId": "baff6f3c-b2f3-478e-c6a4-89348f93222b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ca35ad8a798492186fa18149b612d57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Open the following link in your browser to authorize the client:\n",
            "https://dagshub.com/login/oauth/authorize?state=d96a3b54-4c07-4b5e-a688-57f7b864cfd2&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=b9a51f27d8eae2cc62f6856576ffbb7c85d32944f09a8ca568dd0665dcb044d1\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Accessing as ekvirika\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as ekvirika\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"ekvirika/WalmartRecruiting\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"ekvirika/WalmartRecruiting\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository ekvirika/WalmartRecruiting initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository ekvirika/WalmartRecruiting initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/07 20:19:09 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
            "2025/07/07 20:19:09 WARNING mlflow.spark: With Pyspark >= 3.2, PYSPARK_PIN_THREAD environment variable must be set to false for Spark datasource autologging to work.\n",
            "2025/07/07 20:19:09 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
            "2025/07/07 20:19:09 INFO mlflow.tracking.fluent: Experiment with name 'Prophet_Training' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLflow Tracking URI დაყენებულია: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow\n",
            "MLflow ექსპერიმენტი დაყენებულია: Prophet_Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 3. მონაცემთა ჩატვირთვა ---\n",
        "# შენიშვნა: აქ უნდა მიუთითოთ თქვენი რეალური მონაცემთა ფაილების გზები.\n",
        "# ამ მაგალითისთვის გამოყენებულია ფლეისჰოლდერები.\n",
        "try:\n",
        "    train_df = pd.read_csv(\"train.csv\")\n",
        "    test_df = pd.read_csv(\"test.csv\")\n",
        "    stores_df = pd.read_csv(\"stores.csv\")\n",
        "    features_df = pd.read_csv(\"features.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"შეცდომა: მონაცემთა ფაილები ვერ მოიძებნა. გთხოვთ, დარწმუნდით, რომ 'train.csv', 'test.csv', 'stores.csv', 'features.csv' არსებობს.\")\n",
        "    print(\"ამ ნოუთბუქის გასაშვებად საჭიროა ეს ფაილები ან ხელოვნური მონაცემების შექმნა.\")\n",
        "    exit() # გაჩერება თუ ფაილები ვერ მოიძებნა\n",
        "\n",
        "# --- 4. მონაცემთა წინასწარი დამუშავება (MLflow Run: Prophet_Cleaning) ---\n",
        "with mlflow.start_run(run_name=\"Prophet_Cleaning\") as cleaning_run:\n",
        "    print(\"მონაცემთა წინასწარი დამუშავება იწყება...\")\n",
        "\n",
        "    # თარიღების ფორმატის შეცვლა\n",
        "    train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "    test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
        "    features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
        "\n",
        "    # Extract date-related features\n",
        "    for df in [train_df, test_df]:\n",
        "        df['Year'] = df['Date'].dt.year\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Week'] = df['Date'].dt.isocalendar().week\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "\n",
        "    # Holiday flags for known holidays\n",
        "    holiday_dates = {\n",
        "        \"Super Bowl\": [\"2010-02-12\", \"2011-02-11\", \"2012-02-10\", \"2013-02-08\"],\n",
        "        \"Labor Day\": [\"2010-09-10\", \"2011-09-09\", \"2012-09-07\", \"2013-09-06\"],\n",
        "        \"Thanksgiving\": [\"2010-11-26\", \"2011-11-25\", \"2012-11-23\", \"2013-11-29\"],\n",
        "        \"Christmas\": [\"2010-12-31\", \"2011-12-30\", \"2012-12-28\", \"2013-12-27\"]\n",
        "    }\n",
        "\n",
        "\n",
        "    for holiday, dates in holiday_dates.items():\n",
        "        for df in [train_df, test_df]:\n",
        "            df[holiday] = df['Date'].isin(pd.to_datetime(dates)).astype(int)\n",
        "\n",
        "\n",
        "    # მონაცემთა გაერთიანება\n",
        "    # გაერთიანება features_df-თან\n",
        "    train_df = pd.merge(train_df, features_df, on=['Store', 'Date', 'IsHoliday'], how='left', suffixes=('_train', '_features'))\n",
        "    test_df = pd.merge(test_df, features_df, on=['Store', 'Date', 'IsHoliday'], how='left', suffixes=('_test', '_features'))\n",
        "\n",
        "    # გაერთიანება stores_df-თან\n",
        "    train_df = pd.merge(train_df, stores_df, on='Store', how='left')\n",
        "    test_df = pd.merge(test_df, stores_df, on='Store', how='left')\n",
        "\n",
        "    # Weekly_Sales-ის უარყოფითი მნიშვნელობების დამუშავება (თუ არსებობს)\n",
        "    # Prophet არ უჭერს მხარს უარყოფით მნიშვნელობებს.\n",
        "    train_df['Weekly_Sales'] = train_df['Weekly_Sales'].apply(lambda x: max(0, x))\n",
        "\n",
        "    # დაკარგული მნიშვნელობების დამუშავება\n",
        "    # MarkDown-ის სვეტები ხშირად შეიცავს NaN-ებს, რადგან აქციები არ ყოფილა.\n",
        "    # შევავსოთ 0-ებით ან სხვა შესაბამისი მნიშვნელობით.\n",
        "    markdown_cols = [col for col in train_df.columns if 'MarkDown' in col]\n",
        "    for col in markdown_cols:\n",
        "        train_df[col] = train_df[col].fillna(0)\n",
        "        test_df[col] = test_df[col].fillna(0)\n",
        "\n",
        "    # CPI, Unemployment, Temperature, Fuel_Price-ის დაკარგული მნიშვნელობების შევსება\n",
        "    # შევავსოთ წინა მნიშვნელობით ან საშუალოთი, რადგან ეს დროითი სერიების მონაცემებია.\n",
        "    for col in ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']:\n",
        "        train_df[col] = train_df[col].fillna(method='ffill').fillna(method='bfill')\n",
        "        test_df[col] = test_df[col].fillna(method='ffill').fillna(method='bfill')\n",
        "        # თუ მაინც დარჩა NaN (მაგ. სერიის დასაწყისში), შევავსოთ საშუალოთი\n",
        "        train_df[col] = train_df[col].fillna(train_df[col].mean())\n",
        "        test_df[col] = test_df[col].fillna(test_df[col].mean())\n",
        "\n",
        "    # MLflow-ში ლოგირება\n",
        "    mlflow.log_param(\"initial_train_rows\", train_df.shape[0])\n",
        "    mlflow.log_param(\"initial_test_rows\", test_df.shape[0])\n",
        "    mlflow.log_param(\"regressors_used\", ', '.join(markdown_cols + ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']))\n",
        "\n",
        "    print(\"მონაცემთა წინასწარი დამუშავება დასრულდა.\")\n",
        "    print(f\"დამუშავებული სატრენინგო მონაცემების ნიმუში:\\n{train_df.head()}\")\n",
        "    print(f\"დამუშავებული სატესტო მონაცემების ნიმუში:\\n{test_df.head()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9n36ZebiN_0",
        "outputId": "66432766-c234-41ce-9bf2-27d7b9a1e14d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "მონაცემთა წინასწარი დამუშავება იწყება...\n",
            "მონაცემთა წინასწარი დამუშავება დასრულდა.\n",
            "დამუშავებული სატრენინგო მონაცემების ნიმუში:\n",
            "   Store  Dept       Date  Weekly_Sales  IsHoliday  Year  Month  Week  \\\n",
            "0      1     1 2010-02-05      24924.50      False  2010      2     5   \n",
            "1      1     1 2010-02-12      46039.49       True  2010      2     6   \n",
            "2      1     1 2010-02-19      41595.55      False  2010      2     7   \n",
            "3      1     1 2010-02-26      19403.54      False  2010      2     8   \n",
            "4      1     1 2010-03-05      21827.90      False  2010      3     9   \n",
            "\n",
            "   DayOfWeek  Super Bowl  ...  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  \\\n",
            "0          4           0  ...       2.572        0.0        0.0        0.0   \n",
            "1          4           1  ...       2.548        0.0        0.0        0.0   \n",
            "2          4           0  ...       2.514        0.0        0.0        0.0   \n",
            "3          4           0  ...       2.561        0.0        0.0        0.0   \n",
            "4          4           0  ...       2.625        0.0        0.0        0.0   \n",
            "\n",
            "   MarkDown4  MarkDown5         CPI  Unemployment  Type    Size  \n",
            "0        0.0        0.0  211.096358         8.106     A  151315  \n",
            "1        0.0        0.0  211.242170         8.106     A  151315  \n",
            "2        0.0        0.0  211.289143         8.106     A  151315  \n",
            "3        0.0        0.0  211.319643         8.106     A  151315  \n",
            "4        0.0        0.0  211.350143         8.106     A  151315  \n",
            "\n",
            "[5 rows x 24 columns]\n",
            "დამუშავებული სატესტო მონაცემების ნიმუში:\n",
            "   Store  Dept       Date  IsHoliday  Year  Month  Week  DayOfWeek  \\\n",
            "0      1     1 2012-11-02      False  2012     11    44          4   \n",
            "1      1     1 2012-11-09      False  2012     11    45          4   \n",
            "2      1     1 2012-11-16      False  2012     11    46          4   \n",
            "3      1     1 2012-11-23       True  2012     11    47          4   \n",
            "4      1     1 2012-11-30      False  2012     11    48          4   \n",
            "\n",
            "   Super Bowl  Labor Day  ...  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  \\\n",
            "0           0          0  ...       3.386    6766.44    5147.70      50.82   \n",
            "1           0          0  ...       3.314   11421.32    3370.89      40.28   \n",
            "2           0          0  ...       3.252    9696.28     292.10     103.78   \n",
            "3           0          0  ...       3.211     883.59       4.17   74910.32   \n",
            "4           0          0  ...       3.207    2460.03       0.00    3838.35   \n",
            "\n",
            "   MarkDown4  MarkDown5         CPI  Unemployment  Type    Size  \n",
            "0    3639.90    2737.42  223.462779         6.573     A  151315  \n",
            "1    4646.79    6154.16  223.481307         6.573     A  151315  \n",
            "2    1133.15    6612.69  223.512911         6.573     A  151315  \n",
            "3     209.91     303.32  223.561947         6.573     A  151315  \n",
            "4     150.57    6966.34  223.610984         6.573     A  151315  \n",
            "\n",
            "[5 rows x 23 columns]\n",
            "🏃 View run Prophet_Cleaning at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/7978804592274e3c9093aa85d979e081\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. მოდელის ვარჯიში და კროს-ვალიდაცია ყველა Store/Dept კომბინაციისთვის ---\n",
        "# მიიღეთ ყველა უნიკალური Store-Dept კომბინაცია\n",
        "unique_store_dept_combinations = train_df[['Store', 'Dept']].drop_duplicates().values\n",
        "\n",
        "print(f\"სულ უნიკალური Store-Dept კომბინაციები: {len(unique_store_dept_combinations)}\")\n",
        "\n",
        "# დამატებითი რეგრესორების მომზადება\n",
        "regressor_cols = ['IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment'] + markdown_cols\n",
        "\n",
        "for store_id, dept_id in unique_store_dept_combinations:\n",
        "    # შექმენით მშობელი გაშვება თითოეული Store/Dept კომბინაციისთვის\n",
        "    with mlflow.start_run(run_name=f\"Prophet_Store_{store_id}_Dept_{dept_id}_Experiment\") as parent_run:\n",
        "        print(f\"\\n--- დაწყებულია ექსპერიმენტი Store {store_id}, Dept {dept_id}-ისთვის ---\")\n",
        "\n",
        "        # მონაცემების ფილტრაცია მიმდინარე Store/Dept-ისთვის\n",
        "        train_filtered = train_df[(train_df['Store'] == store_id) & (train_df['Dept'] == dept_id)].copy()\n",
        "        test_filtered = test_df[(test_df['Store'] == store_id) & (test_df['Dept'] == dept_id)].copy()\n",
        "\n",
        "        # Prophet-ისთვის საჭირო სვეტების გადარქმევა: 'ds' (თარიღი) და 'y' (სამიზნე)\n",
        "        train_prophet_full = train_filtered[['Date', 'Weekly_Sales'] + regressor_cols].rename(columns={'Date': 'ds', 'Weekly_Sales': 'y'})\n",
        "        test_prophet_full = test_filtered[['Date'] + regressor_cols].rename(columns={'Date': 'ds'})\n",
        "\n",
        "        # დარწმუნდით, რომ IsHoliday არის რიცხვითი\n",
        "        train_prophet_full['IsHoliday'] = train_prophet_full['IsHoliday'].astype(int)\n",
        "        test_prophet_full['IsHoliday'] = test_prophet_full['IsHoliday'].astype(int)\n",
        "\n",
        "        # შეამოწმეთ, არის თუ არა საკმარისი მონაცემები\n",
        "        if len(train_prophet_full) < 2: # Prophet-ს მინიმუმ 2 წერტილი სჭირდება\n",
        "            print(f\"გამოტოვება: არასაკმარისი მონაცემები Store {store_id}, Dept {dept_id}-ისთვის. (სულ {len(train_prophet_full)} ჩანაწერი)\")\n",
        "            mlflow.log_param(\"status\", \"skipped_insufficient_data\")\n",
        "            mlflow.log_param(\"num_records\", len(train_prophet_full))\n",
        "            continue # გადადით შემდეგ კომბინაციაზე\n",
        "\n",
        "        # --- 5.1. მოდელის ვარჯიში (Nested MLflow Run: Prophet_StoreX_DeptY_Training) ---\n",
        "        with mlflow.start_run(run_name=f\"Prophet_Store_{store_id}_Dept_{dept_id}_Training\", nested=True) as training_run:\n",
        "            print(f\"Prophet მოდელის ვარჯიში იწყება Store {store_id}, Dept {dept_id}-ისთვის...\")\n",
        "\n",
        "            # Prophet მოდელის ინიციალიზაცია\n",
        "            model_params = {\n",
        "                'seasonality_mode': 'multiplicative',\n",
        "                'changepoint_prior_scale': 0.05,\n",
        "                'weekly_seasonality': True,\n",
        "                'daily_seasonality': False,\n",
        "                'yearly_seasonality': True\n",
        "            }\n",
        "\n",
        "            m = Prophet(**model_params)\n",
        "\n",
        "            # დამატებითი რეგრესორების დამატება\n",
        "            for col in regressor_cols:\n",
        "                if col in train_prophet_full.columns and col in test_prophet_full.columns:\n",
        "                    m.add_regressor(col)\n",
        "                    mlflow.log_param(f\"regressor_{col}\", \"added\")\n",
        "                else:\n",
        "                    print(f\"გაფრთხილება: რეგრესორი '{col}' არ მოიძებნა ყველა მონაცემთა ნაკრებში Store {store_id}, Dept {dept_id}-ისთვის.\")\n",
        "\n",
        "            # მოდელის ვარჯიში\n",
        "            m.fit(train_prophet_full)\n",
        "\n",
        "            print(f\"Prophet მოდელის ვარჯიში დასრულდა Store {store_id}, Dept {dept_id}-ისთვის.\")\n",
        "\n",
        "            # MLflow-ში პარამეტრების და ლოგირება\n",
        "            for param, value in model_params.items():\n",
        "                mlflow.log_param(param, value)\n",
        "            mlflow.log_param(\"filtered_store_id\", store_id)\n",
        "            mlflow.log_param(\"filtered_dept_id\", dept_id)\n",
        "            mlflow.log_param(\"final_train_rows_prophet\", train_prophet_full.shape[0])\n",
        "            mlflow.log_param(\"final_test_rows_prophet\", test_prophet_full.shape[0])\n",
        "\n",
        "            # მოდელის შენახვა MLflow-ში\n",
        "            mlflow.pyfunc.log_model(\n",
        "                artifact_path=\"prophet_model\",\n",
        "                python_model=mlflow.pyfunc.PythonModel(),\n",
        "                artifacts={\"prophet_model\": m},\n",
        "                # code_path=[__file__],\n",
        "                conda_env={\n",
        "                    \"channels\": [\"defaults\", \"conda-forge\"],\n",
        "                    \"dependencies\": [\n",
        "                        \"python=3.9\", # შეცვალეთ თქვენი python ვერსიით\n",
        "                        \"pandas\",\n",
        "                        \"numpy\",\n",
        "                        \"prophet\",\n",
        "                        \"scikit-learn\",\n",
        "                        \"matplotlib\",\n",
        "                        \"plotly\",\n",
        "                        \"mlflow\"\n",
        "                    ]\n",
        "                }\n",
        "            )\n",
        "            print(f\"Prophet მოდელი შენახულია MLflow-ში Store {store_id}, Dept {dept_id}-ისთვის.\")\n",
        "\n",
        "            # პროგნოზირება სატრენინგო მონაცემებზე (ვიზუალიზაციისთვის)\n",
        "            forecast_train = m.predict(train_prophet_full[['ds'] + [col for col in regressor_cols if col in train_prophet_full.columns]])\n",
        "\n",
        "            # ვიზუალიზაცია\n",
        "            fig = m.plot(forecast_train)\n",
        "            plt.title(f\"Prophet პროგნოზი (Store {store_id}, Dept {dept_id})\")\n",
        "            plt.xlabel(\"თარიღი\")\n",
        "            plt.ylabel(\"კვირის გაყიდვები\")\n",
        "            mlflow.log_figure(fig, \"prophet_forecast_train.png\")\n",
        "            plt.close(fig)\n",
        "\n",
        "            # კომპონენტების ვიზუალიზაცია\n",
        "            fig2 = m.plot_components(forecast_train)\n",
        "            mlflow.log_figure(fig2, \"prophet_components.png\")\n",
        "            plt.close(fig2)\n",
        "\n",
        "            # --- 5.2. კროს-ვალიდაცია (Nested MLflow Run: Prophet_StoreX_DeptY_Cross_Validation) ---\n",
        "            with mlflow.start_run(run_name=f\"Prophet_Store_{store_id}_Dept_{dept_id}_Cross_Validation\", nested=True) as cv_run:\n",
        "                print(f\"Prophet კროს-ვალიდაცია იწყება Store {store_id}, Dept {dept_id}-ისთვის...\")\n",
        "\n",
        "                # კროს-ვალიდაციის პარამეტრები\n",
        "                # initial: საწყისი სატრენინგო პერიოდი\n",
        "                # period: ყოველი შემდგომი გაფართოების პერიოდი\n",
        "                # horizon: პროგნოზის ჰორიზონტი\n",
        "                # დარწმუნდით, რომ initial საკმარისად დიდია.\n",
        "                # Prophet-ის დოკუმენტაცია გვირჩევს მინიმუმ 20-ჯერ ჰორიზონტის ზომა.\n",
        "                # თუ მონაცემები მცირეა, ეს შეიძლება გამოიწვიოს შეცდომა.\n",
        "                # ვცადოთ დინამიური initial\n",
        "                min_initial_weeks = 20 # Prophet-ის რეკომენდაცია\n",
        "                initial_weeks = max(min_initial_weeks, int(len(train_prophet_full) * 0.7)) # მინიმუმ 70% ან 20 კვირა\n",
        "                cv_initial = f\"{initial_weeks} W\"\n",
        "                cv_period = \"4 W\"\n",
        "                cv_horizon = \"12 W\"\n",
        "\n",
        "                print(f\"კროს-ვალიდაციის პარამეტრები: initial={cv_initial}, period={cv_period}, horizon={cv_horizon}\")\n",
        "\n",
        "                # MLflow-ში ლოგირება\n",
        "                mlflow.log_param(\"cv_initial\", cv_initial)\n",
        "                mlflow.log_param(\"cv_period\", cv_period)\n",
        "                mlflow.log_param(\"cv_horizon\", cv_horizon)\n",
        "\n",
        "                # კროს-ვალიდაციის გაშვება\n",
        "                try:\n",
        "                    df_cv = cross_validation(\n",
        "                        m,\n",
        "                        initial=cv_initial,\n",
        "                        period=cv_period,\n",
        "                        horizon=cv_horizon,\n",
        "                        cutoffs=None,\n",
        "                        parallel=\"processes\"\n",
        "                    )\n",
        "                    print(\"კროს-ვალიდაცია დასრულდა.\")\n",
        "                    print(f\"კროს-ვალიდაციის შედეგების ნიმუში:\\n{df_cv.head()}\")\n",
        "\n",
        "                    # შესრულების მეტრიკების გამოთვლა\n",
        "                    df_p = performance_metrics(df_cv)\n",
        "                    print(f\"შესრულების მეტრიკების ნიმუში:\\n{df_p.head()}\")\n",
        "\n",
        "                    # შეფასების მეტრიკების ლოგირება MLflow-ში\n",
        "                    mlflow.log_metric(\"mae_cv_mean\", df_p['mae'].mean())\n",
        "                    mlflow.log_metric(\"rmse_cv_mean\", df_p['rmse'].mean())\n",
        "                    mlflow.log_metric(\"mape_cv_mean\", df_p['mape'].mean())\n",
        "                    mlflow.log_metric(\"smape_cv_mean\", df_p['smape'].mean())\n",
        "\n",
        "                    # ვიზუალიზაცია: შეცდომის მეტრიკები დროის მიხედვით\n",
        "                    fig_metrics = make_subplots(rows=2, cols=1, shared_xaxes=True,\n",
        "                                                subplot_titles=(\"MAE დროის მიხედვით\", \"RMSE დროის მიხედვით\"))\n",
        "                    fig_metrics.add_trace(go.Scatter(x=df_p['horizon'], y=df_p['mae'], mode='lines', name='MAE'), row=1, col=1)\n",
        "                    fig_metrics.add_trace(go.Scatter(x=df_p['horizon'], y=df_p['rmse'], mode='lines', name='RMSE'), row=2, col=1)\n",
        "                    fig_metrics.update_layout(title_text=f\"Prophet კროს-ვალიდაციის მეტრიკები (Store {store_id}, Dept {dept_id})\", height=600)\n",
        "                    mlflow.log_figure(fig_metrics, \"prophet_cv_metrics.png\")\n",
        "\n",
        "                except ValueError as e:\n",
        "                    print(f\"შეცდომა კროს-ვალიდაციისას Store {store_id}, Dept {dept_id}-ისთვის: {e}\")\n",
        "                    print(\"ეს შეიძლება მოხდეს, თუ მონაცემთა ნაკრები ძალიან მცირეა კროს-ვალიდაციისთვის მითითებული პარამეტრებით.\")\n",
        "                    mlflow.log_param(\"cv_error\", str(e))\n",
        "                except Exception as e:\n",
        "                    print(f\"მოულოდნელი შეცდომა კროს-ვალიდაციისას Store {store_id}, Dept {dept_id}-ისთვის: {e}\")\n",
        "                    mlflow.log_param(\"cv_error\", str(e))\n",
        "\n",
        "            # --- 5.3. მოდელის რეგისტრაცია (MLflow Model Registry) ---\n",
        "            # რეგისტრაციისთვის საჭიროა run_id, საიდანაც მოდელი დაილოგა.\n",
        "            try:\n",
        "                registered_model = mlflow.register_model(\n",
        "                    model_uri=f\"runs:/{training_run.info.run_id}/prophet_model\",\n",
        "                    name=f\"WalmartSalesProphetModel_Store_{store_id}_Dept_{dept_id}\"\n",
        "                )\n",
        "                print(f\"მოდელი 'WalmartSalesProphetModel_Store_{store_id}_Dept_{dept_id}' წარმატებით დარეგისტრირდა Model Registry-ში.\")\n",
        "                print(f\"ვერსია: {registered_model.version}\")\n",
        "            except Exception as e:\n",
        "                print(f\"შეცდომა მოდელის რეგისტრაციისას Store {store_id}, Dept {dept_id}-ისთვის: {e}\")\n",
        "                print(\"დარწმუნდით, რომ MLflow Tracking Server გაშვებულია და გაქვთ წვდომა Model Registry-ზე.\")\n",
        "\n",
        "print(\"\\nყველა Prophet მოდელის ექსპერიმენტი დასრულდა.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "yXHqhiNAh-H8",
        "outputId": "083cb135-1ece-44e4-ae35-f969c20464ce"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "სულ უნიკალური Store-Dept კომბინაციები: 3331\n",
            "\n",
            "--- დაწყებულია ექსპერიმენტი Store 1, Dept 1-ისთვის ---\n",
            "Prophet მოდელის ვარჯიში იწყება Store 1, Dept 1-ისთვის...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmphqh0gn0f/5tdcfd8l.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmphqh0gn0f/75gtmayj.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=82078', 'data', 'file=/tmp/tmphqh0gn0f/5tdcfd8l.json', 'init=/tmp/tmphqh0gn0f/75gtmayj.json', 'output', 'file=/tmp/tmphqh0gn0f/prophet_modelo8couty8/prophet_model-20250707203545.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "20:35:45 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "20:35:45 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prophet მოდელის ვარჯიში დასრულდა Store 1, Dept 1-ისთვის.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/07 20:35:53 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run Prophet_Store_1_Dept_1_Training at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/69b68899f52d459a9f8b89756fb7b45b\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run Prophet_Store_1_Dept_1_Experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/70e872430f584da3bd3df1c8d2602381\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RestException",
          "evalue": "INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRestException\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-26-1955765777.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# მოდელის შენახვა MLflow-ში\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             mlflow.pyfunc.log_model(\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0martifact_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"prophet_model\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mpython_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracing/provider.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                     \u001b[0mis_func_called\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/pyfunc/__init__.py\u001b[0m in \u001b[0;36mlog_model\u001b[0;34m(artifact_path, loader_module, data_path, code_paths, infer_code_paths, conda_env, python_model, artifacts, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, metadata, model_config, streamable, resources, auth_policy, prompts, name, params, tags, model_type, step, model_id)\u001b[0m\n\u001b[1;32m   3577\u001b[0m         \u001b[0mmetadata\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlogged\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3578\u001b[0m     \"\"\"\n\u001b[0;32m-> 3579\u001b[0;31m     return Model.log(\n\u001b[0m\u001b[1;32m   3580\u001b[0m         \u001b[0martifact_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0martifact_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3581\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/models/model.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, run_id, resources, auth_policy, prompts, name, model_type, params, tags, step, model_id, **kwargs)\u001b[0m\n\u001b[1;32m   1159\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_id\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                 }\n\u001b[0;32m-> 1161\u001b[0;31m                 model = mlflow.initialize_logged_model(\n\u001b[0m\u001b[1;32m   1162\u001b[0m                     \u001b[0;31m# TODO: Update model name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracking/fluent.py\u001b[0m in \u001b[0;36minitialize_logged_model\u001b[0;34m(name, source_run_id, tags, params, model_type, experiment_id)\u001b[0m\n\u001b[1;32m   2128\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mnew\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoggedModel\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPENDING\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m     \"\"\"\n\u001b[0;32m-> 2130\u001b[0;31m     model = _create_logged_model(\n\u001b[0m\u001b[1;32m   2131\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m         \u001b[0msource_run_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource_run_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracking/fluent.py\u001b[0m in \u001b[0;36m_create_logged_model\u001b[0;34m(name, source_run_id, tags, params, model_type, experiment_id)\u001b[0m\n\u001b[1;32m   2255\u001b[0m         )\n\u001b[1;32m   2256\u001b[0m     \u001b[0mresolved_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2257\u001b[0;31m     return MlflowClient().create_logged_model(\n\u001b[0m\u001b[1;32m   2258\u001b[0m         \u001b[0mexperiment_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracking/client.py\u001b[0m in \u001b[0;36mcreate_logged_model\u001b[0;34m(self, experiment_id, name, source_run_id, tags, params, model_type)\u001b[0m\n\u001b[1;32m   5369\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5370\u001b[0m         \"\"\"\n\u001b[0;32m-> 5371\u001b[0;31m         return self._tracking_client.create_logged_model(\n\u001b[0m\u001b[1;32m   5372\u001b[0m             \u001b[0mexperiment_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_run_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5373\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracking/_tracking_service/client.py\u001b[0m in \u001b[0;36mcreate_logged_model\u001b[0;34m(self, experiment_id, name, source_run_id, tags, params, model_type)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0mmodel_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     ) -> LoggedModel:\n\u001b[0;32m--> 824\u001b[0;31m         return self.store.create_logged_model(\n\u001b[0m\u001b[1;32m    825\u001b[0m             \u001b[0mexperiment_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/store/tracking/rest_store.py\u001b[0m in \u001b[0;36mcreate_logged_model\u001b[0;34m(self, experiment_id, name, source_run_id, tags, params, model_type)\u001b[0m\n\u001b[1;32m    934\u001b[0m         )\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mresponse_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCreateLoggedModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoggedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/store/tracking/rest_store.py\u001b[0m in \u001b[0;36m_call_endpoint\u001b[0;34m(self, api, json_body, endpoint, retry_timeout_seconds)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_METHOD_TO_INFO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mresponse_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         return call_endpoint(\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_host_creds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/utils/rest_utils.py\u001b[0m in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers, retry_timeout_seconds)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverify_rest_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m     \u001b[0mresponse_to_parse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/utils/rest_utils.py\u001b[0m in \u001b[0;36mverify_rest_response\u001b[0;34m(response, endpoint)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_can_parse_as_json_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRestException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             base_msg = (\n",
            "\u001b[0;31mRestException\u001b[0m: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 6. კროს-ვალიდაცია (MLflow Run: Prophet_Cross_Validation) ---\n",
        "with mlflow.start_run(run_name=\"Prophet_Cross_Validation\", nested=True) as cv_run:\n",
        "    print(\"Prophet კროს-ვალიდაცია იწყება...\")\n",
        "\n",
        "    # კროს-ვალიდაციის პარამეტრები\n",
        "    # initial: საწყისი სატრენინგო პერიოდი\n",
        "    # period: ყოველი შემდგომი გაფართოების პერიოდი\n",
        "    # horizon: პროგნოზის ჰორიზონტი\n",
        "    cv_initial = f\"{int(len(train_prophet_full) * 0.7)} W\" # 70% საწყისი ტრენინგი\n",
        "    cv_period = \"4 W\" # ყოველ 4 კვირაში\n",
        "    cv_horizon = \"12 W\" # 12 კვირიანი პროგნოზი\n",
        "\n",
        "    print(f\"კროს-ვალიდაციის პარამეტრები: initial={cv_initial}, period={cv_period}, horizon={cv_horizon}\")\n",
        "\n",
        "    # MLflow-ში ლოგირება\n",
        "    mlflow.log_param(\"cv_initial\", cv_initial)\n",
        "    mlflow.log_param(\"cv_period\", cv_period)\n",
        "    mlflow.log_param(\"cv_horizon\", cv_horizon)\n",
        "\n",
        "    # კროს-ვალიდაციის გაშვება\n",
        "    try:\n",
        "        df_cv = cross_validation(\n",
        "            m,\n",
        "            initial=cv_initial,\n",
        "            period=cv_period,\n",
        "            horizon=cv_horizon,\n",
        "            cutoffs=None, # ავტომატური cutoffs\n",
        "            parallel=\"processes\" # პარალელური შესრულება\n",
        "        )\n",
        "        print(\"კროს-ვალიდაცია დასრულდა.\")\n",
        "        print(f\"კროს-ვალიდაციის შედეგების ნიმუში:\\n{df_cv.head()}\")\n",
        "\n",
        "        # შესრულების მეტრიკების გამოთვლა\n",
        "        df_p = performance_metrics(df_cv)\n",
        "        print(f\"შესრულების მეტრიკების ნიმუში:\\n{df_p.head()}\")\n",
        "\n",
        "        # შეფასების მეტრიკების ლოგირება MLflow-ში\n",
        "        mlflow.log_metric(\"mae_cv_mean\", df_p['mae'].mean())\n",
        "        mlflow.log_metric(\"rmse_cv_mean\", df_p['rmse'].mean())\n",
        "        mlflow.log_metric(\"mape_cv_mean\", df_p['mape'].mean())\n",
        "        mlflow.log_metric(\"smape_cv_mean\", df_p['smape'].mean())\n",
        "\n",
        "        # ვიზუალიზაცია: შეცდომის მეტრიკები დროის მიხედვით\n",
        "        fig_metrics = make_subplots(rows=2, cols=1, shared_xaxes=True,\n",
        "                                    subplot_titles=(\"MAE დროის მიხედვით\", \"RMSE დროის მიხედვით\"))\n",
        "        fig_metrics.add_trace(go.Scatter(x=df_p['horizon'], y=df_p['mae'], mode='lines', name='MAE'), row=1, col=1)\n",
        "        fig_metrics.add_trace(go.Scatter(x=df_p['horizon'], y=df_p['rmse'], mode='lines', name='RMSE'), row=2, col=1)\n",
        "        fig_metrics.update_layout(title_text=\"Prophet კროს-ვალიდაციის მეტრიკები\", height=600)\n",
        "        mlflow.log_figure(fig_metrics, \"prophet_cv_metrics.png\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"შეცდომა კროს-ვალიდაციისას: {e}\")\n",
        "        print(\"ეს შეიძლება მოხდეს, თუ მონაცემთა ნაკრები ძალიან მცირეა კროს-ვალიდაციისთვის მითითებული პარამეტრებით.\")\n",
        "        mlflow.log_param(\"cv_error\", str(e))\n",
        "    except Exception as e:\n",
        "        print(f\"მოულოდნელი შეცდომა კროს-ვალიდაციისას: {e}\")\n",
        "        mlflow.log_param(\"cv_error\", str(e))\n",
        "\n",
        "print(\"Prophet მოდელის ექსპერიმენტი დასრულდა.\")\n"
      ],
      "metadata": {
        "id": "aunCbvW7iV2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 7. მოდელის რეგისტრაცია (MLflow Model Registry) ---\n",
        "# ეს ნაბიჯი უნდა შესრულდეს მხოლოდ საუკეთესო მოდელისთვის.\n",
        "# ამ ნოუთბუქში, ჩვენ ვრეგისტრირებთ Prophet მოდელს.\n",
        "# model_inference.ipynb გამოიყენებს ამ რეგისტრირებულ მოდელს.\n",
        "\n",
        "# დარწმუნდით, რომ training_run აქტიურია ან გამოიყენეთ run_id training_run-დან\n",
        "# თუ გსურთ მოდელის რეგისტრაცია ცალკე run-ში, შექმენით ახალი run.\n",
        "# ამ მაგალითისთვის, ვრეგისტრირებთ მოდელს, რომელიც ახლახან დავატრენინგეთ.\n",
        "\n",
        "# MLflow-ში შენახული მოდელის ჩატვირთვა MLflow-ის Artifacts-დან\n",
        "# რადგან Prophet არ არის პირდაპირ PyTorch/Sklearn, ჩვენ ის შევინახეთ როგორც pyfunc\n",
        "# და ახლა უნდა ჩავტვირთოთ შესაბამისად.\n",
        "# ეს ნაწილი შეიძლება იყოს model_inference.ipynb-შიც.\n",
        "\n",
        "# მოდელის რეგისტრაცია\n",
        "# რეგისტრაციისთვის საჭიროა run_id, საიდანაც მოდელი დაილოგა.\n",
        "# training_run.info.run_id არის run_id training_run-ისთვის.\n",
        "try:\n",
        "    registered_model = mlflow.register_model(\n",
        "        model_uri=f\"runs:/{training_run.info.run_id}/prophet_model\",\n",
        "        name=\"WalmartSalesProphetModel\"\n",
        "    )\n",
        "    print(f\"მოდელი 'WalmartSalesProphetModel' წარმატებით დარეგისტრირდა Model Registry-ში.\")\n",
        "    print(f\"ვერსია: {registered_model.version}\")\n",
        "except Exception as e:\n",
        "    print(f\"შეცდომა მოდელის რეგისტრაციისას: {e}\")\n",
        "    print(\"დარწმუნდით, რომ MLflow Tracking Server გაშვებულია და გაქვთ წვდომა Model Registry-ზე.\")"
      ],
      "metadata": {
        "id": "rNeLsfeQiQ2k"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9ca35ad8a798492186fa18149b612d57": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_d88cd209fd6e473fb61429496d14899b",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m⠙\u001b[0m Waiting for authorization\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠙</span> Waiting for authorization\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "d88cd209fd6e473fb61429496d14899b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}