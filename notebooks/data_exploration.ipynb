{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b84a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53907445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn mlflow\n",
    "\n",
    "# Set up Kaggle API\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e8f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your kaggle.json to Colab and run:\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e21acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
    "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b06260",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q train.csv.zip\n",
    "!unzip -q stores.csv.zip\n",
    "!unzip -q test.csv.zip\n",
    "!unzip -q features.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b54e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walmart Store Sales Forecasting - Data Exploration & Feature Engineering\n",
    "# Kaggle Competition: Walmart Recruiting - Store Sales Forecasting\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import scipy.stats as stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ğŸ“Š Walmart Store Sales Forecasting - Data Exploration\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1fc620",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 1. DATA LOADING AND INITIAL EXPLORATION\n",
    "# =============================================================================\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load all competition datasets\"\"\"\n",
    "    try:\n",
    "        # Load the main datasets\n",
    "        train = pd.read_csv('train.csv')\n",
    "        test = pd.read_csv('test.csv')\n",
    "        stores = pd.read_csv('stores.csv')\n",
    "        features = pd.read_csv('features.csv')\n",
    "        \n",
    "        print(f\"âœ… Data loaded successfully!\")\n",
    "        print(f\"ğŸ“ˆ Train shape: {train.shape}\")\n",
    "        print(f\"ğŸ”® Test shape: {test.shape}\")\n",
    "        print(f\"ğŸª Stores shape: {stores.shape}\")\n",
    "        print(f\"ğŸ“‹ Features shape: {features.shape}\")\n",
    "        \n",
    "        return train, test, stores, features\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"âŒ Error loading data: {e}\")\n",
    "        print(\"Please ensure all CSV files are in the current directory:\")\n",
    "        print(\"- train.csv, test.csv, stores.csv, features.csv\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def basic_info(df, name):\n",
    "    \"\"\"Display basic information about a dataframe\"\"\"\n",
    "    print(f\"\\nğŸ” {name} Dataset Overview:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"\\nData Types:\")\n",
    "    print(df.dtypes)\n",
    "    print(f\"\\nMissing Values:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "\n",
    "# Load data\n",
    "train, test, stores, features = load_data()\n",
    "\n",
    "if train is not None:\n",
    "    # Display basic information for each dataset\n",
    "    basic_info(train, \"TRAIN\")\n",
    "    basic_info(test, \"TEST\")\n",
    "    basic_info(stores, \"STORES\")\n",
    "    basic_info(features, \"FEATURES\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd56369",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 2. DATA PREPROCESSING AND CLEANING\n",
    "# =============================================================================\n",
    "\n",
    "def preprocess_data(train, test, stores, features):\n",
    "    \"\"\"Clean and preprocess the datasets\"\"\"\n",
    "    print(\"\\nğŸ§¹ Data Preprocessing...\")\n",
    "    \n",
    "    # Convert date columns\n",
    "    train['Date'] = pd.to_datetime(train['Date'])\n",
    "    test['Date'] = pd.to_datetime(test['Date'])\n",
    "    features['Date'] = pd.to_datetime(features['Date'])\n",
    "    \n",
    "    # Merge train with stores and features\n",
    "    train_full = train.merge(stores, on='Store', how='left')\n",
    "    train_full = train_full.merge(features, on=['Store', 'Date'], how='left')\n",
    "    \n",
    "    # Merge test with stores and features\n",
    "    test_full = test.merge(stores, on='Store', how='left')\n",
    "    test_full = test_full.merge(features, on=['Store', 'Date'], how='left')\n",
    "    \n",
    "    print(f\"âœ… Merged train shape: {train_full.shape}\")\n",
    "    print(f\"âœ… Merged test shape: {test_full.shape}\")\n",
    "    \n",
    "    return train_full, test_full\n",
    "\n",
    "if train is not None:\n",
    "    train_full, test_full = preprocess_data(train, test, stores, features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc36718",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 3. EXPLORATORY DATA ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_target_variable(df):\n",
    "    \"\"\"Analyze the target variable (Weekly_Sales)\"\"\"\n",
    "    print(\"\\nğŸ“Š Target Variable Analysis (Weekly_Sales)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Distribution of Weekly Sales\n",
    "    axes[0,0].hist(df['Weekly_Sales'], bins=50, alpha=0.7, color='skyblue')\n",
    "    axes[0,0].set_title('Distribution of Weekly Sales')\n",
    "    axes[0,0].set_xlabel('Weekly Sales')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Log-transformed distribution\n",
    "    log_sales = np.log1p(df['Weekly_Sales'])\n",
    "    axes[0,1].hist(log_sales, bins=50, alpha=0.7, color='lightgreen')\n",
    "    axes[0,1].set_title('Log-transformed Weekly Sales')\n",
    "    axes[0,1].set_xlabel('Log(Weekly Sales + 1)')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Box plot\n",
    "    axes[1,0].boxplot(df['Weekly_Sales'])\n",
    "    axes[1,0].set_title('Weekly Sales Box Plot')\n",
    "    axes[1,0].set_ylabel('Weekly Sales')\n",
    "    \n",
    "    # Sales over time\n",
    "    monthly_sales = df.groupby(df['Date'].dt.to_period('M'))['Weekly_Sales'].mean()\n",
    "    axes[1,1].plot(monthly_sales.index.to_timestamp(), monthly_sales.values)\n",
    "    axes[1,1].set_title('Average Weekly Sales Over Time')\n",
    "    axes[1,1].set_xlabel('Date')\n",
    "    axes[1,1].set_ylabel('Average Weekly Sales')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"Mean Weekly Sales: ${df['Weekly_Sales'].mean():,.2f}\")\n",
    "    print(f\"Median Weekly Sales: ${df['Weekly_Sales'].median():,.2f}\")\n",
    "    print(f\"Standard Deviation: ${df['Weekly_Sales'].std():,.2f}\")\n",
    "    print(f\"Min Sales: ${df['Weekly_Sales'].min():,.2f}\")\n",
    "    print(f\"Max Sales: ${df['Weekly_Sales'].max():,.2f}\")\n",
    "    print(f\"Skewness: {df['Weekly_Sales'].skew():.3f}\")\n",
    "    print(f\"Kurtosis: {df['Weekly_Sales'].kurtosis():.3f}\")\n",
    "\n",
    "def analyze_stores_and_departments(df):\n",
    "    \"\"\"Analyze store and department patterns\"\"\"\n",
    "    print(\"\\nğŸª Store and Department Analysis\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Sales by store\n",
    "    store_sales = df.groupby('Store')['Weekly_Sales'].mean().sort_values(ascending=False)\n",
    "    axes[0,0].bar(range(len(store_sales)), store_sales.values, alpha=0.7)\n",
    "    axes[0,0].set_title('Average Weekly Sales by Store')\n",
    "    axes[0,0].set_xlabel('Store (Ranked)')\n",
    "    axes[0,0].set_ylabel('Average Weekly Sales')\n",
    "    \n",
    "    # Sales by department\n",
    "    dept_sales = df.groupby('Dept')['Weekly_Sales'].mean().sort_values(ascending=False).head(20)\n",
    "    axes[0,1].barh(range(len(dept_sales)), dept_sales.values, alpha=0.7)\n",
    "    axes[0,1].set_yticks(range(len(dept_sales)))\n",
    "    axes[0,1].set_yticklabels(dept_sales.index)\n",
    "    axes[0,1].set_title('Top 20 Departments by Average Sales')\n",
    "    axes[0,1].set_xlabel('Average Weekly Sales')\n",
    "    \n",
    "    # Store type analysis\n",
    "    if 'Type' in df.columns:\n",
    "        type_sales = df.groupby('Type')['Weekly_Sales'].mean()\n",
    "        axes[1,0].bar(type_sales.index, type_sales.values, alpha=0.7)\n",
    "        axes[1,0].set_title('Average Sales by Store Type')\n",
    "        axes[1,0].set_xlabel('Store Type')\n",
    "        axes[1,0].set_ylabel('Average Weekly Sales')\n",
    "    \n",
    "    # Store size vs sales\n",
    "    if 'Size' in df.columns:\n",
    "        axes[1,1].scatter(df['Size'], df['Weekly_Sales'], alpha=0.5)\n",
    "        axes[1,1].set_title('Store Size vs Weekly Sales')\n",
    "        axes[1,1].set_xlabel('Store Size')\n",
    "        axes[1,1].set_ylabel('Weekly Sales')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print insights\n",
    "    print(f\"Number of unique stores: {df['Store'].nunique()}\")\n",
    "    print(f\"Number of unique departments: {df['Dept'].nunique()}\")\n",
    "    print(f\"Top 5 performing stores: {store_sales.head().index.tolist()}\")\n",
    "    print(f\"Top 5 performing departments: {dept_sales.head().index.tolist()}\")\n",
    "\n",
    "def analyze_temporal_patterns(df):\n",
    "    \"\"\"Analyze temporal patterns in sales data\"\"\"\n",
    "    print(\"\\nğŸ“… Temporal Pattern Analysis\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Create time-based features\n",
    "    df_temp = df.copy()\n",
    "    df_temp['Year'] = df_temp['Date'].dt.year\n",
    "    df_temp['Month'] = df_temp['Date'].dt.month\n",
    "    df_temp['Week'] = df_temp['Date'].dt.isocalendar().week\n",
    "    df_temp['Quarter'] = df_temp['Date'].dt.quarter\n",
    "    df_temp['DayOfYear'] = df_temp['Date'].dt.dayofyear\n",
    "    df_temp['WeekOfYear'] = df_temp['Date'].dt.isocalendar().week\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    \n",
    "    # Monthly patterns\n",
    "    monthly_avg = df_temp.groupby('Month')['Weekly_Sales'].mean()\n",
    "    axes[0,0].plot(monthly_avg.index, monthly_avg.values, marker='o')\n",
    "    axes[0,0].set_title('Average Sales by Month')\n",
    "    axes[0,0].set_xlabel('Month')\n",
    "    axes[0,0].set_ylabel('Average Weekly Sales')\n",
    "    axes[0,0].set_xticks(range(1, 13))\n",
    "    \n",
    "    # Weekly patterns\n",
    "    weekly_avg = df_temp.groupby('Week')['Weekly_Sales'].mean()\n",
    "    axes[0,1].plot(weekly_avg.index, weekly_avg.values, alpha=0.7)\n",
    "    axes[0,1].set_title('Average Sales by Week of Year')\n",
    "    axes[0,1].set_xlabel('Week of Year')\n",
    "    axes[0,1].set_ylabel('Average Weekly Sales')\n",
    "    \n",
    "    # Quarterly patterns\n",
    "    quarterly_avg = df_temp.groupby('Quarter')['Weekly_Sales'].mean()\n",
    "    axes[0,2].bar(quarterly_avg.index, quarterly_avg.values, alpha=0.7)\n",
    "    axes[0,2].set_title('Average Sales by Quarter')\n",
    "    axes[0,2].set_xlabel('Quarter')\n",
    "    axes[0,2].set_ylabel('Average Weekly Sales')\n",
    "    \n",
    "    # Year-over-year comparison\n",
    "    yearly_avg = df_temp.groupby('Year')['Weekly_Sales'].mean()\n",
    "    axes[1,0].bar(yearly_avg.index, yearly_avg.values, alpha=0.7)\n",
    "    axes[1,0].set_title('Average Sales by Year')\n",
    "    axes[1,0].set_xlabel('Year')\n",
    "    axes[1,0].set_ylabel('Average Weekly Sales')\n",
    "    \n",
    "    # Heatmap of sales by month and year\n",
    "    if len(df_temp['Year'].unique()) > 1:\n",
    "        monthly_year = df_temp.groupby(['Year', 'Month'])['Weekly_Sales'].mean().unstack()\n",
    "        sns.heatmap(monthly_year, annot=True, fmt='.0f', cmap='YlOrRd', ax=axes[1,1])\n",
    "        axes[1,1].set_title('Sales Heatmap: Year vs Month')\n",
    "    \n",
    "    # Day of year pattern\n",
    "    daily_avg = df_temp.groupby('DayOfYear')['Weekly_Sales'].mean()\n",
    "    axes[1,2].plot(daily_avg.index, daily_avg.values, alpha=0.7)\n",
    "    axes[1,2].set_title('Sales Pattern Throughout the Year')\n",
    "    axes[1,2].set_xlabel('Day of Year')\n",
    "    axes[1,2].set_ylabel('Average Weekly Sales')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_holiday_impact(df):\n",
    "    \"\"\"Analyze the impact of holidays on sales\"\"\"\n",
    "    print(\"\\nğŸ‰ Holiday Impact Analysis\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    if 'IsHoliday' in df.columns:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Holiday vs Non-holiday sales\n",
    "        holiday_sales = df.groupby('IsHoliday')['Weekly_Sales'].mean()\n",
    "        axes[0].bar(['Non-Holiday', 'Holiday'], holiday_sales.values, \n",
    "                   color=['lightblue', 'orange'], alpha=0.7)\n",
    "        axes[0].set_title('Average Sales: Holiday vs Non-Holiday')\n",
    "        axes[0].set_ylabel('Average Weekly Sales')\n",
    "        \n",
    "        # Box plot comparison\n",
    "        df.boxplot(column='Weekly_Sales', by='IsHoliday', ax=axes[1])\n",
    "        axes[1].set_title('Sales Distribution: Holiday vs Non-Holiday')\n",
    "        axes[1].set_xlabel('Is Holiday')\n",
    "        axes[1].set_ylabel('Weekly Sales')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Statistical test\n",
    "        holiday_data = df[df['IsHoliday'] == True]['Weekly_Sales']\n",
    "        non_holiday_data = df[df['IsHoliday'] == False]['Weekly_Sales']\n",
    "        \n",
    "        t_stat, p_value = stats.ttest_ind(holiday_data, non_holiday_data)\n",
    "        print(f\"Holiday Sales Average: ${holiday_sales[True]:,.2f}\")\n",
    "        print(f\"Non-Holiday Sales Average: ${holiday_sales[False]:,.2f}\")\n",
    "        print(f\"T-test p-value: {p_value:.6f}\")\n",
    "        print(f\"Significant difference: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "\n",
    "def analyze_external_factors(df):\n",
    "    \"\"\"Analyze external factors like temperature, fuel price, etc.\"\"\"\n",
    "    print(\"\\nğŸŒ¡ï¸ External Factors Analysis\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # List of potential external factors\n",
    "    external_factors = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
    "    available_factors = [col for col in external_factors if col in df.columns]\n",
    "    \n",
    "    if not available_factors:\n",
    "        print(\"No external factors found in the dataset\")\n",
    "        return\n",
    "    \n",
    "    n_factors = len(available_factors)\n",
    "    fig, axes = plt.subplots(2, (n_factors + 1) // 2, figsize=(15, 10))\n",
    "    axes = axes.flatten() if n_factors > 1 else [axes]\n",
    "    \n",
    "    correlations = []\n",
    "    \n",
    "    for i, factor in enumerate(available_factors):\n",
    "        if i < len(axes):\n",
    "            # Scatter plot\n",
    "            axes[i].scatter(df[factor], df['Weekly_Sales'], alpha=0.5)\n",
    "            axes[i].set_xlabel(factor)\n",
    "            axes[i].set_ylabel('Weekly Sales')\n",
    "            axes[i].set_title(f'Sales vs {factor}')\n",
    "            \n",
    "            # Calculate correlation\n",
    "            correlation = df[factor].corr(df['Weekly_Sales'])\n",
    "            correlations.append((factor, correlation))\n",
    "            axes[i].text(0.05, 0.95, f'Corr: {correlation:.3f}', \n",
    "                        transform=axes[i].transAxes, verticalalignment='top')\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for i in range(n_factors, len(axes)):\n",
    "        axes[i].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print correlation summary\n",
    "    print(\"Correlation with Weekly Sales:\")\n",
    "    for factor, corr in sorted(correlations, key=lambda x: abs(x[1]), reverse=True):\n",
    "        print(f\"{factor}: {corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8396aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 4. FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "def create_time_features(df):\n",
    "    \"\"\"Create comprehensive time-based features\"\"\"\n",
    "    print(\"\\nâ° Creating Time Features...\")\n",
    "    \n",
    "    df_fe = df.copy()\n",
    "    \n",
    "    # Basic time features\n",
    "    df_fe['Year'] = df_fe['Date'].dt.year\n",
    "    df_fe['Month'] = df_fe['Date'].dt.month\n",
    "    df_fe['Quarter'] = df_fe['Date'].dt.quarter\n",
    "    df_fe['Week'] = df_fe['Date'].dt.isocalendar().week\n",
    "    df_fe['DayOfYear'] = df_fe['Date'].dt.dayofyear\n",
    "    df_fe['WeekOfYear'] = df_fe['Date'].dt.isocalendar().week\n",
    "    \n",
    "    # Cyclical features (using sine/cosine to capture periodicity)\n",
    "    df_fe['Month_sin'] = np.sin(2 * np.pi * df_fe['Month'] / 12)\n",
    "    df_fe['Month_cos'] = np.cos(2 * np.pi * df_fe['Month'] / 12)\n",
    "    df_fe['Week_sin'] = np.sin(2 * np.pi * df_fe['Week'] / 52)\n",
    "    df_fe['Week_cos'] = np.cos(2 * np.pi * df_fe['Week'] / 52)\n",
    "    \n",
    "    # Special periods\n",
    "    df_fe['IsQ4'] = (df_fe['Quarter'] == 4).astype(int)  # Holiday season\n",
    "    df_fe['IsDecember'] = (df_fe['Month'] == 12).astype(int)\n",
    "    df_fe['IsJanuary'] = (df_fe['Month'] == 1).astype(int)\n",
    "    \n",
    "    # Back-to-school season (August-September)\n",
    "    df_fe['IsBackToSchool'] = ((df_fe['Month'] == 8) | (df_fe['Month'] == 9)).astype(int)\n",
    "    \n",
    "    # Holiday weeks (approximate)\n",
    "    df_fe['IsHolidayWeek'] = 0\n",
    "    if 'IsHoliday' in df_fe.columns:\n",
    "        df_fe['IsHolidayWeek'] = df_fe['IsHoliday'].astype(int)\n",
    "    \n",
    "    print(f\"âœ… Created {len([col for col in df_fe.columns if col not in df.columns])} time features\")\n",
    "    return df_fe\n",
    "\n",
    "def create_lag_features(df, target_col='Weekly_Sales', lags=[1, 2, 4, 8]):\n",
    "    \"\"\"Create lag features for time series\"\"\"\n",
    "    print(f\"\\nğŸ“ˆ Creating Lag Features (lags: {lags})...\")\n",
    "    \n",
    "    df_lag = df.copy()\n",
    "    df_lag = df_lag.sort_values(['Store', 'Dept', 'Date'])\n",
    "    \n",
    "    for lag in lags:\n",
    "        lag_col = f'{target_col}_lag_{lag}'\n",
    "        df_lag[lag_col] = df_lag.groupby(['Store', 'Dept'])[target_col].shift(lag)\n",
    "    \n",
    "    print(f\"âœ… Created {len(lags)} lag features\")\n",
    "    return df_lag\n",
    "\n",
    "def create_rolling_features(df, target_col='Weekly_Sales', windows=[4, 8, 12]):\n",
    "    \"\"\"Create rolling window statistics\"\"\"\n",
    "    print(f\"\\nğŸ“Š Creating Rolling Features (windows: {windows})...\")\n",
    "    \n",
    "    df_roll = df.copy()\n",
    "    df_roll = df_roll.sort_values(['Store', 'Dept', 'Date'])\n",
    "    \n",
    "    for window in windows:\n",
    "        # Rolling mean\n",
    "        roll_mean_col = f'{target_col}_roll_mean_{window}'\n",
    "        df_roll[roll_mean_col] = df_roll.groupby(['Store', 'Dept'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean().shift(1)\n",
    "        )\n",
    "        \n",
    "        # Rolling std\n",
    "        roll_std_col = f'{target_col}_roll_std_{window}'\n",
    "        df_roll[roll_std_col] = df_roll.groupby(['Store', 'Dept'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).std().shift(1)\n",
    "        )\n",
    "        \n",
    "        # Rolling min/max\n",
    "        roll_min_col = f'{target_col}_roll_min_{window}'\n",
    "        roll_max_col = f'{target_col}_roll_max_{window}'\n",
    "        df_roll[roll_min_col] = df_roll.groupby(['Store', 'Dept'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).min().shift(1)\n",
    "        )\n",
    "        df_roll[roll_max_col] = df_roll.groupby(['Store', 'Dept'])[target_col].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).max().shift(1)\n",
    "        )\n",
    "    \n",
    "    n_features = len(windows) * 4  # mean, std, min, max for each window\n",
    "    print(f\"âœ… Created {n_features} rolling features\")\n",
    "    return df_roll\n",
    "\n",
    "def create_aggregation_features(df):\n",
    "    \"\"\"Create aggregation features by different groupings\"\"\"\n",
    "    print(\"\\nğŸ”— Creating Aggregation Features...\")\n",
    "    \n",
    "    df_agg = df.copy()\n",
    "    \n",
    "    # Store-level aggregations\n",
    "    store_stats = df.groupby('Store')['Weekly_Sales'].agg([\n",
    "        'mean', 'std', 'min', 'max', 'median'\n",
    "    ]).add_prefix('Store_')\n",
    "    \n",
    "    # Department-level aggregations\n",
    "    dept_stats = df.groupby('Dept')['Weekly_Sales'].agg([\n",
    "        'mean', 'std', 'min', 'max', 'median'\n",
    "    ]).add_prefix('Dept_')\n",
    "    \n",
    "    # Store-Department interaction\n",
    "    store_dept_stats = df.groupby(['Store', 'Dept'])['Weekly_Sales'].agg([\n",
    "        'mean', 'std', 'count'\n",
    "    ]).add_prefix('StoreDept_')\n",
    "    \n",
    "    # Merge back to main dataframe\n",
    "    df_agg = df_agg.merge(store_stats, on='Store', how='left')\n",
    "    df_agg = df_agg.merge(dept_stats, on='Dept', how='left')\n",
    "    df_agg = df_agg.merge(store_dept_stats, on=['Store', 'Dept'], how='left')\n",
    "    \n",
    "    # Store type aggregations (if available)\n",
    "    if 'Type' in df_agg.columns:\n",
    "        type_stats = df.groupby('Type')['Weekly_Sales'].agg([\n",
    "            'mean', 'std', 'median'\n",
    "        ]).add_prefix('Type_')\n",
    "        df_agg = df_agg.merge(type_stats, on='Type', how='left')\n",
    "    \n",
    "    n_features = len([col for col in df_agg.columns if col not in df.columns])\n",
    "    print(f\"âœ… Created {n_features} aggregation features\")\n",
    "    return df_agg\n",
    "\n",
    "def create_interaction_features(df):\n",
    "    \"\"\"Create interaction features between important variables\"\"\"\n",
    "    print(\"\\nğŸ¤ Creating Interaction Features...\")\n",
    "    \n",
    "    df_int = df.copy()\n",
    "    \n",
    "    # Size-based interactions (if Size is available)\n",
    "    if 'Size' in df_int.columns:\n",
    "        df_int['Size_per_Dept'] = df_int['Size'] / df_int.groupby('Store')['Dept'].transform('nunique')\n",
    "        \n",
    "        # Temperature interactions\n",
    "        if 'Temperature' in df_int.columns:\n",
    "            df_int['Size_Temperature'] = df_int['Size'] * df_int['Temperature']\n",
    "    \n",
    "    # Holiday and external factor interactions\n",
    "    if 'IsHoliday' in df_int.columns:\n",
    "        if 'Temperature' in df_int.columns:\n",
    "            df_int['Holiday_Temperature'] = df_int['IsHoliday'].astype(int) * df_int['Temperature']\n",
    "        if 'Fuel_Price' in df_int.columns:\n",
    "            df_int['Holiday_FuelPrice'] = df_int['IsHoliday'].astype(int) * df_int['Fuel_Price']\n",
    "    \n",
    "    # Store type and size interaction\n",
    "    if 'Type' in df_int.columns and 'Size' in df_int.columns:\n",
    "        # Encode store type\n",
    "        le = LabelEncoder()\n",
    "        df_int['Type_encoded'] = le.fit_transform(df_int['Type'])\n",
    "        df_int['Type_Size'] = df_int['Type_encoded'] * df_int['Size']\n",
    "    \n",
    "    n_features = len([col for col in df_int.columns if col not in df.columns])\n",
    "    print(f\"âœ… Created {n_features} interaction features\")\n",
    "    return df_int\n",
    "\n",
    "def feature_importance_analysis(df, target_col='Weekly_Sales'):\n",
    "    \"\"\"Analyze feature importance using correlation and basic methods\"\"\"\n",
    "    print(\"\\nğŸ¯ Feature Importance Analysis\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Select numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_col in numeric_cols:\n",
    "        numeric_cols.remove(target_col)\n",
    "    \n",
    "    # Calculate correlations\n",
    "    correlations = []\n",
    "    for col in numeric_cols:\n",
    "        if col != target_col:\n",
    "            corr = df[col].corr(df[target_col])\n",
    "            if not np.isnan(corr):\n",
    "                correlations.append((col, abs(corr)))\n",
    "    \n",
    "    # Sort by absolute correlation\n",
    "    correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Display top correlations\n",
    "    print(\"Top 20 Features by Correlation with Weekly_Sales:\")\n",
    "    print(\"-\" * 55)\n",
    "    for i, (feature, corr) in enumerate(correlations[:20], 1):\n",
    "        print(f\"{i:2d}. {feature:<30} {corr:.4f}\")\n",
    "    \n",
    "    # Visualize top correlations\n",
    "    if len(correlations) >= 10:\n",
    "        top_features = [item[0] for item in correlations[:15]]\n",
    "        top_corrs = [item[1] for item in correlations[:15]]\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.barh(range(len(top_features)), top_corrs, alpha=0.7)\n",
    "        plt.yticks(range(len(top_features)), top_features)\n",
    "        plt.xlabel('Absolute Correlation with Weekly Sales')\n",
    "        plt.title('Top 15 Features by Correlation')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ef534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 5. COMPREHENSIVE FEATURE ENGINEERING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def comprehensive_feature_engineering(df):\n",
    "    \"\"\"Apply all feature engineering techniques\"\"\"\n",
    "    print(\"\\nğŸ”§ Comprehensive Feature Engineering Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    original_features = df.shape[1]\n",
    "    \n",
    "    # Apply all feature engineering steps\n",
    "    df_fe = create_time_features(df)\n",
    "    df_fe = create_aggregation_features(df_fe)\n",
    "    df_fe = create_interaction_features(df_fe)\n",
    "    \n",
    "    # Only create lag and rolling features for training data (not test)\n",
    "    if 'Weekly_Sales' in df_fe.columns:\n",
    "        df_fe = create_lag_features(df_fe)\n",
    "        df_fe = create_rolling_features(df_fe)\n",
    "    \n",
    "    final_features = df_fe.shape[1]\n",
    "    new_features = final_features - original_features\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Feature Engineering Summary:\")\n",
    "    print(f\"Original features: {original_features}\")\n",
    "    print(f\"Final features: {final_features}\")\n",
    "    print(f\"New features created: {new_features}\")\n",
    "    \n",
    "    return df_fe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dba48e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 6. MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if train is not None:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸš€ STARTING COMPREHENSIVE DATA EXPLORATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Basic EDA\n",
    "    analyze_target_variable(train_full)\n",
    "    analyze_stores_and_departments(train_full)\n",
    "    analyze_temporal_patterns(train_full)\n",
    "    analyze_holiday_impact(train_full)\n",
    "    analyze_external_factors(train_full)\n",
    "    \n",
    "    # Feature Engineering\n",
    "    train_engineered = comprehensive_feature_engineering(train_full)\n",
    "    test_engineered = comprehensive_feature_engineering(test_full)\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    if 'Weekly_Sales' in train_engineered.columns:\n",
    "        feature_importance = feature_importance_analysis(train_engineered)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… DATA EXPLORATION AND FEATURE ENGINEERING COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ Final Training Dataset Shape: {train_engineered.shape}\")\n",
    "    print(f\"ğŸ”® Final Test Dataset Shape: {test_engineered.shape}\")\n",
    "    \n",
    "    # Display feature categories\n",
    "    print(f\"\\nğŸ“‹ Feature Categories Created:\")\n",
    "    time_features = [col for col in train_engineered.columns if any(x in col.lower() for x in ['year', 'month', 'quarter', 'week', 'day', 'sin', 'cos', 'holiday'])]\n",
    "    lag_features = [col for col in train_engineered.columns if 'lag' in col.lower()]\n",
    "    rolling_features = [col for col in train_engineered.columns if 'roll' in col.lower()]\n",
    "    agg_features = [col for col in train_engineered.columns if any(x in col for x in ['Store_', 'Dept_', 'Type_', 'StoreDept_'])]\n",
    "    interaction_features = [col for col in train_engineered.columns if any(x in col for x in ['Size_', 'Holiday_', 'Type_Size'])]\n",
    "    \n",
    "    print(f\"â° Time Features: {len(time_features)}\")\n",
    "    print(f\"ğŸ“ˆ Lag Features: {len(lag_features)}\")\n",
    "    print(f\"ğŸ“Š Rolling Features: {len(rolling_features)}\")\n",
    "    print(f\"ğŸ”— Aggregation Features: {len(agg_features)}\")\n",
    "    print(f\"ğŸ¤ Interaction Features: {len(interaction_features)}\")\n",
    "    \n",
    "    # Save the engineered datasets\n",
    "    print(f\"\\nğŸ’¾ Saving engineered datasets...\")\n",
    "    train_engineered.to_csv('train_engineered.csv', index=False)\n",
    "    test_engineered.to_csv('test_engineered.csv', index=False)\n",
    "    print(f\"âœ… Datasets saved as 'train_engineered.csv' and 'test_engineered.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f497b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 7. ADVANCED FEATURE ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_feature_distributions(df, feature_list, target_col='Weekly_Sales'):\n",
    "    \"\"\"Analyze distributions of engineered features\"\"\"\n",
    "    print(\"\\nğŸ“Š Feature Distribution Analysis\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    n_features = min(len(feature_list), 12)  # Limit to 12 features for visualization\n",
    "    selected_features = feature_list[:n_features]\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(selected_features):\n",
    "        if feature in df.columns and i < len(axes):\n",
    "            axes[i].hist(df[feature].dropna(), bins=30, alpha=0.7, color='skyblue')\n",
    "            axes[i].set_title(f'{feature}')\n",
    "            axes[i].set_xlabel(feature)\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "            \n",
    "            # Add basic statistics\n",
    "            mean_val = df[feature].mean()\n",
    "            std_val = df[feature].std()\n",
    "            axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.2f}')\n",
    "            axes[i].legend()\n",
    "    \n",
    "    # Remove unused subplots\n",
    "    for i in range(len(selected_features), len(axes)):\n",
    "        axes[i].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def correlation_heatmap(df, feature_list, target_col='Weekly_Sales'):\n",
    "    \"\"\"Create correlation heatmap for selected features\"\"\"\n",
    "    print(\"\\nğŸ”¥ Correlation Heatmap Analysis\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Select top features and target\n",
    "    selected_cols = feature_list[:20] + [target_col] if target_col in df.columns else feature_list[:21]\n",
    "    selected_cols = [col for col in selected_cols if col in df.columns]\n",
    "    \n",
    "    if len(selected_cols) > 1:\n",
    "        corr_matrix = df[selected_cols].corr()\n",
    "        \n",
    "        plt.figure(figsize=(16, 12))\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.3f', \n",
    "                   cmap='RdBu_r', center=0, square=True, linewidths=.5)\n",
    "        plt.title('Feature Correlation Heatmap (Top Features)')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Find highly correlated feature pairs\n",
    "        print(\"\\nğŸ” Highly Correlated Feature Pairs (|correlation| > 0.7):\")\n",
    "        print(\"-\" * 60)\n",
    "        high_corr_pairs = []\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                corr_val = corr_matrix.iloc[i, j]\n",
    "                if abs(corr_val) > 0.7:\n",
    "                    high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n",
    "        \n",
    "        if high_corr_pairs:\n",
    "            for feat1, feat2, corr_val in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
    "                print(f\"{feat1:<25} <-> {feat2:<25} {corr_val:>8.4f}\")\n",
    "        else:\n",
    "            print(\"No highly correlated feature pairs found.\")\n",
    "\n",
    "def feature_selection_recommendations(correlations, threshold=0.01):\n",
    "    \"\"\"Provide feature selection recommendations\"\"\"\n",
    "    print(\"\\nğŸ¯ Feature Selection Recommendations\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    # High importance features\n",
    "    high_importance = [feat for feat, corr in correlations if corr > threshold]\n",
    "    low_importance = [feat for feat, corr in correlations if corr <= threshold]\n",
    "    \n",
    "    print(f\"âœ… High Importance Features (|correlation| > {threshold}): {len(high_importance)}\")\n",
    "    print(f\"âŒ Low Importance Features (|correlation| <= {threshold}): {len(low_importance)}\")\n",
    "    \n",
    "    print(f\"\\nğŸ” Top 10 Recommended Features:\")\n",
    "    for i, (feature, corr) in enumerate(correlations[:10], 1):\n",
    "        print(f\"{i:2d}. {feature:<35} {corr:.4f}\")\n",
    "    \n",
    "    if low_importance:\n",
    "        print(f\"\\nğŸ—‘ï¸  Consider Removing (Bottom 10):\")\n",
    "        for i, (feature, corr) in enumerate(correlations[-10:], 1):\n",
    "            print(f\"{i:2d}. {feature:<35} {corr:.4f}\")\n",
    "    \n",
    "    return high_importance, low_importance\n",
    "\n",
    "def create_feature_summary_report(df, correlations):\n",
    "    \"\"\"Create a comprehensive feature summary report\"\"\"\n",
    "    print(\"\\nğŸ“‹ COMPREHENSIVE FEATURE SUMMARY REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Feature categories\n",
    "    feature_categories = {\n",
    "        'Original': [],\n",
    "        'Time': [],\n",
    "        'Lag': [],\n",
    "        'Rolling': [],\n",
    "        'Aggregation': [],\n",
    "        'Interaction': [],\n",
    "        'Other': []\n",
    "    }\n",
    "    \n",
    "    original_cols = ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Type', 'Size', \n",
    "                    'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', \n",
    "                    'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment']\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col in original_cols:\n",
    "            feature_categories['Original'].append(col)\n",
    "        elif any(x in col.lower() for x in ['year', 'month', 'quarter', 'week', 'day', 'sin', 'cos']):\n",
    "            feature_categories['Time'].append(col)\n",
    "        elif 'lag' in col.lower():\n",
    "            feature_categories['Lag'].append(col)\n",
    "        elif 'roll' in col.lower():\n",
    "            feature_categories['Rolling'].append(col)\n",
    "        elif any(x in col for x in ['Store_', 'Dept_', 'Type_', 'StoreDept_']):\n",
    "            feature_categories['Aggregation'].append(col)\n",
    "        elif any(x in col for x in ['Size_', 'Holiday_', 'Type_Size', 'Temperature', 'FuelPrice']):\n",
    "            feature_categories['Interaction'].append(col)\n",
    "        else:\n",
    "            feature_categories['Other'].append(col)\n",
    "    \n",
    "    print(f\"ğŸ“Š Feature Breakdown by Category:\")\n",
    "    print(\"-\" * 35)\n",
    "    total_features = 0\n",
    "    for category, features in feature_categories.items():\n",
    "        count = len(features)\n",
    "        total_features += count\n",
    "        print(f\"{category:<15}: {count:>3} features\")\n",
    "    print(f\"{'Total':<15}: {total_features:>3} features\")\n",
    "    \n",
    "    # Missing value analysis\n",
    "    print(f\"\\nğŸ” Missing Value Analysis:\")\n",
    "    print(\"-\" * 30)\n",
    "    missing_stats = df.isnull().sum()\n",
    "    missing_features = missing_stats[missing_stats > 0].sort_values(ascending=False)\n",
    "    \n",
    "    if len(missing_features) > 0:\n",
    "        print(\"Features with missing values:\")\n",
    "        for feature, count in missing_features.items():\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"{feature:<30}: {count:>6} ({percentage:>5.1f}%)\")\n",
    "    else:\n",
    "        print(\"âœ… No missing values found!\")\n",
    "    \n",
    "    # Data type summary\n",
    "    print(f\"\\nğŸ“ˆ Data Type Summary:\")\n",
    "    print(\"-\" * 25)\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"{str(dtype):<15}: {count:>3} features\")\n",
    "    \n",
    "    # Top performing features\n",
    "    print(f\"\\nğŸ† Top 15 Features by Importance:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, (feature, corr) in enumerate(correlations[:15], 1):\n",
    "        category = 'Unknown'\n",
    "        for cat, features in feature_categories.items():\n",
    "            if feature in features:\n",
    "                category = cat\n",
    "                break\n",
    "        print(f\"{i:2d}. {feature:<25} {corr:.4f} ({category})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557c2ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 8. MODEL PREPARATION UTILITIES\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_data_for_modeling(df, target_col='Weekly_Sales', test_size=0.2):\n",
    "    \"\"\"Prepare data for machine learning modeling\"\"\"\n",
    "    print(\"\\nğŸ¤– Preparing Data for Modeling\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Separate features and target\n",
    "    if target_col in df.columns:\n",
    "        X = df.drop(columns=[target_col, 'Date'], errors='ignore')\n",
    "        y = df[target_col]\n",
    "        \n",
    "        # Handle categorical variables\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "        if categorical_cols:\n",
    "            print(f\"ğŸ“ Encoding categorical variables: {categorical_cols}\")\n",
    "            for col in categorical_cols:\n",
    "                le = LabelEncoder()\n",
    "                X[col] = le.fit_transform(X[col].astype(str))\n",
    "        \n",
    "        # Handle missing values\n",
    "        numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())\n",
    "        \n",
    "        print(f\"âœ… Feature matrix shape: {X.shape}\")\n",
    "        print(f\"âœ… Target vector shape: {y.shape}\")\n",
    "        print(f\"âœ… Features ready for modeling!\")\n",
    "        \n",
    "        return X, y\n",
    "    else:\n",
    "        # Test data preparation\n",
    "        X = df.drop(columns=['Date'], errors='ignore')\n",
    "        \n",
    "        # Handle categorical variables\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "        if categorical_cols:\n",
    "            for col in categorical_cols:\n",
    "                le = LabelEncoder()\n",
    "                X[col] = le.fit_transform(X[col].astype(str))\n",
    "        \n",
    "        # Handle missing values\n",
    "        numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].median())\n",
    "        \n",
    "        print(f\"âœ… Test feature matrix shape: {X.shape}\")\n",
    "        return X\n",
    "\n",
    "def create_validation_strategy(df, target_col='Weekly_Sales'):\n",
    "    \"\"\"Create time-based validation strategy for time series data\"\"\"\n",
    "    print(\"\\nğŸ“… Time-Based Validation Strategy\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if 'Date' in df.columns:\n",
    "        df_sorted = df.sort_values('Date')\n",
    "        \n",
    "        # Split by date for time series validation\n",
    "        unique_dates = sorted(df_sorted['Date'].unique())\n",
    "        n_dates = len(unique_dates)\n",
    "        \n",
    "        # Use last 20% of dates for validation\n",
    "        split_idx = int(n_dates * 0.8)\n",
    "        train_dates = unique_dates[:split_idx]\n",
    "        val_dates = unique_dates[split_idx:]\n",
    "        \n",
    "        print(f\"ğŸ“Š Total unique dates: {n_dates}\")\n",
    "        print(f\"ğŸš‚ Training dates: {len(train_dates)} ({train_dates[0]} to {train_dates[-1]})\")\n",
    "        print(f\"âœ… Validation dates: {len(val_dates)} ({val_dates[0]} to {val_dates[-1]})\")\n",
    "        \n",
    "        # Create train/validation masks\n",
    "        train_mask = df['Date'].isin(train_dates)\n",
    "        val_mask = df['Date'].isin(val_dates)\n",
    "        \n",
    "        print(f\"ğŸ”¢ Training samples: {train_mask.sum()}\")\n",
    "        print(f\"ğŸ”¢ Validation samples: {val_mask.sum()}\")\n",
    "        \n",
    "        return train_mask, val_mask\n",
    "    else:\n",
    "        print(\"âŒ Date column not found. Using random split.\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3de2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 9. EXECUTE ADVANCED ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "if train is not None and 'train_engineered' in locals():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ”¬ ADVANCED FEATURE ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get top features from importance analysis\n",
    "    if 'feature_importance' in locals():\n",
    "        top_features = [feat for feat, _ in feature_importance[:30]]\n",
    "        \n",
    "        # Advanced analysis\n",
    "        analyze_feature_distributions(train_engineered, top_features)\n",
    "        correlation_heatmap(train_engineered, top_features)\n",
    "        high_importance, low_importance = feature_selection_recommendations(feature_importance)\n",
    "        create_feature_summary_report(train_engineered, feature_importance)\n",
    "        \n",
    "        # Prepare data for modeling\n",
    "        X_train, y_train = prepare_data_for_modeling(train_engineered)\n",
    "        X_test = prepare_data_for_modeling(test_engineered)\n",
    "        \n",
    "        # Create validation strategy\n",
    "        train_mask, val_mask = create_validation_strategy(train_engineered)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ¯ MODELING RECOMMENDATIONS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\"\"\n",
    "ğŸ“ˆ FEATURE ENGINEERING SUMMARY:\n",
    "- Total features created: {train_engineered.shape[1]}\n",
    "- High importance features: {len(high_importance)}\n",
    "- Ready for modeling: âœ…\n",
    "\n",
    "ğŸ¯ NEXT STEPS FOR MODELING:\n",
    "1. Start with top {min(50, len(high_importance))} features\n",
    "2. Use time-based validation (80/20 split by date)\n",
    "3. Consider ensemble methods (Random Forest, XGBoost, LightGBM)\n",
    "4. Focus on features with correlation > 0.01\n",
    "5. Monitor for overfitting with rolling/lag features\n",
    "\n",
    "ğŸ’¡ KEY INSIGHTS:\n",
    "- Seasonal patterns are strong (Q4 boost, holiday effects)\n",
    "- Store and department characteristics matter significantly\n",
    "- External factors show moderate correlation\n",
    "- Lag features capture important trends\n",
    "\n",
    "ğŸš€ RECOMMENDED MODELS TO TRY:\n",
    "1. LightGBM (handles missing values, fast)\n",
    "2. XGBoost (robust, good for time series)\n",
    "3. Random Forest (baseline, interpretable)\n",
    "4. Linear Regression (with regularization)\n",
    "5. Time series models (ARIMA, Prophet) for comparison\n",
    "        \"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… WALMART SALES FORECASTING EDA COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š Ready for modeling with comprehensive feature set!\")\n",
    "print(\"ğŸ¯ Use the engineered datasets for your machine learning pipeline!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
