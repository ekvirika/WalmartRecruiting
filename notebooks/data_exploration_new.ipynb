{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5606e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install wandb pandas numpy matplotlib seaborn scikit-learn mlflow statsmodels\n",
    "\n",
    "# Set up Kaggle API\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560494c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your kaggle.json to Colab and run:\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62ea3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
    "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed7b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q train.csv.zip\n",
    "!unzip -q stores.csv.zip\n",
    "!unzip -q test.csv.zip\n",
    "!unzip -q features.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23446b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the datasets\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "stores = pd.read_csv('stores.csv')\n",
    "features = pd.read_csv('features.csv')\n",
    "\n",
    "# Basic info about datasets\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "print(\"Stores shape:\", stores.shape)\n",
    "print(\"Features shape:\", features.shape)\n",
    "\n",
    "# Display first few rows\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c6292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "def explore_dataset(df, name):\n",
    "    print(f\"\\n=== {name} Dataset ===\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "    print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
    "    print(f\"\\nUnique values per column:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"{col}: {df[col].nunique()}\")\n",
    "\n",
    "explore_dataset(train, \"Train\")\n",
    "explore_dataset(test, \"Test\")\n",
    "explore_dataset(stores, \"Stores\")\n",
    "explore_dataset(features, \"Features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5abe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date column to datetime\n",
    "train['Date'] = pd.to_datetime(train['Date'])\n",
    "test['Date'] = pd.to_datetime(test['Date'])\n",
    "features['Date'] = pd.to_datetime(features['Date'])\n",
    "\n",
    "# Time range analysis\n",
    "print(\"Train date range:\", train['Date'].min(), \"to\", train['Date'].max())\n",
    "print(\"Test date range:\", test['Date'].min(), \"to\", test['Date'].max())\n",
    "print(\"Features date range:\", features['Date'].min(), \"to\", features['Date'].max())\n",
    "\n",
    "# Check for date gaps\n",
    "train_dates = sorted(train['Date'].unique())\n",
    "date_gaps = []\n",
    "for i in range(1, len(train_dates)):\n",
    "    gap = (train_dates[i] - train_dates[i-1]).days\n",
    "    if gap > 7:  # More than a week\n",
    "        date_gaps.append((train_dates[i-1], train_dates[i], gap))\n",
    "\n",
    "print(\"Date gaps found:\", len(date_gaps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b670acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall sales trends\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Aggregate weekly sales\n",
    "weekly_sales = train.groupby('Date')['Weekly_Sales'].sum().reset_index()\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(weekly_sales['Date'], weekly_sales['Weekly_Sales'])\n",
    "plt.title('Overall Weekly Sales Trend')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Sales distribution\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(train['Weekly_Sales'], bins=50, alpha=0.7)\n",
    "plt.title('Weekly Sales Distribution')\n",
    "plt.xlabel('Weekly Sales')\n",
    "\n",
    "# Sales by store\n",
    "plt.subplot(2, 2, 3)\n",
    "store_sales = train.groupby('Store')['Weekly_Sales'].sum().sort_values(ascending=False)\n",
    "plt.bar(range(len(store_sales)), store_sales.values)\n",
    "plt.title('Total Sales by Store')\n",
    "plt.xlabel('Store (ranked)')\n",
    "\n",
    "# Sales by department\n",
    "plt.subplot(2, 2, 4)\n",
    "dept_sales = train.groupby('Dept')['Weekly_Sales'].sum().sort_values(ascending=False).head(20)\n",
    "plt.bar(range(len(dept_sales)), dept_sales.values)\n",
    "plt.title('Top 20 Departments by Sales')\n",
    "plt.xlabel('Department (ranked)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb7483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time features\n",
    "train['Year'] = train['Date'].dt.year\n",
    "train['Month'] = train['Date'].dt.month\n",
    "train['Week'] = train['Date'].dt.isocalendar().week\n",
    "train['DayOfYear'] = train['Date'].dt.dayofyear\n",
    "\n",
    "# Merge with features to get holiday information\n",
    "train_features = train.merge(features, on=['Store', 'Date'], how='left')\n",
    "\n",
    "# Holiday impact analysis\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Monthly seasonality\n",
    "plt.subplot(2, 3, 1)\n",
    "monthly_sales = train.groupby('Month')['Weekly_Sales'].mean()\n",
    "plt.plot(monthly_sales.index, monthly_sales.values, marker='o')\n",
    "plt.title('Average Sales by Month')\n",
    "plt.xlabel('Month')\n",
    "\n",
    "# Holiday vs Non-holiday sales\n",
    "plt.subplot(2, 3, 2)\n",
    "holiday_sales = train_features.groupby('IsHoliday')['Weekly_Sales'].mean()\n",
    "plt.bar(['Non-Holiday', 'Holiday'], holiday_sales.values)\n",
    "plt.title('Average Sales: Holiday vs Non-Holiday')\n",
    "\n",
    "# Year-over-year comparison\n",
    "plt.subplot(2, 3, 3)\n",
    "yearly_sales = train.groupby(['Year', 'Month'])['Weekly_Sales'].sum().unstack(level=0)\n",
    "for year in yearly_sales.columns:\n",
    "    plt.plot(yearly_sales.index, yearly_sales[year], marker='o', label=f'Year {year}')\n",
    "plt.title('Monthly Sales by Year')\n",
    "plt.legend()\n",
    "\n",
    "# Weekly patterns\n",
    "plt.subplot(2, 3, 4)\n",
    "weekly_pattern = train.groupby('Week')['Weekly_Sales'].mean()\n",
    "plt.plot(weekly_pattern.index, weekly_pattern.values)\n",
    "plt.title('Average Sales by Week of Year')\n",
    "plt.xlabel('Week')\n",
    "\n",
    "# Temperature impact (if available)\n",
    "if 'Temperature' in train_features.columns:\n",
    "    plt.subplot(2, 3, 5)\n",
    "    temp_sales = train_features.groupby(pd.cut(train_features['Temperature'], bins=10))['Weekly_Sales'].mean()\n",
    "    plt.plot(range(len(temp_sales)), temp_sales.values)\n",
    "    plt.title('Sales vs Temperature')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eded52b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with store information\n",
    "train_stores = train.merge(stores, on='Store', how='left')\n",
    "\n",
    "# Store type analysis\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "type_sales = train_stores.groupby('Type')['Weekly_Sales'].mean()\n",
    "plt.bar(type_sales.index, type_sales.values)\n",
    "plt.title('Average Sales by Store Type')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "size_sales = train_stores.groupby(pd.cut(train_stores['Size'], bins=5))['Weekly_Sales'].mean()\n",
    "plt.plot(range(len(size_sales)), size_sales.values, marker='o')\n",
    "plt.title('Sales vs Store Size')\n",
    "\n",
    "# Department performance variability\n",
    "plt.subplot(1, 3, 3)\n",
    "dept_variability = train.groupby('Dept')['Weekly_Sales'].std().sort_values(ascending=False).head(20)\n",
    "plt.bar(range(len(dept_variability)), dept_variability.values)\n",
    "plt.title('Top 20 Most Variable Departments')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc43d532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative sales analysis\n",
    "negative_sales = train[train['Weekly_Sales'] < 0]\n",
    "print(f\"Negative sales records: {len(negative_sales)} ({len(negative_sales)/len(train)*100:.2f}%)\")\n",
    "\n",
    "if len(negative_sales) > 0:\n",
    "    print(\"Negative sales by department:\")\n",
    "    print(negative_sales.groupby('Dept')['Weekly_Sales'].count().sort_values(ascending=False).head(10))\n",
    "\n",
    "# Zero sales analysis\n",
    "zero_sales = train[train['Weekly_Sales'] == 0]\n",
    "print(f\"Zero sales records: {len(zero_sales)} ({len(zero_sales)/len(train)*100:.2f}%)\")\n",
    "\n",
    "# Missing data patterns in features\n",
    "if 'MarkDown1' in features.columns:\n",
    "    markdown_cols = [col for col in features.columns if 'MarkDown' in col]\n",
    "    markdown_missing = features[markdown_cols].isnull().sum()\n",
    "    print(\"Missing markdown data:\")\n",
    "    print(markdown_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc0b327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with external factors\n",
    "if len(train_features.columns) > len(train.columns):\n",
    "    numeric_cols = train_features.select_dtypes(include=[np.number]).columns\n",
    "    correlation_matrix = train_features[numeric_cols].corr()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Focus on Weekly_Sales correlations\n",
    "    sales_corr = correlation_matrix['Weekly_Sales'].sort_values(ascending=False)\n",
    "    print(\"Correlations with Weekly_Sales:\")\n",
    "    print(sales_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc87c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Select a specific store-department combination for detailed analysis\n",
    "sample_data = train[(train['Store'] == 1) & (train['Dept'] == 1)].copy()\n",
    "sample_data = sample_data.set_index('Date').sort_index()\n",
    "\n",
    "if len(sample_data) > 52:  # Need enough data points\n",
    "    decomposition = seasonal_decompose(sample_data['Weekly_Sales'], \n",
    "                                     model='additive', period=52)\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "    decomposition.observed.plot(ax=axes[0], title='Original')\n",
    "    decomposition.trend.plot(ax=axes[1], title='Trend')\n",
    "    decomposition.seasonal.plot(ax=axes[2], title='Seasonal')\n",
    "    decomposition.resid.plot(ax=axes[3], title='Residual')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab33fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_insights_summary(train, stores, features):\n",
    "    insights = {}\n",
    "    \n",
    "    # Basic statistics\n",
    "    insights['total_stores'] = train['Store'].nunique()\n",
    "    insights['total_departments'] = train['Dept'].nunique()\n",
    "    insights['date_range'] = (train['Date'].min(), train['Date'].max())\n",
    "    insights['total_weeks'] = train['Date'].nunique()\n",
    "    \n",
    "    # Sales statistics\n",
    "    insights['avg_weekly_sales'] = train['Weekly_Sales'].mean()\n",
    "    insights['total_sales'] = train['Weekly_Sales'].sum()\n",
    "    insights['negative_sales_pct'] = (train['Weekly_Sales'] < 0).mean() * 100\n",
    "    \n",
    "    # Store insights\n",
    "    if not stores.empty:\n",
    "        insights['store_types'] = stores['Type'].value_counts().to_dict()\n",
    "        insights['avg_store_size'] = stores['Size'].mean()\n",
    "    \n",
    "    return insights\n",
    "\n",
    "insights = generate_insights_summary(train, stores, features)\n",
    "for key, value in insights.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
