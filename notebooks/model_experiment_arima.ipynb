{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekvirika/WalmartRecruiting/blob/main/notebooks/model_experiment_arima.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01e95788",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "01e95788",
        "outputId": "6391d2bb-fd01-4296-f88c-f70a2cc123e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7b6380b",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "d7b6380b",
        "outputId": "47f6d201-019e-40d7-a133-540d7d694bd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting mlflow\n",
            "  Downloading mlflow-3.1.4-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting mlflow-skinny==3.1.4 (from mlflow)\n",
            "  Downloading mlflow_skinny-3.1.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.41)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.1.4->mlflow)\n",
            "  Downloading databricks_sdk-0.61.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.116.1)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (8.7.0)\n",
            "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.1.4->mlflow)\n",
            "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.1.4->mlflow)\n",
            "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.5.3)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.35.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==3.1.4->mlflow) (0.47.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.4->mlflow) (3.23.0)\n",
            "Collecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.4->mlflow)\n",
            "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==3.1.4->mlflow) (0.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.4->mlflow) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.4->mlflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (0.6.1)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow-3.1.4-py3-none-any.whl (24.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-3.1.4-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.61.0-py3-none-any.whl (680 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m680.6/680.6 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gunicorn, graphql-core, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, graphql-relay, docker, alembic, opentelemetry-semantic-conventions, nvidia-cusolver-cu12, graphene, databricks-sdk, opentelemetry-sdk, mlflow-skinny, mlflow\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed alembic-1.16.4 databricks-sdk-0.61.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-3.1.4 mlflow-skinny-3.1.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 opentelemetry-api-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.7.14)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install wandb torch torchvision pandas matplotlib seaborn scikit-learn mlflow\n",
        "\n",
        "# Set up Kaggle API\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa958d7c",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "aa958d7c"
      },
      "outputs": [],
      "source": [
        "# Upload your kaggle.json to Colab and run:\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3fd3e2d",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "e3fd3e2d",
        "outputId": "0dd50443-3528-4372-d617-f8e0632198b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 387MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "668fa338",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "668fa338",
        "outputId": "6a177f91-99aa-4868-f970-5b820c44edc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open stores.csv.zip, stores.csv.zip.zip or stores.csv.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!unzip -q train.csv.zip\n",
        "!unzip -q stores.csv.zip\n",
        "!unzip -q test.csv.zip\n",
        "!unzip -q features.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dagshub mlflow --quiet\n",
        "!pip install optuna\n",
        "!pip install pmdarima\n",
        "# !pip install --upgrade numpy scipy statsmodels pmdarima"
      ],
      "metadata": {
        "id": "HtY6QbhLsAFs",
        "outputId": "789c8b3d-d63e-4e9d-ae14-e77a09358eca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HtY6QbhLsAFs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.2/261.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.4)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.9.0 optuna-4.4.0\n",
            "Collecting pmdarima\n",
            "  Downloading pmdarima-2.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (1.5.1)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (1.16.0)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (0.14.5)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (2.5.0)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (75.2.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.11/dist-packages (from pmdarima) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pmdarima) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pmdarima) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pmdarima) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->pmdarima) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.13.2->pmdarima) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.19->pmdarima) (1.17.0)\n",
            "Downloading pmdarima-2.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pmdarima\n",
            "Successfully installed pmdarima-2.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/ekvirika/WalmartRecruiting.git"
      ],
      "metadata": {
        "id": "IjES_zkFkybH",
        "collapsed": true
      },
      "id": "IjES_zkFkybH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd WalmartRecruiting/\n"
      ],
      "metadata": {
        "id": "nDqnDPn-k6w2",
        "collapsed": true
      },
      "id": "nDqnDPn-k6w2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removed sys.path.append(\"content/WalmartRecruiting\") as it is no longer necessary after changing the working directory."
      ],
      "metadata": {
        "id": "gum7ES_7mDJE"
      },
      "id": "gum7ES_7mDJE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from helpers.data_loading import WalmartDataLoader\n",
        "# from helpers.data_preprocessing import WalmartDataPreprocessor\n",
        "# from helpers.mlflow_logger import MLflowDagsHubLogger"
      ],
      "metadata": {
        "id": "I3UXcHuxk80h",
        "collapsed": true
      },
      "id": "I3UXcHuxk80h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"\n",
        "# MLflow Configuration with DagsHub Integration\n",
        "# For Walmart Sales Forecasting Project\n",
        "# \"\"\"\n",
        "\n",
        "# import mlflow\n",
        "# import mlflow.sklearn\n",
        "# import mlflow.pytorch\n",
        "# import dagshub\n",
        "# import os\n",
        "# from typing import Dict, Any, Optional\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "# import joblib\n",
        "# import tempfile\n",
        "\n",
        "# class MLflowDagsHubLogger:\n",
        "#     \"\"\"\n",
        "#     Comprehensive MLflow logger with DagsHub integration for time series forecasting\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self,\n",
        "#                  dagshub_repo_owner: str,\n",
        "#                  dagshub_repo_name: str,\n",
        "#                  dagshub_token: str = None):\n",
        "#         \"\"\"\n",
        "#         Initialize MLflow with DagsHub integration\n",
        "\n",
        "#         Args:\n",
        "#             dagshub_repo_owner: Your DagsHub username\n",
        "#             dagshub_repo_name: Repository name on DagsHub\n",
        "#             dagshub_token: DagsHub authentication token\n",
        "#         \"\"\"\n",
        "#         self.dagshub_repo_owner = dagshub_repo_owner\n",
        "#         self.dagshub_repo_name = dagshub_repo_name\n",
        "\n",
        "#         # Set up DagsHub\n",
        "#         dagshub.init(repo_owner=dagshub_repo_owner,\n",
        "#                     repo_name=dagshub_repo_name,\n",
        "#                     mlflow=True)\n",
        "\n",
        "#         # Set MLflow tracking URI to DagsHub\n",
        "#         mlflow_tracking_uri = f\"https://dagshub.com/{dagshub_repo_owner}/{dagshub_repo_name}.mlflow\"\n",
        "#         mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
        "\n",
        "#         # Set authentication if token provided\n",
        "#         if dagshub_token:\n",
        "#             os.environ['MLFLOW_TRACKING_USERNAME'] = dagshub_token\n",
        "#             os.environ['MLFLOW_TRACKING_PASSWORD'] = \"\"\n",
        "\n",
        "#         self.current_experiment = None\n",
        "#         self.current_run = None\n",
        "\n",
        "#     def create_experiment(self, experiment_name: str) -> str:\n",
        "#         \"\"\"\n",
        "#         Create or get existing MLflow experiment\n",
        "\n",
        "#         Args:\n",
        "#             experiment_name: Name of the experiment\n",
        "\n",
        "#         Returns:\n",
        "#             experiment_id: ID of the created/existing experiment\n",
        "#         \"\"\"\n",
        "#         try:\n",
        "#             experiment_id = mlflow.create_experiment(experiment_name)\n",
        "#         except mlflow.exceptions.MlflowException:\n",
        "#             # Experiment already exists\n",
        "#             experiment = mlflow.get_experiment_by_name(experiment_name)\n",
        "#             experiment_id = experiment.experiment_id\n",
        "\n",
        "#         self.current_experiment = experiment_name\n",
        "#         mlflow.set_experiment(experiment_name)\n",
        "#         return experiment_id\n",
        "\n",
        "#     def start_run(self, run_name: str, nested: bool = False) -> mlflow.ActiveRun:\n",
        "#         \"\"\"\n",
        "#         Start a new MLflow run\n",
        "\n",
        "#         Args:\n",
        "#             run_name: Name of the run\n",
        "#             nested: Whether this is a nested run\n",
        "\n",
        "#         Returns:\n",
        "#             Active MLflow run\n",
        "#         \"\"\"\n",
        "#         self.current_run = mlflow.start_run(run_name=run_name, nested=nested)\n",
        "#         return self.current_run\n",
        "\n",
        "#     def log_preprocessing_step(self,\n",
        "#                               step_name: str,\n",
        "#                               input_shape: tuple,\n",
        "#                               output_shape: tuple,\n",
        "#                               preprocessing_params: Dict[str, Any] = None,\n",
        "#                               data_quality_metrics: Dict[str, Any] = None):\n",
        "#         \"\"\"\n",
        "#         Log preprocessing step information\n",
        "\n",
        "#         Args:\n",
        "#             step_name: Name of preprocessing step\n",
        "#             input_shape: Shape of input data\n",
        "#             output_shape: Shape of output data\n",
        "#             preprocessing_params: Parameters used in preprocessing\n",
        "#             data_quality_metrics: Data quality metrics\n",
        "#         \"\"\"\n",
        "#         with mlflow.start_run(run_name=f\"{step_name}_preprocessing\", nested=True):\n",
        "#             # Log shapes\n",
        "#             mlflow.log_param(\"input_rows\", input_shape[0])\n",
        "#             mlflow.log_param(\"input_cols\", input_shape[1] if len(input_shape) > 1 else 1)\n",
        "#             mlflow.log_param(\"output_rows\", output_shape[0])\n",
        "#             mlflow.log_param(\"output_cols\", output_shape[1] if len(output_shape) > 1 else 1)\n",
        "\n",
        "#             # Log preprocessing parameters\n",
        "#             if preprocessing_params:\n",
        "#                 for key, value in preprocessing_params.items():\n",
        "#                     mlflow.log_param(f\"preprocess_{key}\", value)\n",
        "\n",
        "#             # Log data quality metrics\n",
        "#             if data_quality_metrics:\n",
        "#                 for key, value in data_quality_metrics.items():\n",
        "#                     mlflow.log_metric(f\"data_quality_{key}\", value)\n",
        "\n",
        "#             # Log step completion\n",
        "#             mlflow.log_metric(\"preprocessing_completed\", 1)\n",
        "\n",
        "#     def log_feature_engineering(self,\n",
        "#                                features_before: list,\n",
        "#                                features_after: list,\n",
        "#                                feature_importance: Dict[str, float] = None,\n",
        "#                                feature_selection_method: str = None):\n",
        "#         \"\"\"\n",
        "#         Log feature engineering information\n",
        "\n",
        "#         Args:\n",
        "#             features_before: List of features before engineering\n",
        "#             features_after: List of features after engineering\n",
        "#             feature_importance: Feature importance scores\n",
        "#             feature_selection_method: Method used for feature selection\n",
        "#         \"\"\"\n",
        "#         with mlflow.start_run(run_name=\"feature_engineering\", nested=True):\n",
        "#             # Log feature counts\n",
        "#             mlflow.log_param(\"features_before_count\", len(features_before))\n",
        "#             mlflow.log_param(\"features_after_count\", len(features_after))\n",
        "\n",
        "#             # Log feature names\n",
        "#             mlflow.log_param(\"features_before\", str(features_before[:20]))  # Limit to first 20\n",
        "#             mlflow.log_param(\"features_after\", str(features_after[:20]))   # Limit to first 20\n",
        "\n",
        "#             # Log feature selection method\n",
        "#             if feature_selection_method:\n",
        "#                 mlflow.log_param(\"feature_selection_method\", feature_selection_method)\n",
        "\n",
        "#             # Log feature importance\n",
        "#             if feature_importance:\n",
        "#                 for feature, importance in list(feature_importance.items())[:20]:  # Top 20\n",
        "#                     mlflow.log_metric(f\"feature_importance_{feature}\", importance)\n",
        "\n",
        "#     def log_model_training(self,\n",
        "#                           model,\n",
        "#                           model_params: Dict[str, Any],\n",
        "#                           training_metrics: Dict[str, float],\n",
        "#                           validation_metrics: Dict[str, float],\n",
        "#                           model_name: str,\n",
        "#                           tags: Dict[str, str] = None):\n",
        "#         \"\"\"\n",
        "#         Log model training information\n",
        "\n",
        "#         Args:\n",
        "#             model: Trained model object\n",
        "#             model_params: Model hyperparameters\n",
        "#             training_metrics: Training metrics\n",
        "#             validation_metrics: Validation metrics\n",
        "#             model_name: Name of the model\n",
        "#             tags: Additional tags for the run\n",
        "#         \"\"\"\n",
        "#         # Log parameters\n",
        "#         for key, value in model_params.items():\n",
        "#             mlflow.log_param(key, value)\n",
        "\n",
        "#         # Log training metrics\n",
        "#         for key, value in training_metrics.items():\n",
        "#             mlflow.log_metric(f\"train_{key}\", value)\n",
        "\n",
        "#         # Log validation metrics\n",
        "#         for key, value in validation_metrics.items():\n",
        "#             mlflow.log_metric(f\"val_{key}\", value)\n",
        "\n",
        "#         # Log model\n",
        "#         if hasattr(model, 'predict'):\n",
        "#             if isinstance(model, Pipeline):\n",
        "#                 mlflow.sklearn.log_model(model, model_name)\n",
        "#             else:\n",
        "#                 # Handle different model types\n",
        "#                 if hasattr(model, '__module__'):\n",
        "#                     if 'torch' in model.__module__:\n",
        "#                         mlflow.pytorch.log_model(model, model_name)\n",
        "#                     elif 'sklearn' in model.__module__:\n",
        "#                         mlflow.sklearn.log_model(model, model_name)\n",
        "#                     else:\n",
        "#                         # Generic model logging\n",
        "#                         with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as tmp:\n",
        "#                             joblib.dump(model, tmp.name)\n",
        "#                             mlflow.log_artifact(tmp.name, f\"{model_name}.pkl\")\n",
        "\n",
        "#         # Log tags\n",
        "#         if tags:\n",
        "#             mlflow.set_tags(tags)\n",
        "\n",
        "#     def log_cross_validation(self,\n",
        "#                            cv_scores: Dict[str, list],\n",
        "#                            cv_method: str,\n",
        "#                            n_folds: int):\n",
        "#         \"\"\"\n",
        "#         Log cross-validation results\n",
        "\n",
        "#         Args:\n",
        "#             cv_scores: Dictionary of CV scores for each metric\n",
        "#             cv_method: Cross-validation method used\n",
        "#             n_folds: Number of folds\n",
        "#         \"\"\"\n",
        "#         with mlflow.start_run(run_name=\"cross_validation\", nested=True):\n",
        "#             mlflow.log_param(\"cv_method\", cv_method)\n",
        "#             mlflow.log_param(\"n_folds\", n_folds)\n",
        "\n",
        "#             for metric_name, scores in cv_scores.items():\n",
        "#                 mlflow.log_metric(f\"cv_{metric_name}_mean\", np.mean(scores))\n",
        "#                 mlflow.log_metric(f\"cv_{metric_name}_std\", np.std(scores))\n",
        "#                 mlflow.log_metric(f\"cv_{metric_name}_min\", np.min(scores))\n",
        "#                 mlflow.log_metric(f\"cv_{metric_name}_max\", np.max(scores))\n",
        "\n",
        "#                 # Log individual fold scores\n",
        "#                 for i, score in enumerate(scores):\n",
        "#                     mlflow.log_metric(f\"cv_{metric_name}_fold_{i+1}\", score)\n",
        "\n",
        "#     def log_hyperparameter_tuning(self,\n",
        "#                                  best_params: Dict[str, Any],\n",
        "#                                  best_score: float,\n",
        "#                                  tuning_method: str,\n",
        "#                                  search_space: Dict[str, Any] = None,\n",
        "#                                  n_trials: int = None):\n",
        "#         \"\"\"\n",
        "#         Log hyperparameter tuning results\n",
        "\n",
        "#         Args:\n",
        "#             best_params: Best parameters found\n",
        "#             best_score: Best score achieved\n",
        "#             tuning_method: Method used for tuning\n",
        "#             search_space: Search space definition\n",
        "#             n_trials: Number of trials performed\n",
        "#         \"\"\"\n",
        "#         with mlflow.start_run(run_name=\"hyperparameter_tuning\", nested=True):\n",
        "#             mlflow.log_param(\"tuning_method\", tuning_method)\n",
        "#             if n_trials:\n",
        "#                 mlflow.log_param(\"n_trials\", n_trials)\n",
        "\n",
        "#             # Log best parameters\n",
        "#             for key, value in best_params.items():\n",
        "#                 mlflow.log_param(f\"best_{key}\", value)\n",
        "\n",
        "#             mlflow.log_metric(\"best_score\", best_score)\n",
        "\n",
        "#             # Log search space\n",
        "#             if search_space:\n",
        "#                 for key, value in search_space.items():\n",
        "#                     mlflow.log_param(f\"search_space_{key}\", str(value))\n",
        "\n",
        "#     def log_time_series_metrics(self,\n",
        "#                                y_true: np.ndarray,\n",
        "#                                y_pred: np.ndarray,\n",
        "#                                is_holiday: np.ndarray = None,\n",
        "#                                prefix: str = \"\"):\n",
        "#         \"\"\"\n",
        "#         Log time series specific metrics\n",
        "\n",
        "#         Args:\n",
        "#             y_true: True values\n",
        "#             y_pred: Predicted values\n",
        "#             is_holiday: Holiday indicator for WMAE calculation\n",
        "#             prefix: Prefix for metric names\n",
        "#         \"\"\"\n",
        "#         prefix = f\"{prefix}_\" if prefix else \"\"\n",
        "\n",
        "#         # Standard metrics\n",
        "#         mae = mean_absolute_error(y_true, y_pred)\n",
        "#         mse = mean_squared_error(y_true, y_pred)\n",
        "#         rmse = np.sqrt(mse)\n",
        "\n",
        "#         mlflow.log_metric(f\"{prefix}mae\", mae)\n",
        "#         mlflow.log_metric(f\"{prefix}mse\", mse)\n",
        "#         mlflow.log_metric(f\"{prefix}rmse\", rmse)\n",
        "\n",
        "#         # MAPE (Mean Absolute Percentage Error)\n",
        "#         mape = np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1))) * 100\n",
        "#         mlflow.log_metric(f\"{prefix}mape\", mape)\n",
        "\n",
        "#         # WMAE (Weighted Mean Absolute Error) if holiday data available\n",
        "#         if is_holiday is not None:\n",
        "#             weights = np.where(is_holiday, 5, 1)\n",
        "#             wmae = np.average(np.abs(y_true - y_pred), weights=weights)\n",
        "#             mlflow.log_metric(f\"{prefix}wmae\", wmae)\n",
        "\n",
        "#     def register_best_model(self,\n",
        "#                            model_name: str,\n",
        "#                            model_version: str,\n",
        "#                            model_stage: str = \"Staging\"):\n",
        "#         \"\"\"\n",
        "#         Register the best model in MLflow Model Registry\n",
        "\n",
        "#         Args:\n",
        "#             model_name: Name for the registered model\n",
        "#             model_version: Version of the model\n",
        "#             model_stage: Stage of the model (Staging, Production, etc.)\n",
        "#         \"\"\"\n",
        "#         try:\n",
        "#             # Create registered model\n",
        "#             mlflow.register_model(\n",
        "#                 model_uri=f\"runs:/{self.current_run.info.run_id}/{model_name}\",\n",
        "#                 name=model_name\n",
        "#             )\n",
        "\n",
        "#             # Transition to specified stage\n",
        "#             client = mlflow.tracking.MlflowClient()\n",
        "#             client.transition_model_version_stage(\n",
        "#                 name=model_name,\n",
        "#                 version=model_version,\n",
        "#                 stage=model_stage\n",
        "#             )\n",
        "\n",
        "#             print(f\"Model {model_name} version {model_version} registered and moved to {model_stage}\")\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error registering model: {e}\")\n",
        "\n",
        "#     def end_run(self):\n",
        "#         \"\"\"End the current MLflow run\"\"\"\n",
        "#         if self.current_run:\n",
        "#             mlflow.end_run()\n",
        "#             self.current_run = None\n",
        "\n",
        "# # Utility function for WMAE calculation\n",
        "# def calculate_wmae(y_true: np.ndarray, y_pred: np.ndarray, is_holiday: np.ndarray) -> float:\n",
        "#     \"\"\"\n",
        "#     Calculate Weighted Mean Absolute Error\n",
        "\n",
        "#     Args:\n",
        "#         y_true: True values\n",
        "#         y_pred: Predicted values\n",
        "#         is_holiday: Holiday indicator (1 for holiday, 0 for non-holiday)\n",
        "\n",
        "#     Returns:\n",
        "#         WMAE score\n",
        "#     \"\"\"\n",
        "#     weights = np.where(is_holiday, 5, 1)\n",
        "#     return np.average(np.abs(y_true - y_pred), weights=weights)\n",
        "\n",
        "# # Example usage configuration\n",
        "# def setup_mlflow_logging(dagshub_repo_owner: str,\n",
        "#                         dagshub_repo_name: str,\n",
        "#                         dagshub_token: str = None) -> MLflowDagsHubLogger:\n",
        "#     \"\"\"\n",
        "#     Setup MLflow logging with DagsHub\n",
        "\n",
        "#     Args:\n",
        "#         dagshub_repo_owner: Your DagsHub username\n",
        "#         dagshub_repo_name: Repository name\n",
        "#         dagshub_token: Authentication token\n",
        "\n",
        "#     Returns:\n",
        "#         Configured logger instance\n",
        "#     \"\"\"\n",
        "#     logger = MLflowDagsHubLogger(\n",
        "#         dagshub_repo_owner=dagshub_repo_owner,\n",
        "#         dagshub_repo_name=dagshub_repo_name,\n",
        "#         dagshub_token=dagshub_token\n",
        "#     )\n",
        "\n",
        "#     return logger"
      ],
      "metadata": {
        "id": "8pRIUDqUinsl"
      },
      "id": "8pRIUDqUinsl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # model_experiment_ARIMA.ipynb\n",
        "# \"\"\"\n",
        "# ARIMA Model Experiment with Comprehensive MLflow Logging\n",
        "# Walmart Sales Forecasting Project\n",
        "# \"\"\"\n",
        "\n",
        "# # =============================================================================\n",
        "# # SETUP AND IMPORTS\n",
        "# # =============================================================================\n",
        "\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# import mlflow\n",
        "# import mlflow.sklearn\n",
        "# from datetime import datetime\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "# # Import your existing modules\n",
        "# from src import data_loader, processing\n",
        "# from feature_engineering import feature_transformers, time_features\n",
        "# from feature_engineering.encoders import CustomOneHotEncoder\n",
        "# from models import store_dept_sarimax\n",
        "# from src.utils import wmae as compute_wmae\n",
        "\n",
        "# # Import our MLflow configuration\n",
        "# from mlflow_dagshub_config import MLflowDagsHubLogger, calculate_wmae, setup_mlflow_logging\n",
        "\n",
        "# # =============================================================================\n",
        "# # MLFLOW SETUP WITH DAGSHUB\n",
        "# # =============================================================================\n",
        "\n",
        "# # TODO: Replace with your actual DagsHub credentials\n",
        "# DAGSHUB_REPO_OWNER = \"ekvirika\"  # Your DagsHub username\n",
        "# DAGSHUB_REPO_NAME = \"WalmartRecruiting\"  # Your repository name\n",
        "# DAGSHUB_TOKEN = \"0adb1004ddd4221395353efea2d8ead625e26197\"  # Your DagsHub token (optional, can use env var)\n",
        "\n",
        "# # Initialize MLflow logger\n",
        "# logger = setup_mlflow_logging(\n",
        "#     dagshub_repo_owner=DAGSHUB_REPO_OWNER,\n",
        "#     dagshub_repo_name=DAGSHUB_REPO_NAME,\n",
        "#     dagshub_token=DAGSHUB_TOKEN\n",
        "# )\n",
        "\n",
        "# # Create experiment for ARIMA models\n",
        "# experiment_id = logger.create_experiment(\"ARIMA_Training\")\n",
        "\n",
        "# # =============================================================================\n",
        "# # DATA LOADING AND INITIAL PREPROCESSING\n",
        "# # =============================================================================\n",
        "\n",
        "# def load_and_preprocess_data():\n",
        "#     \"\"\"Load and preprocess the data with logging\"\"\"\n",
        "\n",
        "#     with logger.start_run(\"ARIMA_Data_Loading\"):\n",
        "#         # Load raw data\n",
        "#         dataframes = data_loader.load_raw_data()\n",
        "#         df = processing.run_preprocessing(dataframes, process_test=False)['train']\n",
        "\n",
        "#         # Log data loading metrics\n",
        "#         logger.log_preprocessing_step(\n",
        "#             step_name=\"data_loading\",\n",
        "#             input_shape=(len(df), len(df.columns)),\n",
        "#             output_shape=(len(df), len(df.columns)),\n",
        "#             data_quality_metrics={\n",
        "#                 \"total_rows\": len(df),\n",
        "#                 \"total_columns\": len(df.columns),\n",
        "#                 \"missing_values_ratio\": df.isnull().sum().sum() / (len(df) * len(df.columns)),\n",
        "#                 \"unique_stores\": df['Store'].nunique(),\n",
        "#                 \"unique_departments\": df['Dept'].nunique(),\n",
        "#                 \"date_range_days\": (df['Date'].max() - df['Date'].min()).days\n",
        "#             }\n",
        "#         )\n",
        "\n",
        "#         # Split data\n",
        "#         X_train, y_train, X_valid, y_valid = processing.split_data(df, separate_target=True)\n",
        "\n",
        "#         # Log split information\n",
        "#         logger.log_preprocessing_step(\n",
        "#             step_name=\"train_test_split\",\n",
        "#             input_shape=(len(df), len(df.columns)),\n",
        "#             output_shape=(len(X_train) + len(X_valid), len(X_train.columns)),\n",
        "#             preprocessing_params={\n",
        "#                 \"train_size\": len(X_train),\n",
        "#                 \"valid_size\": len(X_valid),\n",
        "#                 \"split_ratio\": len(X_train) / (len(X_train) + len(X_valid))\n",
        "#             }\n",
        "#         )\n",
        "\n",
        "#         mlflow.log_metric(\"data_loading_completed\", 1)\n",
        "\n",
        "#     return X_train, y_train, X_valid, y_valid\n",
        "\n",
        "# # =============================================================================\n",
        "# # FEATURE ENGINEERING\n",
        "# # =============================================================================\n",
        "\n",
        "# def create_preprocessing_pipeline():\n",
        "#     \"\"\"Create and log preprocessing pipeline\"\"\"\n",
        "\n",
        "#     with logger.start_run(\"ARIMA_Feature_Engineering\"):\n",
        "#         # Define preprocessing steps\n",
        "#         columns_to_drop = [\n",
        "#             'MarkDown1', 'MarkDown2', 'MarkDown3',\n",
        "#             'MarkDown4', 'MarkDown5'\n",
        "#         ]\n",
        "\n",
        "#         time_feature_params = {\n",
        "#             'add_week_num': True,\n",
        "#             'add_holiday_flags': False,\n",
        "#             'add_holiday_proximity': True,\n",
        "#             'add_holiday_windows': False,\n",
        "#             'add_fourier_features': True,\n",
        "#             'add_month_and_year': True,\n",
        "#             'replace_time_index': False,\n",
        "#             'list_of_holiday_proximity': [],\n",
        "#         }\n",
        "\n",
        "#         # Create pipeline\n",
        "#         preprocess_pipeline = Pipeline(steps=[\n",
        "#             ('drop_markdown', feature_transformers.ChangeColumns(columns_to_drop=columns_to_drop)),\n",
        "#             ('bool_to_int', feature_transformers.BoolToInt()),\n",
        "#             ('type_encoding', CustomOneHotEncoder(columns=['Type'])),\n",
        "#             ('add_time_features', time_features.FeatureAdder(**time_feature_params)),\n",
        "#         ])\n",
        "\n",
        "#         # Log preprocessing parameters\n",
        "#         preprocessing_params = {\n",
        "#             \"columns_dropped\": len(columns_to_drop),\n",
        "#             \"boolean_conversion\": True,\n",
        "#             \"one_hot_encoding\": True,\n",
        "#             \"time_features_added\": len([k for k, v in time_feature_params.items() if v and isinstance(v, bool)])\n",
        "#         }\n",
        "\n",
        "#         for key, value in preprocessing_params.items():\n",
        "#             mlflow.log_param(f\"preprocessing_{key}\", value)\n",
        "\n",
        "#         for key, value in time_feature_params.items():\n",
        "#             mlflow.log_param(f\"time_feature_{key}\", value)\n",
        "\n",
        "#         mlflow.log_metric(\"feature_engineering_completed\", 1)\n",
        "\n",
        "#     return preprocess_pipeline\n",
        "\n",
        "# def apply_feature_engineering(X_train, X_valid, preprocess_pipeline):\n",
        "#     \"\"\"Apply feature engineering with logging\"\"\"\n",
        "\n",
        "#     with logger.start_run(\"ARIMA_Feature_Application\"):\n",
        "#         # Get original features\n",
        "#         original_features = X_train.columns.tolist()\n",
        "\n",
        "#         # Apply preprocessing\n",
        "#         X_train_processed = preprocess_pipeline.fit_transform(X_train)\n",
        "#         X_valid_processed = preprocess_pipeline.transform(X_valid)\n",
        "\n",
        "#         # Get new features\n",
        "#         new_features = X_train_processed.columns.tolist()\n",
        "\n",
        "#         # Log feature engineering\n",
        "#         logger.log_feature_engineering(\n",
        "#             features_before=original_features,\n",
        "#             features_after=new_features,\n",
        "#             feature_selection_method=\"manual_selection_and_engineering\"\n",
        "#         )\n",
        "\n",
        "#         # Log shape changes\n",
        "#         logger.log_preprocessing_step(\n",
        "#             step_name=\"feature_transformation\",\n",
        "#             input_shape=X_train.shape,\n",
        "#             output_shape=X_train_processed.shape,\n",
        "#             preprocessing_params={\n",
        "#                 \"features_added\": len(new_features) - len(original_features),\n",
        "#                 \"transformation_steps\": len(preprocess_pipeline.steps)\n",
        "#             }\n",
        "#         )\n",
        "\n",
        "#         mlflow.log_metric(\"feature_application_completed\", 1)\n",
        "\n",
        "#     return X_train_processed, X_valid_processed\n",
        "\n",
        "# # =============================================================================\n",
        "# # SUBSET SELECTION FOR EFFICIENT TRAINING\n",
        "# # =============================================================================\n",
        "\n",
        "# def select_subset_for_training(X_train, X_valid, y_train, y_valid, n_combinations=100):\n",
        "#     \"\"\"Select subset of store-department combinations for training\"\"\"\n",
        "\n",
        "#     with logger.start_run(\"ARIMA_Subset_Selection\"):\n",
        "#         # Reset indices\n",
        "#         X_train_reset = X_train.reset_index(drop=True)\n",
        "#         X_valid_reset = X_valid.reset_index(drop=True)\n",
        "#         y_train_reset = y_train.reset_index(drop=True)\n",
        "#         y_valid_reset = y_valid.reset_index(drop=True)\n",
        "\n",
        "#         # Select subset\n",
        "#         subset_keys = X_train_reset[['Store', 'Dept']].drop_duplicates().sample(\n",
        "#             n=n_combinations, random_state=42\n",
        "#         )\n",
        "\n",
        "#         # Filter data\n",
        "#         X_train_subset = pd.merge(X_train_reset, subset_keys, on=['Store', 'Dept'], how='inner')\n",
        "#         X_valid_subset = pd.merge(X_valid_reset, subset_keys, on=['Store', 'Dept'], how='inner')\n",
        "\n",
        "#         y_train_subset = y_train_reset.iloc[X_train_subset.index]\n",
        "#         y_valid_subset = y_valid_reset.iloc[X_valid_subset.index]\n",
        "\n",
        "#         # Log subset selection parameters\n",
        "#         mlflow.log_param(\"subset_combinations\", n_combinations)\n",
        "#         mlflow.log_param(\"original_train_size\", len(X_train_reset))\n",
        "#         mlflow.log_param(\"subset_train_size\", len(X_train_subset))\n",
        "#         mlflow.log_param(\"original_valid_size\", len(X_valid_reset))\n",
        "#         mlflow.log_param(\"subset_valid_size\", len(X_valid_subset))\n",
        "#         mlflow.log_param(\"subset_ratio\", len(X_train_subset) / len(X_train_reset))\n",
        "\n",
        "#         # Log data quality metrics for subset\n",
        "#         data_quality_metrics = {\n",
        "#             \"subset_coverage\": n_combinations / X_train_reset[['Store', 'Dept']].drop_duplicates().shape[0],\n",
        "#             \"stores_in_subset\": X_train_subset['Store'].nunique(),\n",
        "#             \"departments_in_subset\": X_train_subset['Dept'].nunique()\n",
        "#         }\n",
        "\n",
        "#         for key, value in data_quality_metrics.items():\n",
        "#             mlflow.log_metric(key, value)\n",
        "\n",
        "#         mlflow.log_metric(\"subset_selection_completed\", 1)\n",
        "\n",
        "#     return X_train_subset, X_valid_subset, y_train_subset, y_valid_subset\n",
        "\n",
        "# # =============================================================================\n",
        "# # ARIMA MODEL TRAINING\n",
        "# # =============================================================================\n",
        "\n",
        "# def train_arima_model(X_train, X_valid, y_train, y_valid, order=(1, 1, 1)):\n",
        "#     \"\"\"Train ARIMA model with comprehensive logging\"\"\"\n",
        "\n",
        "#     run_name = f\"ARIMA_Training_order_{order[0]}_{order[1]}_{order[2]}\"\n",
        "\n",
        "#     with logger.start_run(run_name):\n",
        "#         # Prepare data for ARIMA (only Date, Store, Dept needed)\n",
        "#         X_train_arima = X_train[['Date', 'Store', 'Dept']]\n",
        "#         X_valid_arima = X_valid[['Date', 'Store', 'Dept']]\n",
        "\n",
        "#         # Initialize model\n",
        "#         arima_model = store_dept_sarimax.StoreDeptSARIMAX(\n",
        "#             order=order,\n",
        "#             use_all_exog=False,\n",
        "#         )\n",
        "\n",
        "#         # Log model parameters\n",
        "#         model_params = {\n",
        "#             \"model_type\": \"ARIMA\",\n",
        "#             \"order_p\": order[0],\n",
        "#             \"order_d\": order[1],\n",
        "#             \"order_q\": order[2],\n",
        "#             \"use_exogenous_variables\": False,\n",
        "#             \"seasonal\": False\n",
        "#         }\n",
        "\n",
        "#         # Train model\n",
        "#         print(f\"Training ARIMA model with order {order}...\")\n",
        "#         start_time = datetime.now()\n",
        "\n",
        "#         arima_model.fit(X_train_arima, y_train)\n",
        "\n",
        "#         training_time = (datetime.now() - start_time).total_seconds()\n",
        "#         model_params[\"training_time_seconds\"] = training_time\n",
        "\n",
        "#         # Make predictions\n",
        "#         train_preds = arima_model.predict(X_train_arima).fillna(0)\n",
        "#         valid_preds = arima_model.predict(X_valid_arima).fillna(0)\n",
        "\n",
        "#         # Calculate metrics\n",
        "#         train_metrics = {\n",
        "#             \"mae\": mean_absolute_error(y_train, train_preds),\n",
        "#             \"mse\": mean_squared_error(y_train, train_preds),\n",
        "#             \"rmse\": np.sqrt(mean_squared_error(y_train, train_preds)),\n",
        "#             \"wmae\": calculate_wmae(y_train.values, train_preds.values, X_train['IsHoliday'].values)\n",
        "#         }\n",
        "\n",
        "#         valid_metrics = {\n",
        "#             \"mae\": mean_absolute_error(y_valid, valid_preds),\n",
        "#             \"mse\": mean_squared_error(y_valid, valid_preds),\n",
        "#             \"rmse\": np.sqrt(mean_squared_error(y_valid, valid_preds)),\n",
        "#             \"wmae\": calculate_wmae(y_valid.values, valid_preds.values, X_valid['IsHoliday'].values)\n",
        "#         }\n",
        "\n",
        "#         # Create full pipeline\n",
        "#         full_pipeline = Pipeline([\n",
        "#             ('preprocess', preprocess_pipeline),\n",
        "#             ('model', arima_model)\n",
        "#         ])\n",
        "\n",
        "#         # Log everything\n",
        "#         logger.log_model_training(\n",
        "#             model=full_pipeline,\n",
        "#             model_params=model_params,\n",
        "#             training_metrics=train_metrics,\n",
        "#             validation_metrics=valid_metrics,\n",
        "#             model_name=\"arima_model\",\n",
        "#             tags={\n",
        "#                 \"model_family\": \"ARIMA\",\n",
        "#                 \"experiment_type\": \"baseline\",\n",
        "#                 \"data_subset\": \"store_dept_sample\"\n",
        "#             }\n",
        "#         )\n",
        "\n",
        "#         # Log time series specific metrics\n",
        "#         logger.log_time_series_metrics(\n",
        "#             y_true=y_train.values,\n",
        "#             y_pred=train_preds.values,\n",
        "#             is_holiday=X_train['IsHoliday'].values,\n",
        "#             prefix=\"train\"\n",
        "#         )\n",
        "\n",
        "#         logger.log_time_series_metrics(\n",
        "#             y_true=y_valid.values,\n",
        "#             y_pred=valid_preds.values,\n",
        "#             is_holiday=X_valid['IsHoliday'].values,\n",
        "#             prefix=\"valid\"\n",
        "#         )\n",
        "\n",
        "#         print(f\"ARIMA {order} - Train WMAE: {train_metrics['wmae']:.2f}, Valid WMAE: {valid_metrics['wmae']:.2f}\")\n",
        "\n",
        "#         return full_pipeline, valid_metrics['wmae']\n",
        "\n",
        "# # =============================================================================\n",
        "# # SARIMA MODEL TRAINING\n",
        "# # =============================================================================\n",
        "\n",
        "# def train_sarima_model(X_train, X_valid, y_train, y_valid,\n",
        "#                       order=(1, 1, 1), seasonal_order=(1, 1, 1, 52)):\n",
        "#     \"\"\"Train SARIMA model with comprehensive logging\"\"\"\n",
        "\n",
        "#     run_name = f\"SARIMA_Training_order_{order[0]}_{order[1]}_{order[2]}_seasonal_{seasonal_order[0]}_{seasonal_order[1]}_{seasonal_order[2]}_{seasonal_order[3]}\"\n",
        "\n",
        "#     with logger.start_run(run_name):\n",
        "#         # Prepare data for SARIMA\n",
        "#         X_train_sarima = X_train[['Date', 'Store', 'Dept']]\n",
        "#         X_valid_sarima = X_valid[['Date', 'Store', 'Dept']]\n",
        "\n",
        "#         # Initialize model\n",
        "#         sarima_model = store_dept_sarimax.StoreDeptSARIMAX(\n",
        "#             order=order,\n",
        "#             seasonal_order=seasonal_order,\n",
        "#             use_all_exog=False\n",
        "#         )\n",
        "\n",
        "#         # Log model parameters\n",
        "#         model_params = {\n",
        "#             \"model_type\": \"SARIMA\",\n",
        "#             \"order_p\": order[0],\n",
        "#             \"order_d\": order[1],\n",
        "#             \"order_q\": order[2],\n",
        "#             \"seasonal_p\": seasonal_order[0],\n",
        "#             \"seasonal_d\": seasonal_order[1],\n",
        "#             \"seasonal_q\": seasonal_order[2],\n",
        "#             \"seasonal_period\": seasonal_order[3],\n",
        "#             \"use_exogenous_variables\": False,\n",
        "#             \"seasonal\": True\n",
        "#         }\n",
        "\n",
        "#         # Train model\n",
        "#         print(f\"Training SARIMA model with order {order} and seasonal {seasonal_order}...\")\n",
        "#         start_time = datetime.now()\n",
        "\n",
        "#         sarima_model.fit(X_train_sarima, y_train)\n",
        "\n",
        "#         training_time = (datetime.now() - start_time).total_seconds()\n",
        "#         model_params[\"training_time_seconds\"] = training_time\n",
        "\n",
        "#         # Make predictions\n",
        "#         train_preds = sarima_model.predict(X_train_sarima).fillna(0)\n",
        "#         valid_preds = sarima_model.predict(X_valid_sarima).fillna(0)\n",
        "\n",
        "#         # Calculate metrics\n",
        "#         train_metrics = {\n",
        "#             \"mae\": mean_absolute_error(y_train, train_preds),\n",
        "#             \"mse\": mean_squared_error(y_train, train_preds),\n",
        "#             \"rmse\": np.sqrt(mean_squared_error(y_train, train_preds)),\n",
        "#             \"wmae\": calculate_wmae(y_train.values, train_preds.values, X_train['IsHoliday'].values)\n",
        "#         }\n",
        "\n",
        "#         valid_metrics = {\n",
        "#             \"mae\": mean_absolute_error(y_valid, valid_preds),\n",
        "#             \"mse\": mean_squared_error(y_valid, valid_preds),\n",
        "#             \"rmse\": np.sqrt(mean_squared_error(y_valid, valid_preds)),\n",
        "#             \"wmae\": calculate_wmae(y_valid.values, valid_preds.values, X_valid['IsHoliday'].values)\n",
        "#         }\n",
        "\n",
        "#         # Create full pipeline\n",
        "#         full_pipeline = Pipeline([\n",
        "#             ('preprocess', preprocess_pipeline),\n",
        "#             ('model', sarima_model)\n",
        "#         ])\n",
        "\n",
        "#         # Log everything\n",
        "#         logger.log_model_training(\n",
        "#             model=full_pipeline,\n",
        "#             model_params=model_params,\n",
        "#             training_metrics=train_metrics,\n",
        "#             validation_metrics=valid_metrics,\n",
        "#             model_name=\"sarima_model\",\n",
        "#             tags={\n",
        "#                 \"model_family\": \"SARIMA\",\n",
        "#                 \"experiment_type\": \"seasonal_baseline\",\n",
        "#                 \"data_subset\": \"store_dept_sample\"\n",
        "#             }\n",
        "#         )\n",
        "\n",
        "#         # Log time series specific metrics\n",
        "#         logger.log_time_series_metrics(\n",
        "#             y_true=y_train.values,\n",
        "#             y_pred=train_preds.values,\n",
        "#             is_holiday=X_train['IsHoliday'].values,\n",
        "#             prefix=\"train\"\n",
        "#         )\n",
        "\n",
        "#         logger.log_time_series_metrics(\n",
        "#             y_true=y_valid.values,\n",
        "#             y_pred=valid_preds.values,\n",
        "#             is_holiday=X_valid['IsHoliday'].values,\n",
        "#             prefix=\"valid\"\n",
        "#         )\n",
        "\n",
        "#         print(f\"SARIMA {order} {seasonal_order} - Train WMAE: {train_metrics['wmae']:.2f}, Valid WMAE: {valid_metrics['wmae']:.2f}\")\n",
        "\n",
        "#         return full_pipeline, valid_metrics['wmae']\n",
        "\n",
        "# # =============================================================================\n",
        "# # SARIMAX MODEL TRAINING\n",
        "# # =============================================================================\n",
        "\n",
        "# def train_sarimax_model(X_train, X_valid, y_train, y_valid,\n",
        "#                        order=(1, 1, 1), seasonal_order=(1, 1, 1, 52)):\n",
        "#     \"\"\"Train SARIMAX model with comprehensive logging\"\"\"\n",
        "\n",
        "#     run_name = f\"SARIMAX_Training_order_{order[0]}_{order[1]}_{order[2]}_seasonal_{seasonal_order[0]}_{seasonal_order[1]}_{seasonal_order[2]}_{seasonal_order[3]}\"\n",
        "\n",
        "#     with logger.start_run(run_name):\n",
        "#         # Initialize model with exogenous variables\n",
        "#         sarimax_model = store_dept_sarimax.StoreDeptSARIMAX(\n",
        "#             order=order,\n",
        "#             seasonal_order=seasonal_order,\n",
        "#             use_all_exog=True\n",
        "#         )\n",
        "\n",
        "#         # Log model parameters\n",
        "#         model_params = {\n",
        "#             \"model_type\": \"SARIMAX\",\n",
        "#             \"order_p\": order[0],\n",
        "#             \"order_d\": order[1],\n",
        "#             \"order_q\": order[2],\n",
        "#             \"seasonal_p\": seasonal_order[0],\n",
        "#             \"seasonal_d\": seasonal_order[1],\n",
        "#             \"seasonal_q\": seasonal_order[2],\n",
        "#             \"seasonal_period\": seasonal_order[3],\n",
        "#             \"use_exogenous_variables\": True,\n",
        "#             \"seasonal\": True,\n",
        "#             \"exogenous_features\": X_train.columns.tolist()\n",
        "#         }\n",
        "\n",
        "#         # Train model\n",
        "#         print(f\"Training SARIMAX model with order {order} and seasonal {seasonal_order}...\")\n",
        "#         start_time = datetime.now()\n",
        "\n",
        "#         sarimax_model.fit(X_train, y_train)\n",
        "\n",
        "#         training_time = (datetime.now() - start_time).total_seconds()\n",
        "#         model_params[\"training_time_seconds\"] = training_time\n",
        "\n",
        "#         # Make predictions\n",
        "#         train_preds = sarimax_model.predict(X_train).fillna(0)\n",
        "#         valid_preds = sarimax_model.predict(X_valid).fillna(0)\n",
        "\n",
        "#         # Calculate metrics\n",
        "#         train_metrics = {\n",
        "#             \"mae\": mean_absolute_error(y_train, train_preds),\n",
        "#             \"mse\": mean_squared_error(y_train, train_preds),\n",
        "#             \"rmse\": np.sqrt(mean_squared_error(y_train, train_preds)),\n",
        "#             \"wmae\": calculate_wmae(y_train.values, train_preds.values, X_train['IsHoliday'].values)\n",
        "#         }\n",
        "\n",
        "#         valid_metrics = {\n",
        "#             \"mae\": mean_absolute_error(y_valid, valid_preds),\n",
        "#             \"mse\": mean_squared_error(y_valid, valid_preds),\n",
        "#             \"rmse\": np.sqrt(mean_squared_error(y_valid, valid_preds)),\n",
        "#             \"wmae\": calculate_wmae(y_valid.values, valid_preds.values, X_valid['IsHoliday'].values)\n",
        "#         }\n",
        "\n",
        "#         # Create full pipeline\n",
        "#         full_pipeline = Pipeline([\n",
        "#             ('preprocess', preprocess_pipeline),\n",
        "#             ('model', sarimax_model)\n",
        "#         ])\n",
        "\n",
        "#         # Log everything\n",
        "#         logger.log_model_training(\n",
        "#             model=full_pipeline,\n",
        "#             model_params=model_params,\n",
        "#             training_metrics=train_metrics,\n",
        "#             validation_metrics=valid_metrics,\n",
        "#             model_name=\"sarimax_model\",\n",
        "#             tags={\n",
        "#                 \"model_family\": \"SARIMAX\",\n",
        "#                 \"experiment_type\": \"exogenous_variables\",\n",
        "#                 \"data_subset\": \"store_dept_sample\"\n",
        "#             }\n",
        "#         )\n",
        "\n",
        "#         # Log time series specific metrics\n",
        "#         logger.log_time_series_metrics(\n",
        "#             y_true=y_train.values,\n",
        "#             y_pred=train_preds.values,\n",
        "#             is_holiday=X_train['IsHoliday'].values,\n",
        "#             prefix=\"train\"\n",
        "#         )\n",
        "\n",
        "#         logger.log_time_series_metrics(\n",
        "#             y_true=y_valid.values,\n",
        "#             y_pred=valid_preds.values,\n",
        "#             is_holiday=X_valid['IsHoliday'].values,\n",
        "#             prefix=\"valid\"\n",
        "#         )\n",
        "\n",
        "#         print(f\"SARIMAX {order} {seasonal_order} - Train WMAE: {train_metrics['wmae']:.2f}, Valid WMAE: {valid_metrics['wmae']:.2f}\")\n",
        "\n",
        "#         return full_pipeline, valid_metrics['wmae']\n",
        "\n",
        "# # =============================================================================\n",
        "# # HYPERPARAMETER TUNING\n",
        "# # =============================================================================\n",
        "\n",
        "# def hyperparameter_tuning_arima(X_train, X_valid, y_train, y_valid):\n",
        "#     \"\"\"Perform hyperparameter tuning for ARIMA models\"\"\"\n",
        "\n",
        "#     with logger.start_run(\"ARIMA_Hyperparameter_Tuning\"):\n",
        "#         # Define search space\n",
        "#         search_space = {\n",
        "#             \"p_values\": [0, 1, 2],\n",
        "#             \"d_values\": [1],\n",
        "#             \"q_values\": [0, 1, 2]\n",
        "#         }\n",
        "\n",
        "#         best_score = float('inf')\n",
        "#         best_params = None\n",
        "#         best_pipeline = None\n",
        "\n",
        "#         results = []\n",
        "\n",
        "#         # Grid search\n",
        "#         for p in search_space[\"p_values\"]:\n",
        "#             for d in search_space[\"d_values\"]:\n",
        "#                 for q in search_space[\"q_values\"]:\n",
        "#                     try:\n",
        "#                         order = (p, d, q)\n",
        "#                         pipeline, score = train_arima_model(X_train, X_valid, y_train, y_valid, order)\n",
        "\n",
        "#                         results.append({\n",
        "#                             \"p\": p, \"d\": d, \"q\": q,\n",
        "#                             \"score\": score\n",
        "#                         })\n",
        "\n",
        "#                         if score < best_score:\n",
        "#                             best_score = score\n",
        "#                             best_params = {\"p\": p, \"d\": d, \"q\": q}\n",
        "#                             best_pipeline = pipeline\n",
        "\n",
        "#                         print(f\"ARIMA({p},{d},{q}) - Score: {score:.2f}\")\n",
        "\n",
        "#                     except Exception as e:\n",
        "#                         print(f\"Error with ARIMA({p},{d},{q}): {e}\")\n",
        "#                         continue\n",
        "\n",
        "#         # Log hyperparameter tuning results\n",
        "#         logger.log_hyperparameter_tuning(\n",
        "#             best_params=best_params,\n",
        "#             best_score=best_score,\n",
        "#             tuning_method=\"grid_search\",\n",
        "#             search_space=search_space,\n",
        "#             n_trials=len(results)\n",
        "#         )\n",
        "\n",
        "#         # Log all trial results\n",
        "#         for i, result in enumerate(results):\n",
        "#             mlflow.log_metric(f\"trial_{i}_score\", result[\"score\"])\n",
        "#             mlflow.log_param(f\"trial_{i}_params\", str(result))\n",
        "\n",
        "#         return best_pipeline, best_params, best_score\n",
        "\n",
        "# # =============================================================================\n",
        "# # CROSS VALIDATION\n",
        "# # =============================================================================\n",
        "\n",
        "# def time_series_cross_validation(X_train, y_train, model_func, n_splits=3):\n",
        "#     \"\"\"Perform time series cross validation\"\"\"\n",
        "\n",
        "#     with logger.start_run(\"ARIMA_Cross_Validation\"):\n",
        "#         # Simple time series split\n",
        "#         total_size = len(X_train)\n",
        "#         split_size = total_size // (n_splits + 1)\n",
        "\n",
        "#         cv_scores = {\"wmae\": [], \"mae\": [], \"rmse\": []}\n",
        "\n",
        "#         for i in range(n_splits):\n",
        "#             # Define train and validation indices for this fold\n",
        "#             train_end = split_size * (i + 1)\n",
        "#             val_start = train_end\n",
        "#             val_end = split_size * (i + 2)\n",
        "\n",
        "#             if val_end > total_size:\n",
        "#                 val_end = total_size\n",
        "\n",
        "#             # Split data\n",
        "#             X_train_fold = X_train.iloc[:train_end]\n",
        "#             y_train_fold = y_train.iloc[:train_end]\n",
        "#             X_val_fold = X_train.iloc[val_start:val_end]\n",
        "#             y_val_fold = y_train.iloc[val_start:val_end]\n",
        "\n",
        "#             # Train model\n",
        "#             try:\n",
        "#                 _, score = model_func(X_train_fold, X_val_fold, y_train_fold, y_val_fold)\n",
        "#                 cv_scores[\"wmae\"].append(score)\n",
        "\n",
        "#                 print(f\"Fold {i+1}: WMAE = {score:.2f}\")\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error in fold {i+1}: {e}\")\n",
        "#                 continue\n",
        "\n",
        "#         # Log cross validation results\n",
        "#         if cv_scores[\"wmae\"]:\n",
        "#             logger.log_cross_validation(\n",
        "#                 cv_scores=cv_scores,\n",
        "#                 cv_method=\"time_series_split\",\n",
        "#                 n_folds=len(cv_scores[\"wmae\"])\n",
        "#             )\n",
        "\n",
        "#         return cv_scores\n",
        "\n",
        "# # =============================================================================\n",
        "# # MAIN EXECUTION\n",
        "# # =============================================================================\n",
        "\n",
        "# def main():\n",
        "#     \"\"\"Main execution function\"\"\"\n",
        "\n",
        "#     print(\"Starting ARIMA Model Experiment with MLflow Logging...\")\n",
        "#     print(\"=\" * 60)\n",
        "\n",
        "#     # Step 1: Load and preprocess data\n",
        "#     print(\"Step 1: Loading and preprocessing data...\")\n",
        "#     X_train, y_train, X_valid, y_valid = load_and_preprocess_data()\n",
        "\n",
        "#     # Step 2: Create preprocessing pipeline\n",
        "#     print(\"Step 2: Creating preprocessing pipeline...\")\n",
        "#     global preprocess_pipeline\n",
        "#     preprocess_pipeline = create_preprocessing_pipeline()\n",
        "\n",
        "#     # Step 3: Apply feature engineering\n",
        "#     print(\"Step 3: Applying feature engineering...\")\n",
        "#     X_train_processed, X_valid_processed = apply_feature_engineering(\n",
        "#         X_train, X_valid, preprocess_pipeline\n",
        "#     )\n",
        "\n",
        "#     # Step 4: Select subset for efficient training\n",
        "#     print(\"Step 4: Selecting subset for training...\")\n",
        "#     X_train_subset, X_valid_subset, y_train_subset, y_valid_subset = select_subset_for_training(\n",
        "#         X_train_processed, X_valid_processed, y_train, y_valid, n_combinations=100\n",
        "#     )\n",
        "\n",
        "#     # Step 5: Train individual models\n",
        "#     print(\"Step 5: Training individual models...\")\n",
        "\n",
        "#     # ARIMA models\n",
        "#     print(\"\\n--- Training ARIMA Models ---\")\n",
        "#     arima_pipeline, arima_score = train_arima_model(\n",
        "#         X_train_subset, X_valid_subset, y_train_subset, y_valid_subset\n",
        "#     )\n",
        "\n",
        "#     # SARIMA models\n",
        "#     print(\"\\n--- Training SARIMA Models ---\")\n",
        "#     sarima_pipeline, sarima_score = train_sarima_model(\n",
        "#         X_train_subset, X_valid_subset, y_train_subset, y_valid_subset\n",
        "#     )\n",
        "\n",
        "#     # SARIMAX models\n",
        "#     print(\"\\n--- Training SARIMAX Models ---\")\n",
        "#     sarimax_pipeline, sarimax_score = train_sarimax_model(\n",
        "#         X_train_subset, X_valid_subset, y_train_subset, y_valid_subset\n",
        "#     )\n",
        "\n",
        "#     # Step 6: Hyperparameter tuning for best model\n",
        "#     print(\"\\n--- Hyperparameter Tuning ---\")\n",
        "#     best_pipeline, best_params, best_score = hyperparameter_tuning_arima(\n",
        "#         X_train_subset, X_valid_subset, y_train_subset, y_valid_subset\n",
        "#     )\n",
        "\n",
        "#     # Step 7: Cross validation\n",
        "#     print(\"\\n--- Cross Validation ---\")\n",
        "#     cv_scores = time_series_cross_validation(\n",
        "#         X_train_subset, y_train_subset,\n",
        "#         lambda x1, x2, y1, y2: train_arima_model(x1, x2, y1, y2, best_params)\n",
        "#     )\n",
        "\n",
        "#     # Step 8: Register best model\n",
        "#     print(\"\\n--- Registering Best Model ---\")\n",
        "#     model_comparison = {\n",
        "#         \"ARIMA\": arima_score,\n",
        "#         \"SARIMA\": sarima_score,\n",
        "#         \"SARIMAX\": sarimax_score\n",
        "#     }\n",
        "\n",
        "#     best_model_name = min(model_comparison, key=model_comparison.get)\n",
        "#     print(f\"Best model: {best_model_name} with score: {model_comparison[best_model_name]:.2f}\")\n",
        "\n",
        "#     # Register the best model\n",
        "#     with logger.start_run(\"ARIMA_Model_Registration\"):\n",
        "#         mlflow.log_param(\"best_model_type\", best_model_name)\n",
        "#         mlflow.log_metric(\"best_score\", model_comparison[best_model_name])\n",
        "\n",
        "#         # Log comparison metrics\n",
        "#         for model_name, score in model_comparison.items():\n",
        "#             mlflow.log_metric(f\"{model_name.lower()}_score\", score)\n",
        "\n",
        "#         # Register best pipeline\n",
        "#         if best_model_name == \"ARIMA\":\n",
        "#             mlflow.sklearn.log_model(arima_pipeline, \"best_arima_model\")\n",
        "#         elif best_model_name == \"SARIMA\":\n",
        "#             mlflow.sklearn.log_model(sarima_pipeline, \"best_arima_model\")\n",
        "#         else:\n",
        "#             mlflow.sklearn.log_model(sarimax_pipeline, \"best_arima_model\")\n",
        "\n",
        "#         # Register in model registry\n",
        "#         logger.register_best_model(\n",
        "#             model_name=\"walmart_sales_arima\",\n",
        "#             model_version=\"1\",\n",
        "#             model_stage=\"Staging\"\n",
        "#         )\n",
        "\n",
        "#     # End all runs\n",
        "#     logger.end_run()\n",
        "\n",
        "#     print(\"\\n\" + \"=\" * 60)\n",
        "#     print(\"ARIMA Model Experiment Completed!\")\n",
        "#     print(f\"Best Model: {best_model_name}\")\n",
        "#     print(f\"Best Score: {model_comparison[best_model_name]:.2f}\")\n",
        "#     print(\"Check your DagsHub MLflow UI for detailed logs and artifacts.\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "metadata": {
        "id": "tXsD_7_EiMys",
        "collapsed": true
      },
      "id": "tXsD_7_EiMys",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler, OneHotEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, mean_absolute_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import xgboost as xgb\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import mlflow.xgboost\n",
        "from mlflow.models.signature import infer_signature\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "OYLuVnQPZDWk"
      },
      "id": "OYLuVnQPZDWk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from dagshub import dagshub_logger\n",
        "import os\n",
        "\n",
        "# Set tracking URI manually\n",
        "mlflow.set_tracking_uri(\"https://dagshub.com/ekvirika/WalmartRecruiting.mlflow\")\n",
        "\n",
        "# Use your DagsHub credentials\n",
        "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"ekvirika\"\n",
        "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"0adb1004ddd4221395353efea2d8ead625e26197\"\n",
        "\n",
        "# Optional: set registry if you're using model registry\n",
        "mlflow.set_registry_uri(\"https://dagshub.com/ekvirika/WalmartRecruiting.mlflow\")"
      ],
      "metadata": {
        "id": "OKLSGEWAZH00"
      },
      "id": "OKLSGEWAZH00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train = pd.read_csv('train.csv')\n",
        "# test = pd.read_csv('test.csv')\n",
        "# stores = pd.read_csv('stores.csv')\n",
        "# features = pd.read_csv('features.csv')"
      ],
      "metadata": {
        "id": "oe3MRdW4bZQj"
      },
      "id": "oe3MRdW4bZQj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install darts --quiet"
      ],
      "metadata": {
        "id": "eDV2mIekVxB9",
        "collapsed": true
      },
      "id": "eDV2mIekVxB9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from darts import TimeSeries\n",
        "# from darts.models import ARIMA\n",
        "# from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
        "# from darts.utils.utils import SeasonalityMode\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# # Load and prepare data\n",
        "# df = pd.read_csv(\"train.csv\")\n",
        "# df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "# df = df.sort_values(\"Date\")\n",
        "\n",
        "# # Select one Store-Dept combo to forecast\n",
        "# store_id = 1\n",
        "# dept_id = 1\n",
        "\n",
        "# df_filtered = df[(df[\"Store\"] == store_id) & (df[\"Dept\"] == dept_id)]\n",
        "# df_filtered = df_filtered[df_filtered[\"Weekly_Sales\"] > 0]  # remove zeros\n",
        "# df_filtered = df_filtered.dropna(subset=[\"Weekly_Sales\"])\n",
        "\n",
        "# # Create TimeSeries\n",
        "# series = TimeSeries.from_dataframe(\n",
        "#     df_filtered,\n",
        "#     time_col=\"Date\",\n",
        "#     value_cols=\"Weekly_Sales\"\n",
        "# )\n",
        "\n",
        "# # Create cyclic future covariates (month as sine/cosine)\n",
        "# future_cov = datetime_attribute_timeseries(series, attribute=\"month\", cyclic=True, add_length=6)\n",
        "\n",
        "# # # Define ARIMA model and fit\n",
        "# # model = ARIMA(p=12, d=1, q=2)\n",
        "# # model.fit(series, future_covariates=future_cov)\n",
        "\n",
        "# # # Predict 6 steps ahead\n",
        "# # forecast = model.predict(6, future_covariates=future_cov)\n",
        "\n",
        "# # # Get predicted values\n",
        "# # print(forecast.values())\n"
      ],
      "metadata": {
        "id": "JnBHIdT7Vhvl",
        "collapsed": true
      },
      "id": "JnBHIdT7Vhvl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# from darts.models import ARIMA\n",
        "# from darts.metrics import mape  # Not used now\n",
        "# from darts.datasets import AirPassengersDataset\n",
        "# from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
        "\n",
        "# # WMAE implementation\n",
        "# def weighted_mae(y_true, y_pred, is_holiday):\n",
        "#     weights = np.where(is_holiday, 5, 1)\n",
        "#     return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "# # Assuming `series` is your Darts TimeSeries object\n",
        "# train, val = series[:-6], series[-6:]\n",
        "\n",
        "# # Fix: extract matching holiday array for validation set\n",
        "# val_start_date = val.time_index[0]\n",
        "# is_holiday_array = df[df[\"Date\"] >= val_start_date].head(6)[\"IsHoliday\"].astype(bool).values\n",
        "\n",
        "# best_score = float(\"inf\")\n",
        "# best_order = None\n",
        "# best_model = None\n",
        "\n",
        "# for p in range(0, 4):\n",
        "#     for d in range(0, 2):\n",
        "#         for q in range(0, 4):\n",
        "#             try:\n",
        "#                 model = ARIMA(p=p, d=d, q=q)\n",
        "#                 model.fit(train)\n",
        "#                 pred = model.predict(6)\n",
        "\n",
        "#                 y_true = val.values().squeeze()\n",
        "#                 y_pred = pred.values().squeeze()\n",
        "\n",
        "#                 score = weighted_mae(y_true, y_pred, is_holiday_array)\n",
        "\n",
        "#                 if score < best_score:\n",
        "#                     best_score = score\n",
        "#                     best_order = (p, d, q)\n",
        "#                     best_model = model\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Failed for order {(p,d,q)}: {e}\")\n",
        "#                 continue\n",
        "\n",
        "# print(\"Best order:\", best_order)\n",
        "# print(\"Best WMAE:\", best_score)\n"
      ],
      "metadata": {
        "id": "3LaWzEFNVSXN",
        "collapsed": true
      },
      "id": "3LaWzEFNVSXN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define ARIMA model and fit\n",
        "# model = ARIMA(p=1, d=0, q=0)\n",
        "# model.fit(series, future_covariates=future_cov)\n",
        "\n",
        "# # Predict 6 steps ahead\n",
        "# forecast = model.predict(6, future_covariates=future_cov)\n",
        "\n",
        "# # Get predicted values\n",
        "# print(forecast.values())\n"
      ],
      "metadata": {
        "id": "e3V247r-VsM9",
        "collapsed": true
      },
      "id": "e3V247r-VsM9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# def weighted_mae(y_true, y_pred, is_holiday):\n",
        "#     weights = np.where(is_holiday, 5, 1)  # weight 5 if holiday, else 1\n",
        "#     return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "# # Example true values for the 6 weeks (replace with your actual test values)\n",
        "# y_true = np.array([28000, 26000, 25000, 27000, 24000, 29000])\n",
        "\n",
        "# # Predicted values from model\n",
        "# y_pred = forecast.values().squeeze()\n",
        "\n",
        "# # Holiday flags for the 6 weeks (replace with your actual is_holiday flags for those weeks)\n",
        "# is_holiday_array = np.array([False, False, True, False, False, True])\n",
        "\n",
        "# # Calculate WMAE\n",
        "# wmae_score = weighted_mae(y_true, y_pred, is_holiday_array)\n",
        "# print(f\"WMAE for 6-week forecast: {wmae_score}\")\n"
      ],
      "metadata": {
        "id": "CJ-DV1cAVSVA"
      },
      "id": "CJ-DV1cAVSVA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# from sklearn.base import BaseEstimator, TransformerMixin\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "# from statsmodels.tsa.arima.model import ARIMA\n",
        "# from statsmodels.tsa.stattools import adfuller\n",
        "# from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "# from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "# class DataLoader(BaseEstimator, TransformerMixin):\n",
        "\n",
        "#     def __init__(self, train_path=None, test_path=None, stores_path=None, features_path=None):\n",
        "#         self.train_path = train_path\n",
        "#         self.test_path = test_path\n",
        "#         self.stores_path = stores_path\n",
        "#         self.features_path = features_path\n",
        "#         self.data_loaded = False\n",
        "\n",
        "#     def fit(self, X=None, y=None):\n",
        "#         \"\"\"Fit method - loads and stores data\"\"\"\n",
        "#         if self.train_path:\n",
        "#             self.train_data = pd.read_csv(self.train_path)\n",
        "#         if self.test_path:\n",
        "#             self.test_data = pd.read_csv(self.test_path)\n",
        "#         if self.stores_path:\n",
        "#             self.stores_data = pd.read_csv(self.stores_path)\n",
        "#         if self.features_path:\n",
        "#             self.features_data = pd.read_csv(self.features_path)\n",
        "\n",
        "#         self.data_loaded = True\n",
        "#         print(f\"Data loaded - Train: {self.train_data.shape}, Test: {self.test_data.shape}\")\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X=None):\n",
        "#         \"\"\"Transform method - returns loaded data\"\"\"\n",
        "#         if not self.data_loaded:\n",
        "#             raise ValueError(\"Data not loaded. Call fit() first.\")\n",
        "\n",
        "#         data_dict = {\n",
        "#             'train': self.train_data,\n",
        "#             'test': self.test_data\n",
        "#         }\n",
        "\n",
        "#         if hasattr(self, 'stores_data'):\n",
        "#             data_dict['stores'] = self.stores_data\n",
        "#         if hasattr(self, 'features_data'):\n",
        "#             data_dict['features'] = self.features_data\n",
        "\n",
        "#         return data_dict\n",
        "\n",
        "# class DataCleaner(BaseEstimator, TransformerMixin):\n",
        "#     \"\"\"\n",
        "#     Data cleaning component for preprocessing Walmart data (adapted from XGBoost preprocessing)\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, merge_external_data=True, filter_positive_sales=True,\n",
        "#                   remove_outliers=True, outlier_quantiles=(0.01, 0.99)):\n",
        "#         self.merge_external_data = merge_external_data\n",
        "#         self.filter_positive_sales = filter_positive_sales\n",
        "#         self.remove_outliers = remove_outliers\n",
        "#         self.outlier_quantiles = outlier_quantiles\n",
        "#         self.outlier_bounds = {}\n",
        "\n",
        "#     def fit(self, data_dict, y=None):\n",
        "#         \"\"\"Fit method - learns data characteristics for cleaning\"\"\"\n",
        "#         train_data = data_dict['train'].copy()\n",
        "\n",
        "#         # Convert date columns\n",
        "#         train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "\n",
        "#         # Learn global outlier bounds for Weekly_Sales (following XGBoost approach)\n",
        "#         if self.remove_outliers and 'Weekly_Sales' in train_data.columns:\n",
        "#             # Filter positive sales first if specified\n",
        "#             sales_data = train_data['Weekly_Sales']\n",
        "#             if self.filter_positive_sales:\n",
        "#                 sales_data = sales_data[sales_data > 0]\n",
        "\n",
        "#             if len(sales_data) > 0:\n",
        "#                 q_low = sales_data.quantile(self.outlier_quantiles[0])\n",
        "#                 q_high = sales_data.quantile(self.outlier_quantiles[1])\n",
        "#                 self.outlier_bounds['global'] = (q_low, q_high)\n",
        "#                 print(f\"Learned outlier bounds: {q_low:.2f} - {q_high:.2f}\")\n",
        "\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, data_dict):\n",
        "#         \"\"\"Transform method - applies cleaning operations (XGBoost-style)\"\"\"\n",
        "#         cleaned_data = {}\n",
        "\n",
        "#         for key, df in data_dict.items():\n",
        "#             cleaned_df = df.copy()\n",
        "\n",
        "#             # Convert Date columns\n",
        "#             if 'Date' in cleaned_df.columns:\n",
        "#                 cleaned_df['Date'] = pd.to_datetime(cleaned_df['Date'])\n",
        "\n",
        "#             # Merge external data if available (features and stores)\n",
        "#             if self.merge_external_data and key in ['train', 'test']:\n",
        "#                 # Merge features data\n",
        "#                 if 'features' in data_dict:\n",
        "#                     features_df = data_dict['features'].copy()\n",
        "#                     features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
        "#                     cleaned_df = cleaned_df.merge(features_df, on=['Store', 'Date'], how='left')\n",
        "\n",
        "#                     # Handle IsHoliday column naming issues (from XGBoost preprocessing)\n",
        "#                     if \"IsHoliday_x\" in cleaned_df.columns:\n",
        "#                         cleaned_df[\"IsHoliday\"] = cleaned_df.pop(\"IsHoliday_x\")\n",
        "#                         cleaned_df.drop(columns=[\"IsHoliday_y\"], errors='ignore', inplace=True)\n",
        "\n",
        "#                 # Merge stores data\n",
        "#                 if 'stores' in data_dict:\n",
        "#                     stores_df = data_dict['stores'].copy()\n",
        "#                     cleaned_df = cleaned_df.merge(stores_df, on='Store', how='left')\n",
        "\n",
        "#             # Filter positive sales (following XGBoost approach)\n",
        "#             if 'Weekly_Sales' in cleaned_df.columns and self.filter_positive_sales:\n",
        "#                 original_count = len(cleaned_df)\n",
        "#                 cleaned_df = cleaned_df[cleaned_df[\"Weekly_Sales\"] > 0]\n",
        "#                 filtered_count = original_count - len(cleaned_df)\n",
        "#                 if filtered_count > 0:\n",
        "#                     print(f\"Filtered {filtered_count} non-positive sales records\")\n",
        "\n",
        "#             # Remove outliers using global bounds (XGBoost approach)\n",
        "#             if ('Weekly_Sales' in cleaned_df.columns and self.remove_outliers and\n",
        "#                 'global' in self.outlier_bounds):\n",
        "\n",
        "#                 q_low, q_high = self.outlier_bounds['global']\n",
        "#                 original_count = len(cleaned_df)\n",
        "#                 cleaned_df = cleaned_df[\n",
        "#                     (cleaned_df[\"Weekly_Sales\"] >= q_low) &\n",
        "#                     (cleaned_df[\"Weekly_Sales\"] <= q_high)\n",
        "#                 ]\n",
        "#                 outlier_count = original_count - len(cleaned_df)\n",
        "#                 if outlier_count > 0:\n",
        "#                     print(f\"Removed {outlier_count} outlier records\")\n",
        "\n",
        "#             # Fill missing MarkDown columns with 0 (from XGBoost preprocessing)\n",
        "#             markdown_cols = [col for col in cleaned_df.columns if \"MarkDown\" in col]\n",
        "#             if markdown_cols:\n",
        "#                 cleaned_df[markdown_cols] = cleaned_df[markdown_cols].fillna(0)\n",
        "#                 print(f\"Filled NaN values in {len(markdown_cols)} MarkDown columns with 0\")\n",
        "\n",
        "#             # Sort by Date (from XGBoost preprocessing)\n",
        "#             if 'Date' in cleaned_df.columns:\n",
        "#                 cleaned_df = cleaned_df.sort_values(by=\"Date\")\n",
        "\n",
        "#             cleaned_data[key] = cleaned_df\n",
        "\n",
        "#         print(\"Data cleaning completed\")\n",
        "#         return cleaned_data\n",
        "\n",
        "# class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "#     \"\"\"\n",
        "#     Feature engineering component for time series features (adapted from XGBoost preprocessing)\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, create_lags=True, lag_periods=[1, 2, 4, 52],\n",
        "#                   create_rolling=True, rolling_windows=[4], rolling_stats=['mean', 'std'],\n",
        "#                   create_date_features=True, create_seasonal=True, drop_na_rows=True):\n",
        "#         self.create_lags = create_lags\n",
        "#         self.lag_periods = lag_periods\n",
        "#         self.create_rolling = create_rolling\n",
        "#         self.rolling_windows = rolling_windows\n",
        "#         self.rolling_stats = rolling_stats\n",
        "#         self.create_date_features = create_date_features\n",
        "#         self.create_seasonal = create_seasonal\n",
        "#         self.drop_na_rows = drop_na_rows\n",
        "#         self.feature_columns = []\n",
        "\n",
        "#     def fit(self, data_dict, y=None):\n",
        "#         \"\"\"Fit method - learns feature engineering parameters\"\"\"\n",
        "#         train_data = data_dict['train'].copy()\n",
        "\n",
        "#         # Store original columns\n",
        "#         self.original_columns = train_data.columns.tolist()\n",
        "\n",
        "#         # Create a sample to determine feature columns\n",
        "#         sample_data = self._create_features(train_data.head(1000))\n",
        "#         self.feature_columns = [col for col in sample_data.columns\n",
        "#                                 if col not in self.original_columns]\n",
        "\n",
        "#         print(f\"Feature engineering will create {len(self.feature_columns)} new features\")\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, data_dict):\n",
        "#         \"\"\"Transform method - applies feature engineering\"\"\"\n",
        "#         engineered_data = {}\n",
        "\n",
        "#         for key, df in data_dict.items():\n",
        "#             if 'Weekly_Sales' in df.columns:  # Only for training data\n",
        "#                 engineered_df = self._create_features(df)\n",
        "#             else:\n",
        "#                 engineered_df = df.copy()\n",
        "#                 # Add placeholder columns for test data\n",
        "#                 for col in self.feature_columns:\n",
        "#                     engineered_df[col] = 0\n",
        "\n",
        "#             engineered_data[key] = engineered_df\n",
        "\n",
        "#         print(\"Feature engineering completed\")\n",
        "#         return engineered_data\n",
        "\n",
        "#     def _create_features(self, df):\n",
        "#         \"\"\"Internal method to create time series features (XGBoost-style)\"\"\"\n",
        "#         df = df.copy()\n",
        "#         df = df.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "#         # Create Store_Dept identifier\n",
        "#         df['Store_Dept'] = df['Store'].astype(str) + '_' + df['Dept'].astype(str)\n",
        "\n",
        "#         # Create lag features (following XGBoost approach exactly)\n",
        "#         if self.create_lags and 'Weekly_Sales' in df.columns:\n",
        "#             for lag in self.lag_periods:\n",
        "#                 df[f'lag_{lag}'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(lag)\n",
        "#                 print(f\"Created lag_{lag} feature\")\n",
        "\n",
        "#         # Create rolling statistics (following XGBoost approach)\n",
        "#         if self.create_rolling and 'Weekly_Sales' in df.columns:\n",
        "#             for window in self.rolling_windows:\n",
        "#                 for stat in self.rolling_stats:\n",
        "#                     if stat == 'mean':\n",
        "#                         # XGBoost uses shift(1) before rolling to avoid data leakage\n",
        "#                         df[f'rolling_{stat}_{window}'] = (\n",
        "#                             df.groupby(['Store', 'Dept'])['Weekly_Sales']\n",
        "#                             .shift(1).rolling(window=window).mean()\n",
        "#                         )\n",
        "#                     elif stat == 'std':\n",
        "#                         df[f'rolling_{stat}_{window}'] = (\n",
        "#                             df.groupby(['Store', 'Dept'])['Weekly_Sales']\n",
        "#                             .shift(1).rolling(window=window).std()\n",
        "#                         )\n",
        "#                 print(f\"Created rolling_{stat}_{window} feature\")\n",
        "\n",
        "#         # Create comprehensive date features (from XGBoost preprocessing)\n",
        "#         if self.create_date_features and 'Date' in df.columns:\n",
        "#             df[\"Year\"] = df[\"Date\"].dt.year\n",
        "#             df[\"Month\"] = df[\"Date\"].dt.month\n",
        "#             df[\"Week\"] = df[\"Date\"].dt.isocalendar().week\n",
        "#             df[\"DayOfWeek\"] = df[\"Date\"].dt.dayofweek\n",
        "#             df[\"IsMonthStart\"] = df[\"Date\"].dt.is_month_start.astype(int)\n",
        "#             df[\"IsMonthEnd\"] = df[\"Date\"].dt.is_month_end.astype(int)\n",
        "#             df[\"Quarter\"] = df[\"Date\"].dt.quarter\n",
        "#             print(\"Created comprehensive date features\")\n",
        "\n",
        "#         # Enhanced seasonal features\n",
        "#         if self.create_seasonal:\n",
        "#             if 'Month' in df.columns or self.create_date_features:\n",
        "#                 if 'Month' not in df.columns:\n",
        "#                     df['Month'] = df['Date'].dt.month\n",
        "#                 if 'Week' not in df.columns:\n",
        "#                     df['Week'] = df['Date'].dt.isocalendar().week\n",
        "\n",
        "#                 df['Is_Holiday_Season'] = ((df['Month'] == 11) | (df['Month'] == 12)).astype(int)\n",
        "#                 df['Is_Summer'] = ((df['Month'] >= 6) & (df['Month'] <= 8)).astype(int)\n",
        "#                 df['Sin_Week'] = np.sin(2 * np.pi * df['Week'] / 52)\n",
        "#                 df['Cos_Week'] = np.cos(2 * np.pi * df['Week'] / 52)\n",
        "#                 df['Sin_Month'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
        "#                 df['Cos_Month'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
        "#                 print(\"Created seasonal features\")\n",
        "\n",
        "#         # Drop rows with missing lag/rolling values (following XGBoost approach)\n",
        "#         if self.drop_na_rows:\n",
        "#             original_count = len(df)\n",
        "#             df = df.dropna()\n",
        "#             dropped_count = original_count - len(df)\n",
        "#             if dropped_count > 0:\n",
        "#                 print(f\"Dropped {dropped_count} rows with missing lag/rolling features\")\n",
        "\n",
        "#         return df\n",
        "\n",
        "# class TimeSeriesValidator(BaseEstimator, TransformerMixin):\n",
        "#     \"\"\"\n",
        "#     Time series validation component\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, min_observations=20, test_stationarity=True):\n",
        "#         self.min_observations = min_observations\n",
        "#         self.test_stationarity = test_stationarity\n",
        "#         self.valid_series = set()\n",
        "#         self.stationarity_results = {}\n",
        "\n",
        "#     def fit(self, data_dict, y=None):\n",
        "#         \"\"\"Fit method - validates time series data\"\"\"\n",
        "#         train_data = data_dict['train'].copy()\n",
        "\n",
        "#         # Check each store-department combination\n",
        "#         for store_dept in train_data['Store_Dept'].unique():\n",
        "#             subset = train_data[train_data['Store_Dept'] == store_dept]\n",
        "\n",
        "#             # Check minimum observations\n",
        "#             if len(subset) >= self.min_observations:\n",
        "#                 self.valid_series.add(store_dept)\n",
        "\n",
        "#                 # Test stationarity\n",
        "#                 if self.test_stationarity:\n",
        "#                     sales = subset['Weekly_Sales'].dropna()\n",
        "#                     if len(sales) > 10:\n",
        "#                         try:\n",
        "#                             adf_result = adfuller(sales)\n",
        "#                             self.stationarity_results[store_dept] = {\n",
        "#                                 'adf_statistic': adf_result[0],\n",
        "#                                 'p_value': adf_result[1],\n",
        "#                                 'is_stationary': adf_result[1] <= 0.05\n",
        "#                             }\n",
        "#                         except:\n",
        "#                             self.stationarity_results[store_dept] = {\n",
        "#                                 'is_stationary': False\n",
        "#                             }\n",
        "\n",
        "#         print(f\"Validated {len(self.valid_series)} time series out of {train_data['Store_Dept'].nunique()}\")\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, data_dict):\n",
        "#         \"\"\"Transform method - filters valid time series\"\"\"\n",
        "#         validated_data = {}\n",
        "\n",
        "#         for key, df in data_dict.items():\n",
        "#             if 'Store_Dept' in df.columns:\n",
        "#                 validated_df = df[df['Store_Dept'].isin(self.valid_series)].copy()\n",
        "#             else:\n",
        "#                 validated_df = df.copy()\n",
        "\n",
        "#             validated_data[key] = validated_df\n",
        "\n",
        "#         return validated_data\n",
        "\n",
        "# class ARIMAModelTrainer(BaseEstimator, TransformerMixin):\n",
        "#     \"\"\"\n",
        "#     ARIMA model training component\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, max_p=3, max_d=2, max_q=3, seasonal=False,\n",
        "#                   use_auto_arima=False, n_jobs=1):\n",
        "#         self.max_p = max_p\n",
        "#         self.max_d = max_d\n",
        "#         self.max_q = max_q\n",
        "#         self.seasonal = seasonal\n",
        "#         self.use_auto_arima = use_auto_arima\n",
        "#         self.n_jobs = n_jobs\n",
        "#         self.models = {}\n",
        "#         self.model_params = {}\n",
        "#         self.fitted_values = {}\n",
        "\n",
        "#     def fit(self, data_dict, y=None):\n",
        "#         \"\"\"Fit method - trains ARIMA models for each time series\"\"\"\n",
        "#         train_data = data_dict['train'].copy()\n",
        "\n",
        "#         store_dept_list = train_data['Store_Dept'].unique()\n",
        "#         successful_fits = 0\n",
        "\n",
        "#         print(f\"Training ARIMA models for {len(store_dept_list)} time series...\")\n",
        "\n",
        "#         for i, store_dept in enumerate(store_dept_list):\n",
        "#             if (i + 1) % 10 == 0:\n",
        "#                 print(f\"Progress: {i + 1}/{len(store_dept_list)}\")\n",
        "\n",
        "#             subset = train_data[train_data['Store_Dept'] == store_dept].copy()\n",
        "#             subset = subset.sort_values('Date')\n",
        "\n",
        "#             # Create time series\n",
        "#             ts_data = subset.set_index('Date')['Weekly_Sales']\n",
        "\n",
        "#             # Fill missing dates\n",
        "#             date_range = pd.date_range(start=ts_data.index.min(),\n",
        "#                                       end=ts_data.index.max(),\n",
        "#                                       freq='W-FRI')\n",
        "#             ts_data = ts_data.reindex(date_range, fill_value=0)\n",
        "\n",
        "#             # Fit ARIMA model\n",
        "#             try:\n",
        "#                 if self.use_auto_arima:\n",
        "#                     # Use auto_arima if available\n",
        "#                     try:\n",
        "#                         from pmdarima import auto_arima\n",
        "#                         model = auto_arima(ts_data,\n",
        "#                                           max_p=self.max_p,\n",
        "#                                           max_d=self.max_d,\n",
        "#                                           max_q=self.max_q,\n",
        "#                                           seasonal=self.seasonal,\n",
        "#                                           suppress_warnings=True)\n",
        "#                         fitted_model = model\n",
        "#                         optimal_params = model.order\n",
        "#                     except ImportError:\n",
        "#                         print(\"pmdarima not available, using grid search\")\n",
        "#                         optimal_params = self._find_optimal_parameters(ts_data)\n",
        "#                         model = ARIMA(ts_data, order=optimal_params)\n",
        "#                         fitted_model = model.fit()\n",
        "#                 else:\n",
        "#                     optimal_params = self._find_optimal_parameters(ts_data)\n",
        "#                     model = ARIMA(ts_data, order=optimal_params)\n",
        "#                     fitted_model = model.fit()\n",
        "\n",
        "#                 self.models[store_dept] = fitted_model\n",
        "#                 self.model_params[store_dept] = optimal_params\n",
        "\n",
        "#                 try:\n",
        "#                     self.fitted_values[store_dept] = fitted_model.fittedvalues\n",
        "#                 except:\n",
        "#                     pass\n",
        "\n",
        "#                 successful_fits += 1\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 continue\n",
        "\n",
        "#         print(f\"Successfully trained {successful_fits} ARIMA models\")\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, data_dict):\n",
        "#         \"\"\"Transform method - returns model predictions\"\"\"\n",
        "#         # This method can be used to return fitted values or diagnostics\n",
        "#         result_dict = data_dict.copy()\n",
        "#         result_dict['models'] = self.models\n",
        "#         result_dict['model_params'] = self.model_params\n",
        "#         return result_dict\n",
        "\n",
        "#     def _find_optimal_parameters(self, ts_data):\n",
        "#         \"\"\"Find optimal ARIMA parameters using grid search\"\"\"\n",
        "#         best_aic = float('inf')\n",
        "#         best_params = (1, 1, 1)\n",
        "\n",
        "#         for p in range(self.max_p + 1):\n",
        "#             for d in range(self.max_d + 1):\n",
        "#                 for q in range(self.max_q + 1):\n",
        "#                     try:\n",
        "#                         model = ARIMA(ts_data, order=(p, d, q))\n",
        "#                         fitted_model = model.fit()\n",
        "\n",
        "#                         if fitted_model.aic < best_aic:\n",
        "#                             best_aic = fitted_model.aic\n",
        "#                             best_params = (p, d, q)\n",
        "#                     except:\n",
        "#                         continue\n",
        "\n",
        "#         return best_params\n",
        "\n",
        "# class ARIMAForecaster(BaseEstimator, TransformerMixin):\n",
        "#     \"\"\"\n",
        "#     ARIMA forecasting component\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, forecast_horizon=None):\n",
        "#         self.forecast_horizon = forecast_horizon\n",
        "#         self.forecasts = {}\n",
        "\n",
        "#     def fit(self, data_dict, y=None):\n",
        "#         \"\"\"Fit method - prepares for forecasting\"\"\"\n",
        "#         self.models = data_dict.get('models', {})\n",
        "#         self.test_data = data_dict.get('test', pd.DataFrame())\n",
        "\n",
        "#         if self.forecast_horizon is None and not self.test_data.empty:\n",
        "#             # Determine forecast horizon from test dates\n",
        "#             test_dates = self.test_data['Date'].unique()\n",
        "#             self.forecast_horizon = len(test_dates)\n",
        "\n",
        "#         print(f\"Forecaster prepared for {len(self.models)} models with horizon {self.forecast_horizon}\")\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, data_dict):\n",
        "#         \"\"\"Transform method - generates forecasts\"\"\"\n",
        "#         forecast_results = []\n",
        "\n",
        "#         for store_dept, model in self.models.items():\n",
        "#             try:\n",
        "#                 forecast = model.forecast(steps=self.forecast_horizon)\n",
        "\n",
        "#                 # Create forecast entries\n",
        "#                 store, dept = store_dept.split('_')\n",
        "#                 store, dept = int(store), int(dept)\n",
        "\n",
        "#                 # Get test dates for this store-dept combination\n",
        "#                 if not self.test_data.empty:\n",
        "#                     test_subset = self.test_data[\n",
        "#                         (self.test_data['Store'] == store) &\n",
        "#                         (self.test_data['Dept'] == dept)\n",
        "#                     ].sort_values('Date')\n",
        "\n",
        "#                     for i, (_, row) in enumerate(test_subset.iterrows()):\n",
        "#                         if i < len(forecast):\n",
        "#                             forecast_results.append({\n",
        "#                                 'Store': store,\n",
        "#                                 'Dept': dept,\n",
        "#                                 'Date': row['Date'],\n",
        "#                                 'Weekly_Sales': max(0, forecast.iloc[i])\n",
        "#                             })\n",
        "\n",
        "#                 self.forecasts[store_dept] = forecast\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 continue\n",
        "\n",
        "#         forecast_df = pd.DataFrame(forecast_results)\n",
        "\n",
        "#         result_dict = data_dict.copy()\n",
        "#         result_dict['forecasts'] = forecast_df\n",
        "#         return result_dict\n",
        "\n",
        "# class WalmartARIMAPipeline:\n",
        "#     \"\"\"\n",
        "#     Main pipeline class that orchestrates all components (with XGBoost-style preprocessing)\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, train_path=None, test_path=None, stores_path=None, features_path=None):\n",
        "#         # Initialize pipeline components with XGBoost-style preprocessing\n",
        "#         self.pipeline = Pipeline([\n",
        "#             ('loader', DataLoader(train_path, test_path, stores_path, features_path)),\n",
        "#             ('cleaner', DataCleaner(\n",
        "#                 merge_external_data=True,\n",
        "#                 filter_positive_sales=True,\n",
        "#                 remove_outliers=True,\n",
        "#                 outlier_quantiles=(0.01, 0.99)\n",
        "#             )),\n",
        "#             ('feature_engineer', FeatureEngineer(\n",
        "#                 create_lags=True,\n",
        "#                 lag_periods=[1, 2, 4, 52],  # XGBoost lag periods\n",
        "#                 create_rolling=True,\n",
        "#                 rolling_windows=[4],  # XGBoost rolling window\n",
        "#                 rolling_stats=['mean', 'std'],  # XGBoost rolling stats\n",
        "#                 create_date_features=True,  # XGBoost comprehensive date features\n",
        "#                 drop_na_rows=True  # XGBoost approach to handle NaN\n",
        "#             )),\n",
        "#             ('validator', TimeSeriesValidator(min_observations=15)),\n",
        "#             ('model_trainer', ARIMAModelTrainer(max_p=3, max_d=2, max_q=3)),\n",
        "#             ('forecaster', ARIMAForecaster())\n",
        "#         ])\n",
        "\n",
        "#     def fit(self, X=None, y=None):\n",
        "#         \"\"\"Fit the entire pipeline\"\"\"\n",
        "#         print(\"Starting Walmart ARIMA Pipeline...\")\n",
        "#         self.pipeline.fit(X, y)\n",
        "#         return self\n",
        "\n",
        "#     def predict(self, X=None):\n",
        "#         \"\"\"Generate predictions using the fitted pipeline\"\"\"\n",
        "#         result = self.pipeline.transform(X)\n",
        "#         return result.get('forecasts', pd.DataFrame())\n",
        "\n",
        "#     def get_component(self, component_name):\n",
        "#         \"\"\"Get a specific pipeline component\"\"\"\n",
        "#         return self.pipeline.named_steps.get(component_name)\n",
        "\n",
        "#     def evaluate_models(self, test_size=0.2, use_wmae=True):\n",
        "#         \"\"\"Evaluate model performance with WMAE option (from XGBoost)\"\"\"\n",
        "#         model_trainer = self.get_component('model_trainer')\n",
        "#         data_loader = self.get_component('loader')\n",
        "\n",
        "#         if not model_trainer or not data_loader:\n",
        "#             print(\"Pipeline not fitted yet\")\n",
        "#             return None\n",
        "\n",
        "#         # Get training data\n",
        "#         train_data = data_loader.train_data.copy()\n",
        "#         train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "#         train_data['Store_Dept'] = train_data['Store'].astype(str) + '_' + train_data['Dept'].astype(str)\n",
        "\n",
        "#         # Weighted MAE function (from XGBoost)\n",
        "#         def weighted_mae(y_true, y_pred, is_holiday):\n",
        "#             weights = np.where(is_holiday, 5, 1)\n",
        "#             return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "#         evaluation_results = {}\n",
        "\n",
        "#         # Sample a few store-dept combinations for evaluation\n",
        "#         sample_store_depts = list(model_trainer.models.keys())[:5]\n",
        "\n",
        "#         for store_dept in sample_store_depts:\n",
        "#             if store_dept in model_trainer.models:\n",
        "#                 subset = train_data[train_data['Store_Dept'] == store_dept].copy()\n",
        "#                 subset = subset.sort_values('Date')\n",
        "#                 ts_data = subset.set_index('Date')['Weekly_Sales']\n",
        "\n",
        "#                 # Get holiday information if available\n",
        "#                 is_holiday_data = None\n",
        "#                 if 'IsHoliday' in subset.columns:\n",
        "#                     is_holiday_data = subset.set_index('Date')['IsHoliday'].astype(bool)\n",
        "\n",
        "#                 # Split data\n",
        "#                 split_point = int(len(ts_data) * (1 - test_size))\n",
        "#                 train_split = ts_data.iloc[:split_point]\n",
        "#                 test_split = ts_data.iloc[split_point:]\n",
        "\n",
        "#                 if is_holiday_data is not None:\n",
        "#                     holiday_test = is_holiday_data.iloc[split_point:].values\n",
        "#                 else:\n",
        "#                     holiday_test = np.zeros(len(test_split), dtype=bool)\n",
        "\n",
        "#                 if len(test_split) > 0:\n",
        "#                     try:\n",
        "#                         # Fit model on training split\n",
        "#                         model = ARIMA(train_split, order=model_trainer.model_params[store_dept])\n",
        "#                         fitted_model = model.fit()\n",
        "\n",
        "#                         # Generate forecast\n",
        "#                         forecast = fitted_model.forecast(steps=len(test_split))\n",
        "\n",
        "#                         # Calculate metrics\n",
        "#                         mae = mean_absolute_error(test_split, forecast)\n",
        "#                         rmse = np.sqrt(mean_squared_error(test_split, forecast))\n",
        "\n",
        "#                         # Calculate WMAE if requested and holiday data available\n",
        "#                         wmae = None\n",
        "#                         if use_wmae and is_holiday_data is not None:\n",
        "#                             wmae = weighted_mae(test_split.values, forecast.values, holiday_test)\n",
        "\n",
        "#                         evaluation_results[store_dept] = {\n",
        "#                             'MAE': mae,\n",
        "#                             'RMSE': rmse,\n",
        "#                             'WMAE': wmae,\n",
        "#                             'Model_Params': model_trainer.model_params[store_dept]\n",
        "#                         }\n",
        "\n",
        "#                         print(f\"{store_dept}: MAE={mae:.2f}, RMSE={rmse:.2f}\", end=\"\")\n",
        "#                         if wmae is not None:\n",
        "#                             print(f\", WMAE={wmae:.2f}\")\n",
        "#                         else:\n",
        "#                             print()\n",
        "\n",
        "#                     except Exception as e:\n",
        "#                         continue\n",
        "\n",
        "#         return evaluation_results\n",
        "\n",
        "#     def save_submission(self, filename='walmart_arima_submission.csv'):\n",
        "#         \"\"\"Save forecasts in competition format\"\"\"\n",
        "#         forecasts = self.predict()\n",
        "\n",
        "#         if forecasts.empty:\n",
        "#             print(\"No forecasts available\")\n",
        "#             return\n",
        "\n",
        "#         # Get test data\n",
        "#         data_loader = self.get_component('loader')\n",
        "#         test_data = data_loader.test_data.copy()\n",
        "#         test_data['Date'] = pd.to_datetime(test_data['Date'])\n",
        "\n",
        "#         # Merge forecasts with test data\n",
        "#         submission = test_data.merge(\n",
        "#             forecasts,\n",
        "#             on=['Store', 'Dept', 'Date'],\n",
        "#             how='left'\n",
        "#         )\n",
        "\n",
        "#         # Fill missing forecasts with 0\n",
        "#         submission['Weekly_Sales'] = submission['Weekly_Sales'].fillna(0)\n",
        "\n",
        "#         # Create Id column\n",
        "#         submission['Id'] = (submission['Store'].astype(str) + '_' +\n",
        "#                             submission['Dept'].astype(str) + '_' +\n",
        "#                             submission['Date'].dt.strftime('%Y-%m-%d'))\n",
        "\n",
        "#         # Save submission\n",
        "#         submission[['Id', 'Weekly_Sales']].to_csv(filename, index=False)\n",
        "#         print(f\"Submission saved as {filename}\")\n",
        "\n",
        "# # Example usage with XGBoost-style preprocessing\n",
        "# if __name__ == \"__main__\":\n",
        "# # Initialize pipeline with XGBoost-style preprocessing\n",
        "#     pipeline = WalmartARIMAPipeline(\n",
        "#         train_path='train.csv',\n",
        "#         test_path='test.csv',\n",
        "#         stores_path='stores.csv',\n",
        "#         features_path='features.csv'\n",
        "#     )\n",
        "\n",
        "# # Fit pipeline (includes XGBoost preprocessing steps)\n",
        "# # pipeline.fit()\n",
        "\n",
        "# # Generate predictions\n",
        "# # forecasts = pipeline.predict()\n",
        "\n",
        "# # Evaluate models with WMAE (Walmart's competition metric)\n",
        "# # evaluation = pipeline.evaluate_models(use_wmae=True)\n",
        "\n",
        "# # Save submission\n",
        "# # pipeline.save_submission('walmart_arima_submission.csv')\n",
        "\n",
        "# print(\"Walmart ARIMA Pipeline with XGBoost-style preprocessing initialized!\")\n",
        "# print(\"\\nKey preprocessing features adapted from XGBoost:\")\n",
        "# print(\"✅ Merge external data (features.csv, stores.csv)\")\n",
        "# print(\"✅ Filter positive sales only\")\n",
        "# print(\"✅ Remove outliers using 1st-99th percentile\")\n",
        "# print(\"✅ Fill MarkDown columns with 0\")\n",
        "# print(\"✅ Handle IsHoliday column naming conflicts\")\n",
        "# print(\"✅ Create lag features [1, 2, 4, 52]\")\n",
        "# print(\"✅ Create rolling statistics (mean, std) with window=4\")\n",
        "# print(\"✅ Comprehensive date features (Year, Month, Week, etc.)\")\n",
        "# print(\"✅ Drop rows with missing lag/rolling values\")\n",
        "# print(\"✅ WMAE evaluation metric support\")"
      ],
      "metadata": {
        "id": "iGE5mNPBVSIQ",
        "collapsed": true
      },
      "id": "iGE5mNPBVSIQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_path = 'train.csv'\n",
        "# test_path = 'test.csv'\n",
        "# stores_path = 'stores.csv'\n",
        "# features_path = 'features.csv'\n",
        "\n",
        "\n",
        "# pipeline = WalmartARIMAPipeline(\n",
        "#     train_path=train_path,\n",
        "#     test_path=test_path,\n",
        "#     stores_path=stores_path,\n",
        "#     features_path=features_path\n",
        "# )\n",
        "\n",
        "# # Train the pipeline\n",
        "# pipeline.fit()\n"
      ],
      "metadata": {
        "id": "aBtqJCm-_r4H",
        "collapsed": true
      },
      "id": "aBtqJCm-_r4H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forecast_df = pipeline.predict()\n",
        "# print(forecast_df.head())\n"
      ],
      "metadata": {
        "id": "sJmt01vdVSGT"
      },
      "id": "sJmt01vdVSGT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train[\"Date\"] = pd.to_datetime(train[\"Date\"])\n",
        "# monthly_sales = train.groupby(pd.Grouper(key=\"Date\", freq=\"ME\"))[\"Weekly_Sales\"].sum()"
      ],
      "metadata": {
        "id": "cQlyLgBca55o"
      },
      "id": "cQlyLgBca55o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# monthly_sales.head()"
      ],
      "metadata": {
        "id": "WTGJe1UxcCJa",
        "collapsed": true
      },
      "id": "WTGJe1UxcCJa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train[\"Date\"] = pd.to_datetime(train[\"Date\"])\n",
        "# weekly_sales = train.groupby(pd.Grouper(key=\"Date\", freq=\"W\"))[\"Weekly_Sales\"].sum()\n",
        "# weekly_sales.head()"
      ],
      "metadata": {
        "id": "A5I7ePaHcFYS",
        "collapsed": true
      },
      "id": "A5I7ePaHcFYS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(12, 6))\n",
        "# plt.plot(monthly_sales, label=\"Monthly_Sales\")\n",
        "# plt.title(\"Monthly Sales Over Time\")\n",
        "# plt.xlabel(\"Date\")\n",
        "# plt.ylabel(\"Sales\")\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "aQVnWrigcgW8",
        "collapsed": true
      },
      "id": "aQVnWrigcgW8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(12, 6))\n",
        "# plt.plot(weekly_sales, label=\"Weekly_Sales\")\n",
        "# plt.title(\"Weekly Sales Over Time\")\n",
        "# plt.xlabel(\"Date\")\n",
        "# plt.ylabel(\"Sales\")\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "jU_x-uV_eC5a",
        "collapsed": true
      },
      "id": "jU_x-uV_eC5a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# decomp = seasonal_decompose(monthly_sales, model=\"additive\")\n",
        "# fig = decomp.plot()\n",
        "# fig.set_size_inches(14, 10)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "MIuOdDU3dICl",
        "collapsed": true
      },
      "id": "MIuOdDU3dICl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# def check_stationarity(timeseries):\n",
        "#     result = adfuller(timeseries)\n",
        "#     print(\"Augmented Dickey-Fuller Test Results\")\n",
        "#     print(\"ADF Statistic:\", result[0])\n",
        "#     print(\"p-value:\", result[1])\n",
        "#     print(\"Critical Values:\")\n",
        "\n",
        "#     for key, value in result[4].items():\n",
        "#         print(f\"\\t{key}: {value}\")\n",
        "#     return result[1] < 0.05\n",
        "\n",
        "# is_stationary = check_stationarity(monthly_sales)\n",
        "# print(f\"\\nTime Series is {'stationary' if is_stationary else 'non-stationary'}\")\n"
      ],
      "metadata": {
        "id": "OWkOVQpEde6k",
        "collapsed": true
      },
      "id": "OWkOVQpEde6k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def find_best_arima(self, data, max_p=5, max_d=2, max_q=5):\n",
        "#         \"\"\"\n",
        "#         Manual grid search for best ARIMA parameters\n",
        "#         \"\"\"\n",
        "#         best_aic = float('inf')\n",
        "#         best_params = None\n",
        "#         best_model = None\n",
        "\n",
        "#         print(\"Searching for best ARIMA parameters...\")\n",
        "\n",
        "#         for p in range(max_p + 1):\n",
        "#             for d in range(max_d + 1):\n",
        "#                 for q in range(max_q + 1):\n",
        "#                     try:\n",
        "#                         model = ARIMA(data, order=(p, d, q))\n",
        "#                         fitted_model = model.fit()\n",
        "\n",
        "#                         if fitted_model.aic < best_aic:\n",
        "#                             best_aic = fitted_model.aic\n",
        "#                             best_params = (p, d, q)\n",
        "#                             best_model = fitted_model\n",
        "\n",
        "#                     except Exception as e:\n",
        "#                         continue\n",
        "\n",
        "#         print(f\"Best ARIMA parameters: {best_params}\")\n",
        "#         print(f\"Best AIC: {best_aic}\")\n",
        "\n",
        "#         self.best_params = best_params\n",
        "#         self.fitted_model = best_model\n",
        "\n",
        "#         return best_params, best_model"
      ],
      "metadata": {
        "id": "_JuHt81qj1yK"
      },
      "id": "_JuHt81qj1yK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# auto_model = auto_sarima(monthly_sales, start_p=0, start_q=0, max_p=5, max_q=5, m=12, seasonal=True, d=None, trace=True, error_action=\"ignore\", suppress_warnings=True, stepwise=True)"
      ],
      "metadata": {
        "id": "COCFP5gettjX"
      },
      "id": "COCFP5gettjX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import warnings\n",
        "# from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "# from statsmodels.tsa.stattools import adfuller\n",
        "# from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "# from itertools import product\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "# def auto_sarima_alternative(data,\n",
        "#                           start_p=0, start_q=0,\n",
        "#                           max_p=5, max_q=5,\n",
        "#                           m=12, seasonal=True,\n",
        "#                           d=None, D=None,\n",
        "#                           trace=True,\n",
        "#                           error_action=\"ignore\",\n",
        "#                           stepwise=True,\n",
        "#                           information_criterion='aic',\n",
        "#                           max_order=10):\n",
        "\n",
        "#     # Convert to pandas Series if needed\n",
        "#     if not isinstance(data, pd.Series):\n",
        "#         data = pd.Series(data)\n",
        "\n",
        "#     # Remove any NaN values\n",
        "#     data = data.dropna()\n",
        "\n",
        "#     if trace:\n",
        "#         print(\"Performing stepwise search to minimize {}...\".format(information_criterion.upper()))\n",
        "#         print(f\"Data length: {len(data)}\")\n",
        "\n",
        "#     # Auto-determine differencing orders if not provided\n",
        "#     if d is None:\n",
        "#         d = determine_differencing_order(data, trace=trace)\n",
        "\n",
        "#     if D is None and seasonal:\n",
        "#         D = determine_seasonal_differencing_order(data, m, trace=trace)\n",
        "#     elif not seasonal:\n",
        "#         D = 0\n",
        "\n",
        "#     # Define parameter ranges\n",
        "#     p_range = range(start_p, max_p + 1)\n",
        "#     q_range = range(start_q, max_q + 1)\n",
        "\n",
        "#     if seasonal:\n",
        "#         P_range = range(0, min(3, max_p + 1))  # Usually 0-2 for seasonal P\n",
        "#         Q_range = range(0, min(3, max_q + 1))  # Usually 0-2 for seasonal Q\n",
        "#     else:\n",
        "#         P_range = [0]\n",
        "#         Q_range = [0]\n",
        "\n",
        "#     best_ic = float('inf')\n",
        "#     best_model = None\n",
        "#     best_order = None\n",
        "#     best_seasonal_order = None\n",
        "\n",
        "#     # Generate all combinations\n",
        "#     if stepwise:\n",
        "#         # Stepwise search (more efficient)\n",
        "#         param_combinations = stepwise_search(p_range, q_range, P_range, Q_range, d, D, m, max_order)\n",
        "#     else:\n",
        "#         # Full grid search\n",
        "#         param_combinations = list(product(p_range, [d], q_range, P_range, [D], Q_range))\n",
        "#         param_combinations = [(p, d, q, P, D, Q) for p, d, q, P, D, Q in param_combinations\n",
        "#                             if p + q + P + Q <= max_order]\n",
        "\n",
        "#     total_combinations = len(param_combinations)\n",
        "#     if trace:\n",
        "#         print(f\"Fitting {total_combinations} models...\")\n",
        "\n",
        "#     for i, (p, d_val, q, P, D_val, Q) in enumerate(param_combinations):\n",
        "#         try:\n",
        "#             # Fit SARIMA model\n",
        "#             order = (p, d_val, q)\n",
        "#             seasonal_order = (P, D_val, Q, m) if seasonal else (0, 0, 0, 0)\n",
        "\n",
        "#             model = SARIMAX(data,\n",
        "#                           order=order,\n",
        "#                           seasonal_order=seasonal_order,\n",
        "#                           enforce_stationarity=False,\n",
        "#                           enforce_invertibility=False)\n",
        "\n",
        "#             fitted_model = model.fit(disp=False)\n",
        "\n",
        "#             # Get information criterion\n",
        "#             if information_criterion == 'aic':\n",
        "#                 ic_value = fitted_model.aic\n",
        "#             elif information_criterion == 'bic':\n",
        "#                 ic_value = fitted_model.bic\n",
        "#             elif information_criterion == 'hqic':\n",
        "#                 ic_value = fitted_model.hqic\n",
        "#             else:\n",
        "#                 ic_value = fitted_model.aic\n",
        "\n",
        "#             if trace:\n",
        "#                 print(f\"SARIMA{order}x{seasonal_order} - {information_criterion.upper()}: {ic_value:.3f}\")\n",
        "\n",
        "#             # Check if this is the best model so far\n",
        "#             if ic_value < best_ic:\n",
        "#                 best_ic = ic_value\n",
        "#                 best_model = fitted_model\n",
        "#                 best_order = order\n",
        "#                 best_seasonal_order = seasonal_order\n",
        "\n",
        "#         except Exception as e:\n",
        "#             if error_action == \"ignore\":\n",
        "#                 if trace:\n",
        "#                     print(f\"SARIMA{(p, d_val, q)}x{(P, D_val, Q, m)} - Failed: {str(e)[:50]}\")\n",
        "#                 continue\n",
        "#             else:\n",
        "#                 raise e\n",
        "\n",
        "#     if best_model is None:\n",
        "#         raise ValueError(\"No suitable model found. Try expanding parameter ranges.\")\n",
        "\n",
        "#     if trace:\n",
        "#         print(f\"\\nBest model: SARIMA{best_order}x{best_seasonal_order}\")\n",
        "#         print(f\"Best {information_criterion.upper()}: {best_ic:.3f}\")\n",
        "\n",
        "#     return best_model, best_order, best_seasonal_order\n",
        "\n",
        "# def determine_differencing_order(data, max_d=2, trace=False):\n",
        "#     \"\"\"Determine optimal differencing order using ADF test\"\"\"\n",
        "#     for d in range(max_d + 1):\n",
        "#         if d == 0:\n",
        "#             diff_data = data\n",
        "#         else:\n",
        "#             diff_data = data.diff(periods=d).dropna()\n",
        "\n",
        "#         # Perform ADF test\n",
        "#         adf_result = adfuller(diff_data)\n",
        "#         p_value = adf_result[1]\n",
        "\n",
        "#         if trace:\n",
        "#             print(f\"Testing d={d}: ADF p-value = {p_value:.4f}\")\n",
        "\n",
        "#         # If p-value < 0.05, series is stationary\n",
        "#         if p_value < 0.05:\n",
        "#             if trace:\n",
        "#                 print(f\"Series is stationary with d={d}\")\n",
        "#             return d\n",
        "\n",
        "#     if trace:\n",
        "#         print(f\"Using maximum d={max_d}\")\n",
        "#     return max_d\n",
        "\n",
        "# def determine_seasonal_differencing_order(data, m, max_D=1, trace=False):\n",
        "#     \"\"\"Determine seasonal differencing order\"\"\"\n",
        "#     for D in range(max_D + 1):\n",
        "#         if D == 0:\n",
        "#             diff_data = data\n",
        "#         else:\n",
        "#             diff_data = data.diff(periods=m*D).dropna()\n",
        "\n",
        "#         if len(diff_data) < 2*m:  # Need enough data\n",
        "#             continue\n",
        "\n",
        "#         # Simple test for seasonal stationarity\n",
        "#         # You could implement a more sophisticated test here\n",
        "#         try:\n",
        "#             adf_result = adfuller(diff_data)\n",
        "#             p_value = adf_result[1]\n",
        "\n",
        "#             if trace:\n",
        "#                 print(f\"Testing seasonal D={D}: ADF p-value = {p_value:.4f}\")\n",
        "\n",
        "#             if p_value < 0.05:\n",
        "#                 if trace:\n",
        "#                     print(f\"Seasonally stationary with D={D}\")\n",
        "#                 return D\n",
        "#         except:\n",
        "#             continue\n",
        "\n",
        "#     return 0\n",
        "\n",
        "# def stepwise_search(p_range, q_range, P_range, Q_range, d, D, m, max_order):\n",
        "#     \"\"\"\n",
        "#     Implement a stepwise search similar to auto_arima\n",
        "#     Start with simple models and expand\n",
        "#     \"\"\"\n",
        "#     # Start with simple models\n",
        "#     initial_models = [\n",
        "#         (0, d, 0, 0, D, 0),  # (0,d,0)(0,D,0)\n",
        "#         (1, d, 0, 0, D, 0),  # (1,d,0)(0,D,0)\n",
        "#         (0, d, 1, 0, D, 0),  # (0,d,1)(0,D,0)\n",
        "#         (1, d, 1, 0, D, 0),  # (1,d,1)(0,D,0)\n",
        "#     ]\n",
        "\n",
        "#     # Add seasonal variants if applicable\n",
        "#     if max(P_range) > 0 or max(Q_range) > 0:\n",
        "#         seasonal_models = [\n",
        "#             (0, d, 0, 1, D, 0),  # (0,d,0)(1,D,0)\n",
        "#             (0, d, 0, 0, D, 1),  # (0,d,0)(0,D,1)\n",
        "#             (1, d, 0, 1, D, 0),  # (1,d,0)(1,D,0)\n",
        "#             (0, d, 1, 0, D, 1),  # (0,d,1)(0,D,1)\n",
        "#         ]\n",
        "#         initial_models.extend(seasonal_models)\n",
        "\n",
        "#     # Generate more combinations around promising areas\n",
        "#     all_combinations = []\n",
        "#     for p in p_range:\n",
        "#         for q in q_range:\n",
        "#             for P in P_range:\n",
        "#                 for Q in Q_range:\n",
        "#                     if p + q + P + Q <= max_order:\n",
        "#                         all_combinations.append((p, d, q, P, D, Q))\n",
        "\n",
        "#     # Combine initial models with full search, removing duplicates\n",
        "#     combined = list(set(initial_models + all_combinations))\n",
        "\n",
        "#     return combined\n",
        "\n",
        "\n",
        "# best_model, best_order, best_seasonal_order = auto_sarima_alternative(\n",
        "#         monthly_sales,\n",
        "#         start_p=0, start_q=0,\n",
        "#         max_p=5, max_q=5,\n",
        "#         m=12, seasonal=True,\n",
        "#         d=None,\n",
        "#         trace=True,\n",
        "#         error_action=\"ignore\",\n",
        "#         stepwise=True\n",
        "#     )"
      ],
      "metadata": {
        "id": "O_2Nxdk8vbyQ",
        "collapsed": true
      },
      "id": "O_2Nxdk8vbyQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from statsmodels.tsa.arima.model import ARIMA"
      ],
      "metadata": {
        "id": "M-avhZHRyRWU"
      },
      "id": "M-avhZHRyRWU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# order = (1, 0, 5)\n",
        "# seasonal_order = (0, 0, 2, 12)\n",
        "# model = ARIMA(monthly_sales, order=order, seasonal_order=seasonal_order)\n",
        "# results = model.fit()"
      ],
      "metadata": {
        "id": "h-UxBBbkwTIB"
      },
      "id": "h-UxBBbkwTIB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# per = 12\n",
        "# forecast = results.get_forecast(steps=per)\n",
        "# mean_forecast = forecast.predicted_mean"
      ],
      "metadata": {
        "id": "B6BC8U1-ySPR"
      },
      "id": "B6BC8U1-ySPR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# conf_int_95 = forecast.conf_int(alpha=0.05)\n",
        "# conf_int_80 = forecast.conf_int(alpha=0.20)\n",
        "# conf_int_70 = forecast.conf_int(alpha=0.30)"
      ],
      "metadata": {
        "id": "MseM2C8Oy5vW"
      },
      "id": "MseM2C8Oy5vW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(15, 7))\n",
        "\n",
        "# plt.plot(monthly_sales, label=\"Historical Data\", color=\"blue\")\n",
        "# plt.plot(mean_forecast, label=\"Forecast\", color=\"red\", linewidth=2)\n",
        "\n",
        "# plt.fill_between(mean_forecast.index,\n",
        "#                  conf_int_95.iloc[:, 0],\n",
        "#                  conf_int_95.iloc[:, 1],\n",
        "#                  color=\"red\", alpha=0.1,\n",
        "#                  label=\"95% CI\")\n",
        "\n",
        "# plt.fill_between(mean_forecast.index,\n",
        "#                  conf_int_80.iloc[:, 0],\n",
        "#                  conf_int_80.iloc[:, 1],\n",
        "#                  color=\"red\", alpha=0.2,\n",
        "#                  label=\"80% CI\")\n",
        "\n",
        "# plt.fill_between(mean_forecast.index,\n",
        "#                  conf_int_70.iloc[:, 0],\n",
        "#                  conf_int_70.iloc[:, 1],\n",
        "#                  color=\"red\", alpha=0.3,\n",
        "#                  label=\"70% CI\")\n",
        "\n",
        "# plt.title(\"Sales Forecast with ARIMA and Confidence Intervals\")\n",
        "# plt.xlabel(\"Date\")\n",
        "# plt.ylabel(\"Sales ($)\")\n",
        "# plt.legend()\n",
        "# plt.grid(True, alpha=0.3)\n",
        "\n",
        "# last_forecast = mean_forecast.iloc[-1]\n",
        "# ranges_95_lower = conf_int_95.iloc[-1, 0]\n",
        "# ranges_95_upper = conf_int_95.iloc[-1, 1]\n",
        "# ranges_80_lower = conf_int_95.iloc[-1, 0]\n",
        "# ranges_80_upper = conf_int_95.iloc[-1, 1]\n",
        "# ranges_70_lower = conf_int_95.iloc[-1, 0]\n",
        "# ranges_70_upper = conf_int_95.iloc[-1, 1]\n",
        "\n",
        "# info_text = f\"Final Forecast: ${last_forecast:,.0f}\\n\\n\" \\\n",
        "#             f\"95% CI: ${ranges_95_lower:,.0f} to ${ranges_95_upper:,.0f}\\n\" \\\n",
        "#             f\"80% CI: ${ranges_80_lower:,.0f} to ${ranges_80_upper:,.0f}\\n\" \\\n",
        "#             f\"70% CI: ${ranges_70_lower:,.0f} to ${ranges_70_upper:,.0f}\"\n",
        "\n",
        "# plt.text(0.02, 0.98, info_text,\n",
        "#          transform=plt.gca().transAxes,\n",
        "#          verticalalignment=\"top\",\n",
        "#          bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8))\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "BQYV0q4PzMGv",
        "collapsed": true
      },
      "id": "BQYV0q4PzMGv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# print(\"\\nModel Performance Metrics:\")\n",
        "# mse = mean_squared_error(monthly_sales, results.fittedvalues)\n",
        "# rmse = np.sqrt(mse)\n",
        "# mae = mean_absolute_error(monthly_sales, results.fittedvalues)\n",
        "\n",
        "# print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "# print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "# print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "\n",
        "# print(\"\\nForecast Ranges for Final Period\")\n",
        "# print(f\"Point Forecast: ${last_forecast:,.2f}\")\n",
        "# print(\"\\nConfidence Intervals\")\n",
        "# print(f\"95% CI: ${ranges_95_lower:,.2f} to ${ranges_95_upper:,.2f}\")\n",
        "# print(f\"95% CI: ${ranges_80_lower:,.2f} to ${ranges_80_upper:,.2f}\")\n",
        "# print(f\"95% CI: ${ranges_70_lower:,.2f} to ${ranges_70_upper:,.2f}\")\n",
        "\n",
        "# print(\"\\nInterval Widths as Percentage of Forecast\")\n",
        "# print(f\"95% CI: +-{((ranges_95_upper-ranges_95_lower)/2/last_forecast*100):,.1f}%\")\n",
        "# print(f\"80% CI: +-{((ranges_80_upper-ranges_80_lower)/2/last_forecast*100):,.1f}%\")\n",
        "# print(f\"70% CI: +-{((ranges_70_upper-ranges_70_lower)/2/last_forecast*100):,.1f}%\")\n",
        "\n",
        "# forecast_df = pd.DataFrame({\n",
        "#     \"Forecast\": mean_forecast,\n",
        "#     \"95% Lower\": conf_int_95.iloc[:, 0],\n",
        "#     \"95% Lower\": conf_int_95.iloc[:, 1],\n",
        "#     \"80% Lower\": conf_int_80.iloc[:, 0],\n",
        "#     \"80% Lower\": conf_int_80.iloc[:, 1],\n",
        "#     \"70% Lower\": conf_int_70.iloc[:, 0],\n",
        "#     \"70% Lower\": conf_int_70.iloc[:, 1],\n",
        "# })\n",
        "\n",
        "# print(\"\\nDetailed Forecast with Confidence Intervals:\")\n",
        "# print(forecast_df)"
      ],
      "metadata": {
        "id": "2n0ot6Qg1L9L",
        "collapsed": true
      },
      "id": "2n0ot6Qg1L9L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "43J6aYp_Aosh"
      },
      "id": "43J6aYp_Aosh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ERTqejwQPgtq"
      },
      "id": "ERTqejwQPgtq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b3ae655",
        "collapsed": true
      },
      "source": [
        "# with open('/content/WalmartRecruiting/helpers/data_loading.py', 'r') as f:\n",
        "#     file_content = f.read()\n",
        "# print(file_content)"
      ],
      "id": "4b3ae655",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cf2ec49",
        "collapsed": true
      },
      "source": [
        "# with open('/content/WalmartRecruiting/helpers/data_preprocessing.py', 'r') as f:\n",
        "#     file_content = f.read()\n",
        "# print(file_content)"
      ],
      "id": "9cf2ec49",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P0kKm6wa2irM"
      },
      "id": "P0kKm6wa2irM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nbo5B4LT4vif"
      },
      "id": "nbo5B4LT4vif",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nv98Qa6p4vai"
      },
      "id": "nv98Qa6p4vai",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8q_RTEI54vPF"
      },
      "id": "8q_RTEI54vPF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import mlflow.pytorch\n",
        "import dagshub\n",
        "import os\n",
        "from typing import Dict, Any, Optional\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import joblib\n",
        "import tempfile"
      ],
      "metadata": {
        "id": "TQAKxJOr2_LN"
      },
      "id": "TQAKxJOr2_LN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install darts\n",
        "!pip install mlflow\n",
        "!pip install dagshub"
      ],
      "metadata": {
        "id": "QipooYHi5Ln6",
        "outputId": "813fad99-29bd-4d92-a0c3-e6fbb9ee0599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true
      },
      "id": "QipooYHi5Ln6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting darts\n",
            "  Downloading darts-0.36.0-py3-none-any.whl.metadata (56 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.0/56.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: holidays>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from darts) (0.77)\n",
            "Requirement already satisfied: joblib>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from darts) (1.5.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from darts) (3.10.0)\n",
            "Requirement already satisfied: narwhals>=1.25.1 in /usr/local/lib/python3.11/dist-packages (from darts) (1.48.1)\n",
            "Collecting nfoursid>=1.0.0 (from darts)\n",
            "  Downloading nfoursid-1.0.2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from darts) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from darts) (2.2.2)\n",
            "Collecting pyod>=0.9.5 (from darts)\n",
            "  Downloading pyod-2.0.5-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from darts) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from darts) (1.6.1)\n",
            "Collecting scipy<1.16.0,>=1.3.2 (from darts)\n",
            "  Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: shap>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from darts) (0.48.0)\n",
            "Collecting statsforecast>=1.4 (from darts)\n",
            "  Downloading statsforecast-2.0.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Requirement already satisfied: statsmodels>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from darts) (0.14.5)\n",
            "Requirement already satisfied: tqdm>=4.60.0 in /usr/local/lib/python3.11/dist-packages (from darts) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from darts) (4.14.1)\n",
            "Requirement already satisfied: xarray>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from darts) (2025.7.1)\n",
            "Requirement already satisfied: xgboost>=2.1.4 in /usr/local/lib/python3.11/dist-packages (from darts) (3.0.2)\n",
            "Collecting pytorch-lightning>=1.5.0 (from darts)\n",
            "  Downloading pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting tensorboardX>=2.1 (from darts)\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from darts) (2.6.0+cu124)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from holidays>=0.11.1->darts) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->darts) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->darts) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->darts) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->darts) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->darts) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->darts) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->darts) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->darts) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->darts) (2025.2)\n",
            "Requirement already satisfied: numba>=0.51 in /usr/local/lib/python3.11/dist-packages (from pyod>=0.9.5->darts) (0.60.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning>=1.5.0->darts) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (2025.3.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning>=1.5.0->darts)\n",
            "  Downloading torchmetrics-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning>=1.5.0->darts)\n",
            "  Downloading lightning_utilities-0.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->darts) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->darts) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->darts) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->darts) (2025.7.14)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->darts) (3.6.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap>=0.40.0->darts) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap>=0.40.0->darts) (3.1.1)\n",
            "Collecting coreforecast>=0.0.12 (from statsforecast>=1.4->darts)\n",
            "  Downloading coreforecast-0.0.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting fugue>=0.8.1 (from statsforecast>=1.4->darts)\n",
            "  Downloading fugue-0.9.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting utilsforecast>=0.1.4 (from statsforecast>=1.4->darts)\n",
            "  Downloading utilsforecast-0.2.12-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.14.0->darts) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX>=2.1->darts) (5.29.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->darts) (1.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (3.12.14)\n",
            "Collecting triad>=0.9.7 (from fugue>=0.8.1->statsforecast>=1.4->darts)\n",
            "  Downloading triad-0.9.8-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting adagio>=0.2.4 (from fugue>=0.8.1->statsforecast>=1.4->darts)\n",
            "  Downloading adagio-0.2.6-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning>=1.5.0->darts) (75.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51->pyod>=0.9.5->darts) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->holidays>=0.11.1->darts) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->darts) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (1.20.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from triad>=0.9.7->fugue>=0.8.1->statsforecast>=1.4->darts) (18.1.0)\n",
            "Collecting fs (from triad>=0.9.7->fugue>=0.8.1->statsforecast>=1.4->darts)\n",
            "  Downloading fs-2.4.16-py2.py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: appdirs~=1.4.3 in /usr/local/lib/python3.11/dist-packages (from fs->triad>=0.9.7->fugue>=0.8.1->statsforecast>=1.4->darts) (1.4.4)\n",
            "Downloading darts-0.36.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nfoursid-1.0.2-py3-none-any.whl (18 kB)\n",
            "Downloading pyod-2.0.5-py3-none-any.whl (200 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.6/200.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.2-py3-none-any.whl (825 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading statsforecast-2.0.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (340 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.0/340.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coreforecast-0.0.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (285 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.8/285.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fugue-0.9.1-py3-none-any.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.0-py3-none-any.whl (29 kB)\n",
            "Downloading torchmetrics-1.8.0-py3-none-any.whl (981 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.9/981.9 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading utilsforecast-0.2.12-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading adagio-0.2.6-py3-none-any.whl (19 kB)\n",
            "Downloading triad-0.9.8-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fs-2.4.16-py2.py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX, scipy, lightning-utilities, fs, coreforecast, utilsforecast, triad, pyod, nfoursid, torchmetrics, adagio, pytorch-lightning, fugue, statsforecast, darts\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.0\n",
            "    Uninstalling scipy-1.16.0:\n",
            "      Successfully uninstalled scipy-1.16.0\n",
            "Successfully installed adagio-0.2.6 coreforecast-0.0.16 darts-0.36.0 fs-2.4.16 fugue-0.9.1 lightning-utilities-0.15.0 nfoursid-1.0.2 pyod-2.0.5 pytorch-lightning-2.5.2 scipy-1.15.3 statsforecast-2.0.2 tensorboardX-2.6.4 torchmetrics-1.8.0 triad-0.9.8 utilsforecast-0.2.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "scipy"
                ]
              },
              "id": "d5b3dffe97e64eee90deb1b202ee2848"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.11/dist-packages (3.1.4)\n",
            "Requirement already satisfied: mlflow-skinny==3.1.4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.4)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.16.4)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.11/dist-packages (from mlflow) (23.0.0)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.41)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (3.1.1)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.61.0)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.116.1)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (3.1.45)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (1.36.0)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (25.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (2.11.7)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (4.14.1)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.35.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.6)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==3.1.4->mlflow) (0.47.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.4->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.4->mlflow) (3.23.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.4->mlflow) (0.57b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.4->mlflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.4->mlflow) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.4->mlflow) (2025.7.14)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==3.1.4->mlflow) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.4->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.4->mlflow) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.4->mlflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (0.6.1)\n",
            "Requirement already satisfied: dagshub in /usr/local/lib/python3.11/dist-packages (0.6.2)\n",
            "Requirement already satisfied: PyYAML>=5 in /usr/local/lib/python3.11/dist-packages (from dagshub) (6.0.2)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.4.4)\n",
            "Requirement already satisfied: click>=8.0.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (8.2.1)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.28.1)\n",
            "Requirement already satisfied: GitPython>=3.1.29 in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.1.45)\n",
            "Requirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (13.9.4)\n",
            "Requirement already satisfied: dacite~=1.6.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.6.0)\n",
            "Requirement already satisfied: tenacity>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from dagshub) (8.5.0)\n",
            "Requirement already satisfied: gql[requests] in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.5.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.6.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.2.2)\n",
            "Requirement already satisfied: treelib>=1.6.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.8.0)\n",
            "Requirement already satisfied: pathvalidate>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.9.0.post0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.40.1)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.0.4)\n",
            "Requirement already satisfied: dagshub-annotation-converter>=0.1.5 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.1.11)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (5.4.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (11.3.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (4.14.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython>=3.1.29->dagshub) (4.0.12)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->dagshub) (0.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (2.19.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from treelib>=1.6.4->dagshub) (1.17.0)\n",
            "Requirement already satisfied: botocore<1.41.0,>=1.40.1 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.40.1)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (0.13.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (0.9.0)\n",
            "Requirement already satisfied: graphql-core<3.2.7,>=3.2 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (3.2.6)\n",
            "Requirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.20.1)\n",
            "Requirement already satisfied: backoff<3.0,>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (2.2.1)\n",
            "Requirement already satisfied: requests<3,>=2.26 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2025.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->dagshub) (1.3.1)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.41.0,>=1.40.1->boto3->dagshub) (2.5.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython>=3.1.29->dagshub) (5.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->dagshub) (0.1.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->dagshub) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.5->dagshub) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.5->dagshub) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.5->dagshub) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.26->gql[requests]->dagshub) (3.4.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub) (1.1.0)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (0.3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "stores = pd.read_csv('stores.csv')\n",
        "features = pd.read_csv('features.csv')"
      ],
      "metadata": {
        "id": "HRmeV3b02int"
      },
      "id": "HRmeV3b02int",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class BaseMerger(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, features_df, stores_df):\n",
        "        self.features_df = features_df.copy()\n",
        "        self.stores_df = stores_df.copy()\n",
        "        self.features_df[\"Date\"] = pd.to_datetime(self.features_df[\"Date\"])\n",
        "        self.stores_df[\"Store\"] = self.stores_df[\"Store\"].astype(int)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[\"Date\"] = pd.to_datetime(X[\"Date\"])\n",
        "        X[\"Store\"] = X[\"Store\"].astype(int)\n",
        "        merged = X.merge(self.features_df, on=[\"Store\", \"Date\"], how=\"left\")\n",
        "        merged = merged.merge(self.stores_df, on=\"Store\", how=\"left\")\n",
        "        return merged\n"
      ],
      "metadata": {
        "id": "nKqz-qYi2ilS"
      },
      "id": "nKqz-qYi2ilS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureAdder(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, is_train=True):\n",
        "        self.is_train = is_train\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[\"Date\"] = pd.to_datetime(X[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "        if self.is_train and \"Weekly_Sales\" in X.columns:\n",
        "            for lag in [1, 2, 4, 52]:\n",
        "                X[f\"lag_{lag}\"] = X.groupby(\"Store\")[\"Weekly_Sales\"].shift(lag)\n",
        "            X[\"rolling_mean_4\"] = X.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(4).mean()\n",
        "            X[\"rolling_std_4\"] = X.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(4).std()\n",
        "\n",
        "        markdown_cols = [col for col in X.columns if \"MarkDown\" in col]\n",
        "        X[markdown_cols] = X[markdown_cols].fillna(0)\n",
        "\n",
        "        X[\"Year\"] = X[\"Date\"].dt.year\n",
        "        X[\"Month\"] = X[\"Date\"].dt.month\n",
        "        X[\"Week\"] = X[\"Date\"].dt.isocalendar().week\n",
        "        X[\"DayOfWeek\"] = X[\"Date\"].dt.dayofweek\n",
        "        X[\"IsMonthStart\"] = X[\"Date\"].dt.is_month_start.astype(int)\n",
        "        X[\"IsMonthEnd\"] = X[\"Date\"].dt.is_month_end.astype(int)\n",
        "        X[\"Quarter\"] = X[\"Date\"].dt.quarter\n",
        "\n",
        "        # ⚠️ Don't drop Date here — it's needed for aggregations!\n",
        "        return X"
      ],
      "metadata": {
        "id": "nnbeG9Xn2ijE"
      },
      "id": "nnbeG9Xn2ijE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MissingValueFiller(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, is_train=True):\n",
        "        self.q_low = None\n",
        "        self.q_high = None\n",
        "        self.is_train = is_train\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if self.is_train and \"Weekly_Sales\" in X.columns:\n",
        "            self.q_low = X[\"Weekly_Sales\"].quantile(0.01)\n",
        "            self.q_high = X[\"Weekly_Sales\"].quantile(0.99)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[\"Date\"] = pd.to_datetime(X[\"Date\"])\n",
        "\n",
        "        if self.is_train and \"Weekly_Sales\" in X.columns:\n",
        "            X = X[X[\"Weekly_Sales\"] > 0]\n",
        "            if self.q_low is not None and self.q_high is not None:\n",
        "                X = X[(X[\"Weekly_Sales\"] >= self.q_low) & (X[\"Weekly_Sales\"] <= self.q_high)]\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "L9XngACe2igy"
      },
      "id": "L9XngACe2igy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.fill_values = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "        for col in numeric_cols:\n",
        "            if X[col].isnull().any():\n",
        "                self.fill_values[col] = X[col].mean()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        for col, fill_value in self.fill_values.items():\n",
        "            if col in X.columns:\n",
        "                X[col] = X[col].fillna(fill_value)\n",
        "\n",
        "        if \"Type\" in X.columns:\n",
        "            X[\"Type\"] = X[\"Type\"].map({\"A\": 3, \"B\": 2, \"C\": 1}).fillna(0)\n",
        "\n",
        "        if \"IsHoliday\" in X.columns:\n",
        "            X[\"IsHoliday\"] = X[\"IsHoliday\"].astype(int)\n",
        "\n",
        "        X = X.dropna()  # Now okay, since lags are filled\n",
        "        return X"
      ],
      "metadata": {
        "id": "BBAlb1Uv2ieN"
      },
      "id": "BBAlb1Uv2ieN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class StoreAggregator(BaseEstimator, TransformerMixin):\n",
        "#     # def __init__(self):\n",
        "#     #     self.timeseries = {}\n",
        "\n",
        "#     # def fit(self, X, y=None):\n",
        "#     #     return self\n",
        "\n",
        "#     # def transform(self, X):\n",
        "#     #     self.timeseries = {}\n",
        "#     #     for store in X['Store'].unique():\n",
        "#     #         self.aggregate_store_info(store, X)\n",
        "#     #     return self.timeseries\n",
        "\n",
        "#     # def aggregate_store_info(self, store_id, X):\n",
        "#     #     store_data = X[X['Store'] == store_id].copy()\n",
        "\n",
        "#     #     # Check if Weekly_Sales exists (train data) or not (test data)\n",
        "#     #     has_weekly_sales = 'Weekly_Sales' in store_data.columns\n",
        "\n",
        "#     #     if has_weekly_sales:\n",
        "#     #         sum_columns = ['Weekly_Sales', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "#     #     else:\n",
        "#     #         sum_columns = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "\n",
        "#     #     first_columns = ['IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "#     #                     'Type', 'Size', 'Day', 'Month', 'Year', 'SuperbowlWeek',\n",
        "#     #                     'LaborDayWeek', 'ThanksgivingWeek', 'ChristmasWeek',\n",
        "#     #                     'Days_to_Thanksgiving', 'Days_to_Christmas']\n",
        "\n",
        "#     #     agg_dict = {}\n",
        "\n",
        "#     #     # Add sum columns that exist in the data\n",
        "#     #     for col in sum_columns:\n",
        "#     #         if col in store_data.columns:\n",
        "#     #             agg_dict[col] = 'sum'\n",
        "\n",
        "#     #     # Add first columns that exist in the data\n",
        "#     #     for col in first_columns:\n",
        "#     #       if col in store_data.columns:\n",
        "#     #             agg_dict[col] = 'first'\n",
        "\n",
        "#     #     aggregated = store_data.groupby(['Date', 'Store']).agg(agg_dict).reset_index()\n",
        "#     #     aggregated = aggregated.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "#     #     # Calculate department proportions only if Weekly_Sales exists\n",
        "#     #     if has_weekly_sales:\n",
        "#     #         dept_proportions = self.calculate_dept_proportions(store_data)\n",
        "#     #     else:\n",
        "#     #         dept_proportions = None\n",
        "\n",
        "#     #     self.timeseries[store_id] = (aggregated, dept_proportions)\n",
        "#     #     return aggregated\n",
        "\n",
        "#     # def calculate_dept_proportions(self, store_data):\n",
        "#     #     dept_totals = store_data.groupby('Dept')['Weekly_Sales'].sum()\n",
        "#     #     store_total = store_data['Weekly_Sales'].sum()\n",
        "\n",
        "#     #     if store_total == 0:\n",
        "#     #         num_depts = len(dept_totals)\n",
        "#     #         return {dept: 1.0/num_depts for dept in dept_totals.index}\n",
        "\n",
        "#     #     dept_proportions = (dept_totals / store_total).to_dict()\n",
        "#     #     return dept_proportions\n",
        "\n",
        "#     def __init__(self):\n",
        "#         self.numeric_cols = []\n",
        "#         self.categorical_cols = []\n",
        "\n",
        "#     def fit(self, X, y=None):\n",
        "#         self.numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "#         self.categorical_cols = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         # This transformer just passes through the data but stores column info\n",
        "#         return X\n",
        "\n",
        "#     def get_feature_names_out(self):\n",
        "#         return self.numeric_cols, self.categorical_cols"
      ],
      "metadata": {
        "id": "KErRY9Bv3d5e"
      },
      "id": "KErRY9Bv3d5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('merge', BaseMerger(features, stores)),\n",
        "    ('value_fill', MissingValueFiller()),\n",
        "    ('feature_add', FeatureAdder()),\n",
        "    ('cat_encoder', CategoricalEncoder())\n",
        "    # ('store_aggregator', StoreAggregator()),\n",
        "])"
      ],
      "metadata": {
        "id": "ZtfMQdnc3rqL"
      },
      "id": "ZtfMQdnc3rqL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dict = pipeline.fit_transform(train)\n",
        "\n",
        "\n",
        "# grouped = processed_df.groupby(\"Store\")\n",
        "# train_dict = {store: df for store, df in grouped}\n",
        "\n",
        "processed_df = pipeline.fit_transform(train)  # Save the transformed data to processed_df\n",
        "\n",
        "grouped = processed_df.groupby(\"Store\")\n",
        "train_dict = {store: df for store, df in grouped}\n"
      ],
      "metadata": {
        "id": "eDOUr54Z3ukz"
      },
      "id": "eDOUr54Z3ukz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "Gd5DdjUM6RgG"
      },
      "id": "Gd5DdjUM6RgG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from darts import TimeSeries\n",
        "from darts.models.forecasting.arima import ARIMA\n",
        "\n",
        "# # def train_store_models(p, d, q, train_weeks):\n",
        "# #     mape_array = []\n",
        "# #     store_models = {}\n",
        "\n",
        "# #     for store in train_dict.keys():\n",
        "# #         #extract store data\n",
        "# #         # df, prop = train_dict[store]\n",
        "# #         df = train_dict[store]\n",
        "\n",
        "# #         if not isinstance(df, pd.DataFrame):\n",
        "# #             df = pd.DataFrame(df)  # force conversion from Modin/Polars to Pandas\n",
        "\n",
        "# #         df['Date'] = pd.to_datetime(df['Date'])\n",
        "# #         df['Date'] = df['Date'] - pd.to_timedelta(df['Date'].dt.weekday, unit='d')  # aligns to Monday\n",
        "\n",
        "\n",
        "# #         df = df.groupby('Date', as_index=False)['Weekly_Sales'].sum()\n",
        "# #         df = df.sort_values(\"Date\")\n",
        "\n",
        "# #         # Create a continuous weekly index\n",
        "# #         full_index = pd.date_range(start=df['Date'].min(), end=df['Date'].max(), freq='W-MON')\n",
        "# #         df = df.set_index('Date').reindex(full_index).fillna(0).rename_axis(\"Date\").reset_index()\n",
        "\n",
        "\n",
        "# #         #Make timeseries object\n",
        "# #         full_ts = TimeSeries.from_dataframe(\n",
        "# #             df,\n",
        "# #             time_col='Date',\n",
        "# #             value_cols=['Weekly_Sales'],\n",
        "# #             fill_missing_dates=True,\n",
        "# #             freq='W'  # or 'D' for daily, 'M' for monthly, adjust to your data!\n",
        "# #         )\n",
        "\n",
        "# #         train_ts = full_ts[:train_weeks]\n",
        "# #         val_ts = full_ts[train_weeks:]\n",
        "\n",
        "# #         #Train and validate model\n",
        "# #         model = ARIMA(p=p,d=d,q=q)\n",
        "# #         model.fit(train_ts)\n",
        "# #         predictions = model.predict(len(val_ts))\n",
        "\n",
        "# #         # Calculate MAPE\n",
        "# #         actual_values = val_ts.values().flatten()\n",
        "# #         pred_values = predictions.values().flatten()\n",
        "# #         mask = actual_values != 0\n",
        "# #         mape = np.mean(np.abs((actual_values[mask] - pred_values[mask]) / actual_values[mask])) * 100\n",
        "# #         mape_array.append(mape)\n",
        "\n",
        "# #         #Save store model\n",
        "# #         store_models[store] = model\n",
        "\n",
        "# #     return (mape_array, store_models)\n",
        "\n",
        "# from darts import TimeSeries\n",
        "# from darts.models.forecasting.arima import ARIMA\n",
        "\n",
        "# def train_store_models(p, d, q, train_weeks):\n",
        "#     mape_array = []\n",
        "#     store_models = {}\n",
        "\n",
        "#     for store in train_dict.keys():\n",
        "#         df = train_dict[store]\n",
        "\n",
        "#         if not isinstance(df, pd.DataFrame):\n",
        "#             df = pd.DataFrame(df)\n",
        "\n",
        "#         # Ensure datetime type\n",
        "#         df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "#         # Align dates to start of week (Monday)\n",
        "#         df['Date'] = df['Date'] - pd.to_timedelta(df['Date'].dt.weekday, unit='D')\n",
        "\n",
        "#         # Aggregate sales per week\n",
        "#         df = df.groupby('Date', as_index=False)['Weekly_Sales'].sum()\n",
        "#         df = df.sort_values(\"Date\")\n",
        "\n",
        "#         # Reindex to weekly (aligned to Monday!)\n",
        "#         full_index = pd.date_range(start=df['Date'].min(), end=df['Date'].max(), freq='W-MON')\n",
        "#         df = df.set_index('Date').reindex(full_index).fillna(0).rename_axis(\"Date\").reset_index()\n",
        "\n",
        "#         # 🧠 Important: use freq='W-MON' to match what you actually created\n",
        "#         full_ts = TimeSeries.from_dataframe(\n",
        "#             df,\n",
        "#             time_col='Date',\n",
        "#             value_cols=['Weekly_Sales'],\n",
        "#             fill_missing_dates=False,  # already filled!\n",
        "#             freq='W-MON'  # must match your date range\n",
        "#         )\n",
        "\n",
        "#         # Train/Val split\n",
        "#         train_ts = full_ts[:train_weeks]\n",
        "#         val_ts = full_ts[train_weeks:]\n",
        "\n",
        "#         # Train model\n",
        "#         model = ARIMA(p=p, d=d, q=q)\n",
        "#         model.fit(train_ts)\n",
        "#         predictions = model.predict(len(val_ts))\n",
        "\n",
        "#         # MAPE\n",
        "#         actual_values = val_ts.values().flatten()\n",
        "#         pred_values = predictions.values().flatten()\n",
        "#         mask = actual_values != 0\n",
        "#         mape = np.mean(np.abs((actual_values[mask] - pred_values[mask]) / actual_values[mask])) * 100\n",
        "#         mape_array.append(mape)\n",
        "\n",
        "#         # Store the model\n",
        "#         store_models[store] = model\n",
        "\n",
        "#     return (mape_array, store_models)\n",
        "\n",
        "\n",
        "import warnings\n",
        "from numpy.linalg import LinAlgError\n",
        "\n",
        "def train_store_models(p, d, q, train_weeks):\n",
        "    mape_array = []\n",
        "    store_models = {}\n",
        "\n",
        "    for store in train_dict.keys():\n",
        "        df = train_dict[store]\n",
        "\n",
        "        if not isinstance(df, pd.DataFrame):\n",
        "            df = pd.DataFrame(df)\n",
        "\n",
        "        try:\n",
        "            # Prep time series\n",
        "            df['Date'] = pd.to_datetime(df['Date'])\n",
        "            df['Date'] = df['Date'] - pd.to_timedelta(df['Date'].dt.weekday, unit='D')\n",
        "            df = df.groupby('Date', as_index=False)['Weekly_Sales'].sum()\n",
        "            df = df.sort_values(\"Date\")\n",
        "\n",
        "            full_index = pd.date_range(start=df['Date'].min(), end=df['Date'].max(), freq='W-MON')\n",
        "            df = df.set_index('Date').reindex(full_index).fillna(0).rename_axis(\"Date\").reset_index()\n",
        "\n",
        "            full_ts = TimeSeries.from_dataframe(\n",
        "                df,\n",
        "                time_col='Date',\n",
        "                value_cols=['Weekly_Sales'],\n",
        "                fill_missing_dates=False,\n",
        "                freq='W-MON'\n",
        "            )\n",
        "\n",
        "            train_ts = full_ts[:train_weeks]\n",
        "            val_ts = full_ts[train_weeks:]\n",
        "\n",
        "            # Train model\n",
        "            model = ARIMA(p=p, d=d, q=q)\n",
        "            model.fit(train_ts)\n",
        "            predictions = model.predict(len(val_ts))\n",
        "\n",
        "            # Evaluate\n",
        "            actual_values = val_ts.values().flatten()\n",
        "            pred_values = predictions.values().flatten()\n",
        "            mask = actual_values != 0\n",
        "            mape = np.mean(np.abs((actual_values[mask] - pred_values[mask]) / actual_values[mask])) * 100\n",
        "            mape_array.append(mape)\n",
        "\n",
        "            store_models[store] = model\n",
        "\n",
        "        except (ValueError, LinAlgError, RuntimeError) as e:\n",
        "            warnings.warn(f\"Store {store} failed for ARIMA({p},{d},{q}): {e}\")\n",
        "            continue\n",
        "\n",
        "    return mape_array, store_models\n"
      ],
      "metadata": {
        "id": "AAZ1gCnH37Be"
      },
      "id": "AAZ1gCnH37Be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dagshub\n",
        "# import mlflow\n",
        "# import pickle\n",
        "# import numpy as np\n",
        "# from datetime import datetime\n",
        "\n",
        "# # Set experiment name\n",
        "# mlflow.set_experiment(\"ARIMA_Parameter_Optimization\")\n",
        "\n",
        "# # Start MLflow run for the entire parameter search\n",
        "# with mlflow.start_run(run_name=f\"ARIMA_Grid_Search_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n",
        "\n",
        "#     # Log experiment configuration\n",
        "#     mlflow.log_param(\"model_type\", \"ARIMA\")\n",
        "#     mlflow.log_param(\"validation_weeks\", 123)\n",
        "#     # mlflow.log_param(\"total_param_combinations\", len(param_list))\n",
        "#     # mlflow.log_param(\"param_list\", str(param_list))\n",
        "\n",
        "#     from tqdm import tqdm\n",
        "#     param_list = [\n",
        "#         [1,1,0],\n",
        "#         [0,1,1],\n",
        "#         [1,1,1],\n",
        "#         [2,1,1],\n",
        "#         [1,1,2],\n",
        "#         [2,1,2],\n",
        "#         [0,1,2],\n",
        "#         [3,1,1],\n",
        "#         [1,1,3],\n",
        "#         [4,1,1]\n",
        "#     ]\n",
        "\n",
        "#     max_mape = 100\n",
        "#     best_models = {}\n",
        "#     best_params = []\n",
        "\n",
        "#     for p in tqdm(param_list):\n",
        "#         # Create nested run for each parameter combination\n",
        "#         with mlflow.start_run(run_name=f\"ARIMA({p[0]},{p[1]},{p[2]})\", nested=True):\n",
        "\n",
        "#             # Log individual parameters\n",
        "#             mlflow.log_param(\"p\", p[0])\n",
        "#             mlflow.log_param(\"d\", p[1])\n",
        "#             mlflow.log_param(\"q\", p[2])\n",
        "\n",
        "#             # Train models\n",
        "#             mape_array, store_models = train_store_models(p[0], p[1], p[2], 123)\n",
        "#             mean_mape = sum(mape_array)/len(mape_array)\n",
        "\n",
        "#             # Log metrics\n",
        "#             mlflow.log_metric(\"mean_mape\", mean_mape)\n",
        "#             mlflow.log_metric(\"mape_std\", np.std(mape_array))\n",
        "#             mlflow.log_metric(\"num_successful_stores\", len(store_models))\n",
        "\n",
        "#             print(f'Storewise mean mape: {mean_mape} for params{p[0], p[1], p[2]}')\n",
        "\n",
        "#             # Check if this is the best model\n",
        "#             if mean_mape < max_mape:\n",
        "#                 best_models = store_models\n",
        "#                 max_mape = mean_mape\n",
        "#                 best_params = [p[0], p[1], p[2]]\n",
        "\n",
        "#                 # Log as best model so far\n",
        "#                 mlflow.log_metric(\"is_best_model\", 1)\n",
        "#             else:\n",
        "#                 mlflow.log_metric(\"is_best_model\", 0)\n",
        "\n",
        "#     # Log best model results\n",
        "#     mlflow.log_param(\"best_p\", best_params[0])\n",
        "#     mlflow.log_param(\"best_d\", best_params[1])\n",
        "#     mlflow.log_param(\"best_q\", best_params[2])\n",
        "#     mlflow.log_metric(\"best_mean_mape\", max_mape)\n",
        "\n",
        "#     # Save final best model\n",
        "#     with open(\"final_best_arima_models.pkl\", \"wb\") as f:\n",
        "#         pickle.dump(best_models, f)\n",
        "#     mlflow.log_artifact(\"final_best_arima_models.pkl\")\n",
        "\n",
        "import dagshub\n",
        "import mlflow\n",
        "import pickle\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define your own function: train_store_models(p, d, q, validation_weeks)\n",
        "# Make sure it returns:\n",
        "# - mape_array: list of MAPE values for each store\n",
        "# - store_models: dict of trained models, e.g., {(store, dept): model}\n",
        "\n",
        "# Set MLflow experiment\n",
        "mlflow.set_experiment(\"ARIMA_Parameter_Optimization\")\n",
        "\n",
        "# Parameter combinations to try\n",
        "param_list = [\n",
        "    [1, 1, 0],\n",
        "    [0, 1, 1],\n",
        "    [1, 1, 1],\n",
        "    [2, 1, 1],\n",
        "    [1, 1, 2],\n",
        "    [2, 1, 2],\n",
        "    [0, 1, 2],\n",
        "    [3, 1, 1],\n",
        "    [1, 1, 3],\n",
        "    [4, 1, 1]\n",
        "]\n",
        "\n",
        "# Initialize best model tracking\n",
        "best_models = {}\n",
        "best_params = None\n",
        "lowest_mape = float('inf')  # Lower is better\n",
        "\n",
        "# Start top-level MLflow run\n",
        "with mlflow.start_run(run_name=f\"ARIMA_Grid_Search_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n",
        "\n",
        "    # Log basic metadata\n",
        "    mlflow.log_param(\"model_type\", \"ARIMA\")\n",
        "    mlflow.log_param(\"validation_weeks\", 123)\n",
        "    mlflow.log_param(\"param_search_space\", str(param_list))\n",
        "    mlflow.log_param(\"num_combinations\", len(param_list))\n",
        "\n",
        "    for p in tqdm(param_list):\n",
        "        p_val, d_val, q_val = p\n",
        "\n",
        "        # Nested MLflow run for each param combination\n",
        "        with mlflow.start_run(run_name=f\"ARIMA({p_val},{d_val},{q_val})\", nested=True):\n",
        "            mlflow.log_param(\"p\", p_val)\n",
        "            mlflow.log_param(\"d\", d_val)\n",
        "            mlflow.log_param(\"q\", q_val)\n",
        "\n",
        "            # Train and evaluate\n",
        "            mape_array, store_models = train_store_models(p_val, d_val, q_val, 123)\n",
        "            mean_mape = np.mean(mape_array)\n",
        "            mape_std = np.std(mape_array)\n",
        "\n",
        "            # Log metrics\n",
        "            mlflow.log_metric(\"mean_mape\", mean_mape)\n",
        "            mlflow.log_metric(\"mape_std\", mape_std)\n",
        "            mlflow.log_metric(\"successful_stores\", len(store_models))\n",
        "\n",
        "            print(f\"Params: {p}, Mean MAPE: {mean_mape:.4f}, Std: {mape_std:.4f}\")\n",
        "\n",
        "            # Track best model\n",
        "            if mean_mape < lowest_mape:\n",
        "                best_models = store_models\n",
        "                best_params = p\n",
        "                lowest_mape = mean_mape\n",
        "                mlflow.log_metric(\"is_best_model\", 1)\n",
        "            else:\n",
        "                mlflow.log_metric(\"is_best_model\", 0)\n",
        "\n",
        "    # Log final best config\n",
        "    if best_params:\n",
        "        mlflow.log_param(\"best_p\", best_params[0])\n",
        "        mlflow.log_param(\"best_d\", best_params[1])\n",
        "        mlflow.log_param(\"best_q\", best_params[2])\n",
        "        mlflow.log_metric(\"best_mean_mape\", lowest_mape)\n",
        "\n",
        "        # Save best model dict\n",
        "        with open(\"final_best_arima_models.pkl\", \"wb\") as f:\n",
        "            pickle.dump(best_models, f)\n",
        "        mlflow.log_artifact(\"final_best_arima_models.pkl\")\n"
      ],
      "metadata": {
        "id": "6tqVDGAX3695",
        "outputId": "cf1be4a8-edd8-4494-8a1d-c54e022b200e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6tqVDGAX3695",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params: [1, 1, 0], Mean MAPE: 7.2830, Std: 3.5574\n",
            "🏃 View run ARIMA(1,1,0) at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8/runs/f9174fd3251847e2937b8cd381f91d75\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 1/10 [00:12<01:51, 12.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params: [0, 1, 1], Mean MAPE: 6.9985, Std: 3.3769\n",
            "🏃 View run ARIMA(0,1,1) at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8/runs/14f4daeb809b4791a6e98d78e09b2cf9\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:30<02:05, 15.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params: [1, 1, 1], Mean MAPE: 6.4478, Std: 3.1096\n",
            "🏃 View run ARIMA(1,1,1) at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8/runs/274c2b4dde7e409d825db19cf77b6014\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:48<01:57, 16.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params: [2, 1, 1], Mean MAPE: 6.2080, Std: 3.1039\n",
            "🏃 View run ARIMA(2,1,1) at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8/runs/ceb053960d1248a28eae1d45a4bf2b0c\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [01:06<01:44, 17.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params: [1, 1, 2], Mean MAPE: 6.6082, Std: 3.2221\n",
            "🏃 View run ARIMA(1,1,2) at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8/runs/d575c53961be4f1eadff13395205551a\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [01:24<01:28, 17.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params: [2, 1, 2], Mean MAPE: 5.6746, Std: 3.0645\n",
            "🏃 View run ARIMA(2,1,2) at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8/runs/d12998cc527642da9eeda25a97e51599\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [01:43<01:11, 17.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params: [0, 1, 2], Mean MAPE: 6.7549, Std: 3.2736\n",
            "🏃 View run ARIMA(0,1,2) at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8/runs/b84f6f1c8ce04553a81142f63af47295\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [02:01<00:53, 17.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params: [3, 1, 1], Mean MAPE: 6.5358, Std: 3.0509\n",
            "🏃 View run ARIMA(3,1,1) at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8/runs/eb5373f8edde473789d59741fb16a479\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [02:19<00:36, 18.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params: [1, 1, 3], Mean MAPE: 6.2813, Std: 3.0096\n",
            "🏃 View run ARIMA(1,1,3) at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8/runs/c7214951a57841ea8f6dcdd15602f5db\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [02:37<00:18, 18.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Params: [4, 1, 1], Mean MAPE: 6.7052, Std: 3.1142\n",
            "🏃 View run ARIMA(4,1,1) at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8/runs/4865cb18a4924105967e51edf775cdfd\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [02:57<00:00, 17.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run ARIMA_Grid_Search_20250802_164405 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8/runs/0ba385fe59404c71a756a26800583fc3\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predictions = {}\n",
        "# train_end_date = pd.to_datetime(train['Date'].max())\n",
        "# for index, entry in tqdm(test.iterrows()):\n",
        "#     store = entry['Store']\n",
        "#     dept = entry['Dept']\n",
        "#     date = pd.to_datetime(entry['Date'])  # Convert to datetime\n",
        "#     pred_weeks = (date - train_end_date).days // 7\n",
        "\n",
        "#     model = best_models[store]\n",
        "#     prediction = model.predict(pred_weeks).values()[-1, 0]\n",
        "\n",
        "#     predictions[(store, dept, date)] = prediction\n",
        "\n",
        "#     if dept in sale_proportions.keys():\n",
        "#         prediction = prediction*sale_proportions[dept]\n",
        "#     else:\n",
        "#         prediction = prediction/len(sale_proportions)\n",
        "\n",
        "#     predictions[(store, dept, date)] = prediction\n",
        "\n",
        "predictions = {}\n",
        "train_end_date = pd.to_datetime(train['Date'].max())\n",
        "\n",
        "# (Optional) Compute proportions if needed\n",
        "sale_proportions = (\n",
        "    train.groupby(['Store', 'Dept'])['Weekly_Sales'].sum() /\n",
        "    train.groupby(['Store'])['Weekly_Sales'].sum()\n",
        ").to_dict()\n",
        "\n",
        "for index, entry in tqdm(test.iterrows(), total=len(test)):\n",
        "    store = entry['Store']\n",
        "    dept = entry['Dept']\n",
        "    date = pd.to_datetime(entry['Date'])\n",
        "    pred_weeks = (date - train_end_date).days // 7\n",
        "\n",
        "    try:\n",
        "        model = best_models[store]\n",
        "        prediction = model.predict(pred_weeks).values()[-1, 0]\n",
        "\n",
        "        # Apply department-level proportion\n",
        "        key = (store, dept)\n",
        "        if key in sale_proportions:\n",
        "            prediction *= sale_proportions[key]\n",
        "        else:\n",
        "            prediction /= len(sale_proportions)  # or set prediction = 0\n",
        "\n",
        "        predictions[(store, dept, date)] = prediction\n",
        "\n",
        "    except KeyError:\n",
        "        print(f\"No model found for store {store}, skipping.\")\n"
      ],
      "metadata": {
        "id": "B3r6jQMW364i",
        "outputId": "65b4de6b-1cfd-4f7d-c173-bb8708f57bd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "B3r6jQMW364i",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 115064/115064 [06:17<00:00, 304.62it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df = pd.DataFrame([\n",
        "    {\n",
        "        'Id': f\"{store}_{dept}_{date.strftime('%Y-%m-%d')}\",\n",
        "        'Weekly_Sales': weekly_sales\n",
        "    }\n",
        "    for (store, dept, date), weekly_sales in predictions.items()\n",
        "])\n",
        "\n",
        "submission_df.to_csv('submission_arima.csv', index=False)"
      ],
      "metadata": {
        "id": "6ak04TFS361z"
      },
      "id": "6ak04TFS361z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('submission_arima.csv')\n"
      ],
      "metadata": {
        "id": "7d0KDEwU36xt",
        "outputId": "065fa4bf-bb24-4e20-eca3-04d47371ba09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "id": "7d0KDEwU36xt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f4acb77f-b76f-4ab0-8e37-741776dd3171\", \"submission_arima.csv\", 4030576)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W1Yrc6cs_qMi"
      },
      "id": "W1Yrc6cs_qMi",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}