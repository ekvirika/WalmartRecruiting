{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e95788",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b6380b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn mlflow\n",
    "\n",
    "# Set up Kaggle API\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa958d7c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Upload your kaggle.json to Colab and run:\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fd3e2d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
    "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668fa338",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!unzip -q train.csv.zip\n",
    "!unzip -q stores.csv.zip\n",
    "!unzip -q test.csv.zip\n",
    "!unzip -q features.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c283f75b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Time series specific libraries\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Other utilities\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import itertools\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import joblib\n",
    "\n",
    "class WalmartDataPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom preprocessor for Walmart sales data.\n",
    "    Handles missing values, creates time-based features, and prepares data for ARIMA modeling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 date_column: str = 'Date',\n",
    "                 sales_column: str = 'Weekly_Sales',\n",
    "                 store_column: str = 'Store',\n",
    "                 dept_column: str = 'Dept',\n",
    "                 fill_method: str = 'forward'):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessor.\n",
    "        \n",
    "        Args:\n",
    "            date_column: Name of the date column\n",
    "            sales_column: Name of the sales column to forecast\n",
    "            store_column: Name of the store identifier column\n",
    "            dept_column: Name of the department identifier column\n",
    "            fill_method: Method for handling missing values ('forward', 'backward', 'mean')\n",
    "        \"\"\"\n",
    "        self.date_column = date_column\n",
    "        self.sales_column = sales_column\n",
    "        self.store_column = store_column\n",
    "        self.dept_column = dept_column\n",
    "        self.fill_method = fill_method\n",
    "        self.scalers = {}\n",
    "        self.holiday_dates = []\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the preprocessor to the training data.\"\"\"\n",
    "        # Store unique stores and departments\n",
    "        self.stores = X[self.store_column].unique()\n",
    "        self.departments = X[self.dept_column].unique()\n",
    "        \n",
    "        # Identify holiday dates (when IsHoliday is True)\n",
    "        if 'IsHoliday' in X.columns:\n",
    "            self.holiday_dates = X[X['IsHoliday'] == True][self.date_column].unique()\n",
    "        \n",
    "        # Fit scalers for each store-department combination\n",
    "        for store in self.stores:\n",
    "            for dept in self.departments:\n",
    "                key = f\"{store}_{dept}\"\n",
    "                mask = (X[self.store_column] == store) & (X[self.dept_column] == dept)\n",
    "                store_dept_data = X[mask]\n",
    "                \n",
    "                if len(store_dept_data) > 0:\n",
    "                    scaler = StandardScaler()\n",
    "                    sales_data = store_dept_data[self.sales_column].values.reshape(-1, 1)\n",
    "                    # Handle missing values before fitting scaler\n",
    "                    sales_data_clean = sales_data[~np.isnan(sales_data.flatten())]\n",
    "                    if len(sales_data_clean) > 0:\n",
    "                        scaler.fit(sales_data_clean.reshape(-1, 1))\n",
    "                        self.scalers[key] = scaler\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform the input data.\"\"\"\n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        # Convert date column to datetime\n",
    "        X_transformed[self.date_column] = pd.to_datetime(X_transformed[self.date_column])\n",
    "        \n",
    "        # Sort by date\n",
    "        X_transformed = X_transformed.sort_values([self.store_column, self.dept_column, self.date_column])\n",
    "        \n",
    "        # Create time-based features\n",
    "        X_transformed['Year'] = X_transformed[self.date_column].dt.year\n",
    "        X_transformed['Month'] = X_transformed[self.date_column].dt.month\n",
    "        X_transformed['Week'] = X_transformed[self.date_column].dt.isocalendar().week\n",
    "        X_transformed['DayOfYear'] = X_transformed[self.date_column].dt.dayofyear\n",
    "        X_transformed['Quarter'] = X_transformed[self.date_column].dt.quarter\n",
    "        \n",
    "        # Create cyclical features for seasonality\n",
    "        X_transformed['Month_sin'] = np.sin(2 * np.pi * X_transformed['Month'] / 12)\n",
    "        X_transformed['Month_cos'] = np.cos(2 * np.pi * X_transformed['Month'] / 12)\n",
    "        X_transformed['Week_sin'] = np.sin(2 * np.pi * X_transformed['Week'] / 52)\n",
    "        X_transformed['Week_cos'] = np.cos(2 * np.pi * X_transformed['Week'] / 52)\n",
    "        \n",
    "        # Handle missing values in sales data\n",
    "        X_transformed = self._handle_missing_values(X_transformed)\n",
    "        \n",
    "        return X_transformed\n",
    "    \n",
    "    def _handle_missing_values(self, df):\n",
    "        \"\"\"Handle missing values in the sales data.\"\"\"\n",
    "        df_filled = df.copy()\n",
    "        \n",
    "        for store in self.stores:\n",
    "            for dept in self.departments:\n",
    "                mask = (df_filled[self.store_column] == store) & (df_filled[self.dept_column] == dept)\n",
    "                store_dept_data = df_filled[mask]\n",
    "                \n",
    "                if len(store_dept_data) > 0:\n",
    "                    if self.fill_method == 'forward':\n",
    "                        df_filled.loc[mask, self.sales_column] = store_dept_data[self.sales_column].fillna(method='ffill')\n",
    "                    elif self.fill_method == 'backward':\n",
    "                        df_filled.loc[mask, self.sales_column] = store_dept_data[self.sales_column].fillna(method='bfill')\n",
    "                    elif self.fill_method == 'mean':\n",
    "                        mean_sales = store_dept_data[self.sales_column].mean()\n",
    "                        df_filled.loc[mask, self.sales_column] = store_dept_data[self.sales_column].fillna(mean_sales)\n",
    "        \n",
    "        return df_filled\n",
    "\n",
    "class ARIMATimeSeriesModel(BaseEstimator):\n",
    "    \"\"\"\n",
    "    ARIMA model wrapper for time series forecasting.\n",
    "    Handles automatic order selection and fitting for multiple time series.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 order: Optional[Tuple[int, int, int]] = None,\n",
    "                 seasonal_order: Optional[Tuple[int, int, int, int]] = None,\n",
    "                 auto_arima: bool = True,\n",
    "                 max_p: int = 5,\n",
    "                 max_d: int = 2,\n",
    "                 max_q: int = 5,\n",
    "                 seasonal: bool = True,\n",
    "                 m: int = 52):  # 52 weeks in a year\n",
    "        \"\"\"\n",
    "        Initialize ARIMA model.\n",
    "        \n",
    "        Args:\n",
    "            order: ARIMA order (p, d, q) if not using auto_arima\n",
    "            seasonal_order: Seasonal ARIMA order (P, D, Q, s) if not using auto_arima\n",
    "            auto_arima: Whether to automatically select best ARIMA parameters\n",
    "            max_p, max_d, max_q: Maximum values for auto ARIMA parameter search\n",
    "            seasonal: Whether to include seasonal components\n",
    "            m: Seasonal period (52 for weekly data)\n",
    "        \"\"\"\n",
    "        self.order = order\n",
    "        self.seasonal_order = seasonal_order\n",
    "        self.auto_arima = auto_arima\n",
    "        self.max_p = max_p\n",
    "        self.max_d = max_d\n",
    "        self.max_q = max_q\n",
    "        self.seasonal = seasonal\n",
    "        self.m = m\n",
    "        self.models = {}\n",
    "        self.best_orders = {}\n",
    "        \n",
    "    def _check_stationarity(self, series: pd.Series) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Check if a time series is stationary using ADF and KPSS tests.\n",
    "        \n",
    "        Args:\n",
    "            series: Time series data\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with test results\n",
    "        \"\"\"\n",
    "        # Remove NaN values\n",
    "        series_clean = series.dropna()\n",
    "        \n",
    "        # Augmented Dickey-Fuller test\n",
    "        adf_result = adfuller(series_clean)\n",
    "        adf_statistic = adf_result[0]\n",
    "        adf_p_value = adf_result[1]\n",
    "        \n",
    "        # KPSS test\n",
    "        kpss_result = kpss(series_clean)\n",
    "        kpss_statistic = kpss_result[0]\n",
    "        kpss_p_value = kpss_result[1]\n",
    "        \n",
    "        return {\n",
    "            'adf_statistic': adf_statistic,\n",
    "            'adf_p_value': adf_p_value,\n",
    "            'kpss_statistic': kpss_statistic,\n",
    "            'kpss_p_value': kpss_p_value,\n",
    "            'is_stationary': adf_p_value < 0.05 and kpss_p_value > 0.05\n",
    "        }\n",
    "    \n",
    "    def _find_best_arima_order(self, series: pd.Series) -> Tuple[int, int, int]:\n",
    "        \"\"\"\n",
    "        Find the best ARIMA order using grid search with AIC criterion.\n",
    "        \n",
    "        Args:\n",
    "            series: Time series data\n",
    "            \n",
    "        Returns:\n",
    "            Best ARIMA order (p, d, q)\n",
    "        \"\"\"\n",
    "        best_aic = float('inf')\n",
    "        best_order = (0, 0, 0)\n",
    "        \n",
    "        # Remove NaN values\n",
    "        series_clean = series.dropna()\n",
    "        \n",
    "        if len(series_clean) < 10:  # Need minimum data points\n",
    "            return (1, 1, 1)\n",
    "        \n",
    "        # Grid search for best parameters\n",
    "        for p in range(self.max_p + 1):\n",
    "            for d in range(self.max_d + 1):\n",
    "                for q in range(self.max_q + 1):\n",
    "                    try:\n",
    "                        if self.seasonal:\n",
    "                            # For seasonal ARIMA\n",
    "                            model = ARIMA(series_clean, order=(p, d, q), \n",
    "                                        seasonal_order=(1, 1, 1, self.m))\n",
    "                        else:\n",
    "                            # For non-seasonal ARIMA\n",
    "                            model = ARIMA(series_clean, order=(p, d, q))\n",
    "                        \n",
    "                        fitted_model = model.fit()\n",
    "                        aic = fitted_model.aic\n",
    "                        \n",
    "                        if aic < best_aic:\n",
    "                            best_aic = aic\n",
    "                            best_order = (p, d, q)\n",
    "                    except:\n",
    "                        continue\n",
    "        \n",
    "        return best_order\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit ARIMA models for each store-department combination.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data with time series\n",
    "            y: Not used (for compatibility)\n",
    "        \"\"\"\n",
    "        # Group by store and department\n",
    "        groups = X.groupby(['Store', 'Dept'])\n",
    "        \n",
    "        for (store, dept), group_data in groups:\n",
    "            key = f\"{store}_{dept}\"\n",
    "            \n",
    "            # Sort by date\n",
    "            group_data = group_data.sort_values('Date')\n",
    "            \n",
    "            # Extract sales time series\n",
    "            sales_series = group_data['Weekly_Sales']\n",
    "            \n",
    "            # Skip if insufficient data\n",
    "            if len(sales_series) < 10:\n",
    "                continue\n",
    "            \n",
    "            # Check stationarity\n",
    "            stationarity_test = self._check_stationarity(sales_series)\n",
    "            \n",
    "            try:\n",
    "                if self.auto_arima:\n",
    "                    # Find best ARIMA order\n",
    "                    best_order = self._find_best_arima_order(sales_series)\n",
    "                    self.best_orders[key] = best_order\n",
    "                    \n",
    "                    if self.seasonal:\n",
    "                        # Fit seasonal ARIMA\n",
    "                        model = ARIMA(sales_series, order=best_order,\n",
    "                                    seasonal_order=(1, 1, 1, self.m))\n",
    "                    else:\n",
    "                        # Fit non-seasonal ARIMA\n",
    "                        model = ARIMA(sales_series, order=best_order)\n",
    "                else:\n",
    "                    # Use predefined order\n",
    "                    if self.seasonal and self.seasonal_order:\n",
    "                        model = ARIMA(sales_series, order=self.order,\n",
    "                                    seasonal_order=self.seasonal_order)\n",
    "                    else:\n",
    "                        model = ARIMA(sales_series, order=self.order)\n",
    "                \n",
    "                # Fit the model\n",
    "                fitted_model = model.fit()\n",
    "                self.models[key] = fitted_model\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to fit ARIMA for Store {store}, Dept {dept}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for the test set.\n",
    "        \n",
    "        Args:\n",
    "            X: Test data\n",
    "            \n",
    "        Returns:\n",
    "            Predictions array\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for idx, row in X.iterrows():\n",
    "            store = row['Store']\n",
    "            dept = row['Dept']\n",
    "            key = f\"{store}_{dept}\"\n",
    "            \n",
    "            if key in self.models:\n",
    "                try:\n",
    "                    # Make prediction for one step ahead\n",
    "                    forecast = self.models[key].forecast(steps=1)\n",
    "                    predictions.append(forecast[0])\n",
    "                except:\n",
    "                    # If prediction fails, use last known value or mean\n",
    "                    predictions.append(0)  # Default fallback\n",
    "            else:\n",
    "                predictions.append(0)  # Default for unseen store-dept combinations\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def get_model_summary(self, store: int, dept: int) -> str:\n",
    "        \"\"\"Get summary of fitted model for specific store-department combination.\"\"\"\n",
    "        key = f\"{store}_{dept}\"\n",
    "        if key in self.models:\n",
    "            return str(self.models[key].summary())\n",
    "        else:\n",
    "            return f\"No model found for Store {store}, Dept {dept}\"\n",
    "\n",
    "class ARIMAPipeline:\n",
    "    \"\"\"\n",
    "    Complete ARIMA pipeline for Walmart sales forecasting.\n",
    "    Combines preprocessing, modeling, and evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 auto_arima: bool = True,\n",
    "                 seasonal: bool = True,\n",
    "                 test_size: float = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize the ARIMA pipeline.\n",
    "        \n",
    "        Args:\n",
    "            auto_arima: Whether to use automatic ARIMA parameter selection\n",
    "            seasonal: Whether to include seasonal components\n",
    "            test_size: Fraction of data to use for testing\n",
    "        \"\"\"\n",
    "        self.auto_arima = auto_arima\n",
    "        self.seasonal = seasonal\n",
    "        self.test_size = test_size\n",
    "        \n",
    "        # Initialize components\n",
    "        self.preprocessor = WalmartDataPreprocessor()\n",
    "        self.model = ARIMATimeSeriesModel(auto_arima=auto_arima, seasonal=seasonal)\n",
    "        \n",
    "        # Create scikit-learn pipeline\n",
    "        self.pipeline = Pipeline([\n",
    "            ('preprocessor', self.preprocessor),\n",
    "            ('model', self.model)\n",
    "        ])\n",
    "        \n",
    "        self.is_fitted = False\n",
    "        self.train_metrics = {}\n",
    "        self.test_metrics = {}\n",
    "    \n",
    "    def load_data(self, train_path: str, test_path: str, stores_path: str = None, features_path: str = None):\n",
    "        \"\"\"\n",
    "        Load Walmart dataset from CSV files.\n",
    "        \n",
    "        Args:\n",
    "            train_path: Path to training data CSV\n",
    "            test_path: Path to test data CSV\n",
    "            stores_path: Path to stores data CSV (optional)\n",
    "            features_path: Path to features data CSV (optional)\n",
    "        \"\"\"\n",
    "        # Load main datasets\n",
    "        self.train_data = pd.read_csv(train_path)\n",
    "        self.test_data = pd.read_csv(test_path)\n",
    "        \n",
    "        # Load additional datasets if provided\n",
    "        if stores_path:\n",
    "            self.stores_data = pd.read_csv(stores_path)\n",
    "            # Merge stores data\n",
    "            self.train_data = self.train_data.merge(self.stores_data, on='Store', how='left')\n",
    "            self.test_data = self.test_data.merge(self.stores_data, on='Store', how='left')\n",
    "        \n",
    "        if features_path:\n",
    "            self.features_data = pd.read_csv(features_path)\n",
    "            # Merge features data\n",
    "            self.train_data = self.train_data.merge(self.features_data, on=['Store', 'Date'], how='left')\n",
    "            self.test_data = self.test_data.merge(self.features_data, on=['Store', 'Date'], how='left')\n",
    "        \n",
    "        print(f\"Training data shape: {self.train_data.shape}\")\n",
    "        print(f\"Test data shape: {self.test_data.shape}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def explore_data(self):\n",
    "        \"\"\"Perform exploratory data analysis.\"\"\"\n",
    "        print(\"=== Data Exploration ===\")\n",
    "        print(f\"Training period: {self.train_data['Date'].min()} to {self.train_data['Date'].max()}\")\n",
    "        print(f\"Number of stores: {self.train_data['Store'].nunique()}\")\n",
    "        print(f\"Number of departments: {self.train_data['Dept'].nunique()}\")\n",
    "        print(f\"Missing values in Weekly_Sales: {self.train_data['Weekly_Sales'].isna().sum()}\")\n",
    "        \n",
    "        # Plot some sample time series\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Plot sales for a few store-department combinations\n",
    "        sample_stores = self.train_data['Store'].unique()[:2]\n",
    "        sample_depts = self.train_data['Dept'].unique()[:2]\n",
    "        \n",
    "        for i, store in enumerate(sample_stores):\n",
    "            for j, dept in enumerate(sample_depts):\n",
    "                mask = (self.train_data['Store'] == store) & (self.train_data['Dept'] == dept)\n",
    "                store_dept_data = self.train_data[mask].copy()\n",
    "                \n",
    "                if len(store_dept_data) > 0:\n",
    "                    store_dept_data['Date'] = pd.to_datetime(store_dept_data['Date'])\n",
    "                    store_dept_data = store_dept_data.sort_values('Date')\n",
    "                    \n",
    "                    axes[i, j].plot(store_dept_data['Date'], store_dept_data['Weekly_Sales'])\n",
    "                    axes[i, j].set_title(f'Store {store}, Dept {dept}')\n",
    "                    axes[i, j].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Show basic statistics\n",
    "        print(\"\\n=== Sales Statistics ===\")\n",
    "        print(self.train_data['Weekly_Sales'].describe())\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def train_test_split(self):\n",
    "        \"\"\"Split data into training and validation sets.\"\"\"\n",
    "        # Sort by date\n",
    "        self.train_data['Date'] = pd.to_datetime(self.train_data['Date'])\n",
    "        self.train_data = self.train_data.sort_values(['Store', 'Dept', 'Date'])\n",
    "        \n",
    "        # Split by time for each store-department combination\n",
    "        train_list = []\n",
    "        val_list = []\n",
    "        \n",
    "        for (store, dept), group in self.train_data.groupby(['Store', 'Dept']):\n",
    "            group = group.sort_values('Date')\n",
    "            split_idx = int(len(group) * (1 - self.test_size))\n",
    "            \n",
    "            train_part = group.iloc[:split_idx]\n",
    "            val_part = group.iloc[split_idx:]\n",
    "            \n",
    "            train_list.append(train_part)\n",
    "            val_list.append(val_part)\n",
    "        \n",
    "        self.X_train = pd.concat(train_list, ignore_index=True)\n",
    "        self.X_val = pd.concat(val_list, ignore_index=True)\n",
    "        \n",
    "        print(f\"Training set size: {len(self.X_train)}\")\n",
    "        print(f\"Validation set size: {len(self.X_val)}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"Fit the ARIMA pipeline.\"\"\"\n",
    "        print(\"=== Training ARIMA Models ===\")\n",
    "        \n",
    "        # Fit the pipeline\n",
    "        self.pipeline.fit(self.X_train)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        # Make predictions on training set\n",
    "        train_predictions = self.pipeline.predict(self.X_train)\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_actual = self.X_train['Weekly_Sales'].values\n",
    "        self.train_metrics = {\n",
    "            'MAE': mean_absolute_error(train_actual, train_predictions),\n",
    "            'RMSE': np.sqrt(mean_squared_error(train_actual, train_predictions)),\n",
    "            'MAPE': np.mean(np.abs((train_actual - train_predictions) / train_actual)) * 100\n",
    "        }\n",
    "        \n",
    "        print(f\"Training MAE: {self.train_metrics['MAE']:.2f}\")\n",
    "        print(f\"Training RMSE: {self.train_metrics['RMSE']:.2f}\")\n",
    "        print(f\"Training MAPE: {self.train_metrics['MAPE']:.2f}%\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate the model on validation set.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before evaluation\")\n",
    "        \n",
    "        print(\"=== Evaluating on Validation Set ===\")\n",
    "        \n",
    "        # Make predictions on validation set\n",
    "        val_predictions = self.pipeline.predict(self.X_val)\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_actual = self.X_val['Weekly_Sales'].values\n",
    "        self.test_metrics = {\n",
    "            'MAE': mean_absolute_error(val_actual, val_predictions),\n",
    "            'RMSE': np.sqrt(mean_squared_error(val_actual, val_predictions)),\n",
    "            'MAPE': np.mean(np.abs((val_actual - val_predictions) / val_actual)) * 100\n",
    "        }\n",
    "        \n",
    "        print(f\"Validation MAE: {self.test_metrics['MAE']:.2f}\")\n",
    "        print(f\"Validation RMSE: {self.test_metrics['RMSE']:.2f}\")\n",
    "        print(f\"Validation MAPE: {self.test_metrics['MAPE']:.2f}%\")\n",
    "        \n",
    "        # Plot some predictions vs actual\n",
    "        self._plot_predictions(val_actual, val_predictions)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _plot_predictions(self, actual, predictions, n_samples=4):\n",
    "        \"\"\"Plot predictions vs actual values for sample store-department combinations.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Get unique store-department combinations\n",
    "        store_dept_combinations = self.X_val[['Store', 'Dept']].drop_duplicates().head(n_samples)\n",
    "        \n",
    "        for i, (_, row) in enumerate(store_dept_combinations.iterrows()):\n",
    "            if i >= n_samples:\n",
    "                break\n",
    "                \n",
    "            store, dept = row['Store'], row['Dept']\n",
    "            mask = (self.X_val['Store'] == store) & (self.X_val['Dept'] == dept)\n",
    "            \n",
    "            store_dept_data = self.X_val[mask].copy()\n",
    "            store_dept_actual = actual[mask]\n",
    "            store_dept_pred = predictions[mask]\n",
    "            \n",
    "            if len(store_dept_data) > 0:\n",
    "                store_dept_data['Date'] = pd.to_datetime(store_dept_data['Date'])\n",
    "                store_dept_data = store_dept_data.sort_values('Date')\n",
    "                \n",
    "                axes[i].plot(store_dept_data['Date'], store_dept_actual, label='Actual', marker='o')\n",
    "                axes[i].plot(store_dept_data['Date'], store_dept_pred, label='Predicted', marker='s')\n",
    "                axes[i].set_title(f'Store {store}, Dept {dept}')\n",
    "                axes[i].legend()\n",
    "                axes[i].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def predict_test(self):\n",
    "        \"\"\"Make predictions on the test set.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "        \n",
    "        print(\"=== Making Test Predictions ===\")\n",
    "        \n",
    "        # Make predictions\n",
    "        test_predictions = self.pipeline.predict(self.test_data)\n",
    "        \n",
    "        # Create submission dataframe\n",
    "        submission = pd.DataFrame({\n",
    "            'Id': self.test_data['Id'],\n",
    "            'Weekly_Sales': test_predictions\n",
    "        })\n",
    "        \n",
    "        return submission\n",
    "    \n",
    "    def save_model(self, model_path: str):\n",
    "        \"\"\"Save the trained pipeline.\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before saving\")\n",
    "        \n",
    "        joblib.dump(self.pipeline, model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    def load_model(self, model_path: str):\n",
    "        \"\"\"Load a pre-trained pipeline.\"\"\"\n",
    "        self.pipeline = joblib.load(model_path)\n",
    "        self.is_fitted = True\n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "# MLflow experiment tracking functions\n",
    "def log_arima_experiment(pipeline: ARIMAPipeline, experiment_name: str = \"ARIMA_Training\"):\n",
    "    \"\"\"Log ARIMA experiment to MLflow.\"\"\"\n",
    "    \n",
    "    # Set or create experiment\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"ARIMA_Training\"):\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"auto_arima\", pipeline.auto_arima)\n",
    "        mlflow.log_param(\"seasonal\", pipeline.seasonal)\n",
    "        mlflow.log_param(\"test_size\", pipeline.test_size)\n",
    "        \n",
    "        # Log metrics\n",
    "        if pipeline.train_metrics:\n",
    "            for metric, value in pipeline.train_metrics.items():\n",
    "                mlflow.log_metric(f\"train_{metric.lower()}\", value)\n",
    "        \n",
    "        if pipeline.test_metrics:\n",
    "            for metric, value in pipeline.test_metrics.items():\n",
    "                mlflow.log_metric(f\"val_{metric.lower()}\", value)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(\n",
    "            pipeline.pipeline,\n",
    "            \"arima_model\",\n",
    "            registered_model_name=\"WalmartSales_ARIMA\"\n",
    "        )\n",
    "        \n",
    "        print(\"Experiment logged to MLflow successfully!\")\n",
    "\n",
    "# Example usage function\n",
    "def run_arima_pipeline():\n",
    "    \"\"\"Example function showing how to use the ARIMA pipeline.\"\"\"\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = ARIMAPipeline(\n",
    "        auto_arima=True,\n",
    "        seasonal=True,\n",
    "        test_size=0.2\n",
    "    )\n",
    "    \n",
    "    # Load data (you need to provide actual file paths)\n",
    "    # pipeline.load_data(\n",
    "    #     train_path='train.csv',\n",
    "    #     test_path='test.csv',\n",
    "    #     stores_path='stores.csv',\n",
    "    #     features_path='features.csv'\n",
    "    # )\n",
    "    \n",
    "    # Explore data\n",
    "    # pipeline.explore_data()\n",
    "    \n",
    "    # Split data\n",
    "    # pipeline.train_test_split()\n",
    "    \n",
    "    # Train model\n",
    "    # pipeline.fit()\n",
    "    \n",
    "    # Evaluate model\n",
    "    # pipeline.evaluate()\n",
    "    \n",
    "    # Make test predictions\n",
    "    # submission = pipeline.predict_test()\n",
    "    # submission.to_csv('arima_submission.csv', index=False)\n",
    "    \n",
    "    # Save model\n",
    "    # pipeline.save_model('arima_model.joblib')\n",
    "    \n",
    "    # Log to MLflow\n",
    "    # log_arima_experiment(pipeline, \"ARIMA_Experiments\")\n",
    "    \n",
    "    print(\"ARIMA pipeline completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the pipeline\n",
    "    run_arima_pipeline()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
