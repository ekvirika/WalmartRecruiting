{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999760df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ab412",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn mlflow\n",
    "\n",
    "# Set up Kaggle API\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159efafd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Upload your kaggle.json to Colab and run:\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56603c2f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
    "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39259775",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!unzip -q train.csv.zip\n",
    "!unzip -q stores.csv.zip\n",
    "!unzip -q test.csv.zip\n",
    "!unzip -q features.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2625074f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b91041f",
   "metadata": {},
   "source": [
    "# 1. DATA PREPROCESSING CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f7b61",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class WalmartDataPreprocessor:\n",
    "    \"\"\"\n",
    "    Data preprocessing class for Walmart sales data\n",
    "    Handles cleaning, feature engineering, and time series preparation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_columns = None\n",
    "        self.target_column = 'Weekly_Sales'\n",
    "        \n",
    "    def load_data(self, train_path, test_path, stores_path, features_path):\n",
    "        \"\"\"Load all datasets and merge them\"\"\"\n",
    "        # Load datasets\n",
    "        train = pd.read_csv(train_path)\n",
    "        test = pd.read_csv(test_path)\n",
    "        stores = pd.read_csv(stores_path)\n",
    "        features = pd.read_csv(features_path)\n",
    "        \n",
    "        # Merge datasets\n",
    "        train = train.merge(stores, on='Store', how='left')\n",
    "        train = train.merge(features, on=['Store', 'Date'], how='left')\n",
    "        \n",
    "        test = test.merge(stores, on='Store', how='left')\n",
    "        test = test.merge(features, on=['Store', 'Date'], how='left')\n",
    "        \n",
    "        return train, test\n",
    "    \n",
    "    def create_time_features(self, df):\n",
    "        \"\"\"Create time-based features from Date column\"\"\"\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        df['Year'] = df['Date'].dt.year\n",
    "        df['Month'] = df['Date'].dt.month\n",
    "        df['Week'] = df['Date'].dt.isocalendar().week\n",
    "        df['Day'] = df['Date'].dt.day\n",
    "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "        df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
    "        \n",
    "        # Create cyclical features\n",
    "        df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "        df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "        df['Week_sin'] = np.sin(2 * np.pi * df['Week'] / 52)\n",
    "        df['Week_cos'] = np.cos(2 * np.pi * df['Week'] / 52)\n",
    "        df['DayOfWeek_sin'] = np.sin(2 * np.pi * df['DayOfWeek'] / 7)\n",
    "        df['DayOfWeek_cos'] = np.cos(2 * np.pi * df['DayOfWeek'] / 7)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def handle_missing_values(self, df):\n",
    "        \"\"\"Handle missing values in the dataset\"\"\"\n",
    "        # Fill missing values with appropriate strategies\n",
    "        numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_columns:\n",
    "            if col != self.target_column:\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "        # Fill categorical missing values\n",
    "        categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "        for col in categorical_columns:\n",
    "            df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'Unknown')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_lag_features(self, df, lags=[1, 2, 3, 4, 8, 12]):\n",
    "        \"\"\"Create lag features for time series\"\"\"\n",
    "        df = df.sort_values(['Store', 'Dept', 'Date'])\n",
    "        \n",
    "        for lag in lags:\n",
    "            df[f'Sales_lag_{lag}'] = df.groupby(['Store', 'Dept'])[self.target_column].shift(lag)\n",
    "        \n",
    "        # Create rolling statistics\n",
    "        for window in [4, 8, 12]:\n",
    "            df[f'Sales_rolling_mean_{window}'] = df.groupby(['Store', 'Dept'])[self.target_column].rolling(window=window).mean().reset_index(0, drop=True)\n",
    "            df[f'Sales_rolling_std_{window}'] = df.groupby(['Store', 'Dept'])[self.target_column].rolling(window=window).std().reset_index(0, drop=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_features(self, df, is_train=True):\n",
    "        \"\"\"Prepare final feature set\"\"\"\n",
    "        # Create time features\n",
    "        df = self.create_time_features(df)\n",
    "        \n",
    "        # Handle missing values\n",
    "        df = self.handle_missing_values(df)\n",
    "        \n",
    "        # Create lag features only for training data\n",
    "        if is_train:\n",
    "            df = self.create_lag_features(df)\n",
    "        \n",
    "        # Select relevant features\n",
    "        feature_cols = [\n",
    "            'Store', 'Dept', 'Size', 'Type',\n",
    "            'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5',\n",
    "            'CPI', 'Unemployment', 'IsHoliday',\n",
    "            'Year', 'Month', 'Week', 'Day', 'DayOfWeek',\n",
    "            'Month_sin', 'Month_cos', 'Week_sin', 'Week_cos', 'DayOfWeek_sin', 'DayOfWeek_cos'\n",
    "        ]\n",
    "        \n",
    "        # Add lag features if they exist\n",
    "        lag_features = [col for col in df.columns if 'lag' in col or 'rolling' in col]\n",
    "        feature_cols.extend(lag_features)\n",
    "        \n",
    "        # Handle categorical variables\n",
    "        df = pd.get_dummies(df, columns=['Type'], prefix='Type')\n",
    "        \n",
    "        # Update feature columns with dummy variables\n",
    "        feature_cols = [col for col in feature_cols if col != 'Type']\n",
    "        feature_cols.extend([col for col in df.columns if col.startswith('Type_')])\n",
    "        \n",
    "        # Keep only existing columns\n",
    "        feature_cols = [col for col in feature_cols if col in df.columns]\n",
    "        \n",
    "        self.feature_columns = feature_cols\n",
    "        return df[feature_cols + ([self.target_column] if is_train else [])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824262b9",
   "metadata": {},
   "source": [
    "# 2. DLINEAR MODEL IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd4f7d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class DLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    DLinear: Decomposition Linear Model for Time Series Forecasting\n",
    "    \n",
    "    The model decomposes the time series into trend and seasonal components\n",
    "    and applies linear layers to each component separately.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seq_len, pred_len, enc_in, individual=False, kernel_size=25):\n",
    "        super(DLinear, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.individual = individual\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        # Decomposition\n",
    "        self.decomposition = SeriesDecomposition(kernel_size)\n",
    "        \n",
    "        if self.individual:\n",
    "            self.Linear_Seasonal = nn.ModuleList([\n",
    "                nn.Linear(self.seq_len, self.pred_len) for _ in range(enc_in)\n",
    "            ])\n",
    "            self.Linear_Trend = nn.ModuleList([\n",
    "                nn.Linear(self.seq_len, self.pred_len) for _ in range(enc_in)\n",
    "            ])\n",
    "        else:\n",
    "            self.Linear_Seasonal = nn.Linear(self.seq_len, self.pred_len)\n",
    "            self.Linear_Trend = nn.Linear(self.seq_len, self.pred_len)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [Batch, Input length, Channel]\n",
    "        seasonal_init, trend_init = self.decomposition(x)\n",
    "        \n",
    "        seasonal_init = seasonal_init.permute(0, 2, 1)  # [Batch, Channel, Input length]\n",
    "        trend_init = trend_init.permute(0, 2, 1)\n",
    "        \n",
    "        if self.individual:\n",
    "            seasonal_output = torch.zeros([seasonal_init.size(0), seasonal_init.size(1), self.pred_len],\n",
    "                                        dtype=seasonal_init.dtype).to(seasonal_init.device)\n",
    "            trend_output = torch.zeros([trend_init.size(0), trend_init.size(1), self.pred_len],\n",
    "                                     dtype=trend_init.dtype).to(trend_init.device)\n",
    "            \n",
    "            for i in range(seasonal_init.size(1)):\n",
    "                seasonal_output[:, i, :] = self.Linear_Seasonal[i](seasonal_init[:, i, :])\n",
    "                trend_output[:, i, :] = self.Linear_Trend[i](trend_init[:, i, :])\n",
    "        else:\n",
    "            seasonal_output = self.Linear_Seasonal(seasonal_init)\n",
    "            trend_output = self.Linear_Trend(trend_init)\n",
    "        \n",
    "        x = seasonal_output + trend_output\n",
    "        return x.permute(0, 2, 1)  # [Batch, Output length, Channel]\n",
    "\n",
    "\n",
    "class SeriesDecomposition(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block for separating trend and seasonal components\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, kernel_size):\n",
    "        super(SeriesDecomposition, self).__init__()\n",
    "        self.moving_avg = MovingAverage(kernel_size, stride=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "\n",
    "class MovingAverage(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block for trend extraction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(MovingAverage, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, features]\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb2111",
   "metadata": {},
   "source": [
    "# 3. DATASET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986c5d1e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class WalmartTimeSeriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for Walmart time series data\n",
    "    Creates sequences for training DLinear model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, seq_len=52, pred_len=1, target_col='Weekly_Sales'):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.target_col = target_col\n",
    "        \n",
    "        # Prepare sequences\n",
    "        self.sequences = self._create_sequences()\n",
    "    \n",
    "    def _create_sequences(self):\n",
    "        sequences = []\n",
    "        \n",
    "        # Group by Store and Dept for creating sequences\n",
    "        for (store, dept), group in self.data.groupby(['Store', 'Dept']):\n",
    "            group = group.sort_values('Date')\n",
    "            \n",
    "            # Skip if not enough data\n",
    "            if len(group) < self.seq_len + self.pred_len:\n",
    "                continue\n",
    "            \n",
    "            # Create sequences\n",
    "            for i in range(len(group) - self.seq_len - self.pred_len + 1):\n",
    "                seq_data = group.iloc[i:i+self.seq_len]\n",
    "                target_data = group.iloc[i+self.seq_len:i+self.seq_len+self.pred_len]\n",
    "                \n",
    "                # Extract features (excluding target column)\n",
    "                feature_cols = [col for col in group.columns if col != self.target_col and col not in ['Date', 'Store', 'Dept']]\n",
    "                \n",
    "                x = seq_data[feature_cols].values\n",
    "                y = target_data[self.target_col].values\n",
    "                \n",
    "                sequences.append((x, y, store, dept))\n",
    "        \n",
    "        return sequences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y, store, dept = self.sequences[idx]\n",
    "        return torch.FloatTensor(x), torch.FloatTensor(y), store, dept\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7bd4a7",
   "metadata": {},
   "source": [
    "# 4. TRAINING CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e4e2cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class DLinearTrainer:\n",
    "    \"\"\"\n",
    "    Training class for DLinear model with MLflow logging\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.best_loss = float('inf')\n",
    "        \n",
    "    def train_epoch(self, dataloader, optimizer, criterion):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (x, y, _, _) in enumerate(dataloader):\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = self.model(x)\n",
    "            \n",
    "            # Reshape output to match target\n",
    "            output = output.squeeze(-1)  # Remove last dimension if it's 1\n",
    "            loss = criterion(output, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / len(dataloader)\n",
    "    \n",
    "    def validate(self, dataloader, criterion):\n",
    "        \"\"\"Validate the model\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y, _, _ in dataloader:\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                output = self.model(x)\n",
    "                output = output.squeeze(-1)\n",
    "                loss = criterion(output, y)\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / len(dataloader)\n",
    "    \n",
    "    def fit(self, train_loader, val_loader, epochs=100, lr=0.001):\n",
    "        \"\"\"Train the model with MLflow logging\"\"\"\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # MLflow logging\n",
    "        mlflow.log_param(\"learning_rate\", lr)\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"optimizer\", \"Adam\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_loss = self.train_epoch(train_loader, optimizer, criterion)\n",
    "            val_loss = self.validate(val_loader, criterion)\n",
    "            \n",
    "            # Log metrics\n",
    "            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < self.best_loss:\n",
    "                self.best_loss = val_loss\n",
    "                torch.save(self.model.state_dict(), 'best_dlinear_model.pth')\n",
    "                mlflow.log_metric(\"best_val_loss\", val_loss, step=epoch)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        return self.model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa95722",
   "metadata": {},
   "source": [
    "# 5. MAIN PIPELINE CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b92e87",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class DLinearPipeline:\n",
    "    \"\"\"\n",
    "    Main pipeline class that orchestrates the entire DLinear workflow\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seq_len=52, pred_len=1):\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.preprocessor = WalmartDataPreprocessor()\n",
    "        self.model = None\n",
    "        self.trainer = None\n",
    "        \n",
    "    def load_and_prepare_data(self, train_path, test_path, stores_path, features_path):\n",
    "        \"\"\"Load and prepare data for training\"\"\"\n",
    "        print(\"Loading and preparing data...\")\n",
    "        \n",
    "        # Load data\n",
    "        train_data, test_data = self.preprocessor.load_data(\n",
    "            train_path, test_path, stores_path, features_path\n",
    "        )\n",
    "        \n",
    "        # Prepare features\n",
    "        train_prepared = self.preprocessor.prepare_features(train_data, is_train=True)\n",
    "        test_prepared = self.preprocessor.prepare_features(test_data, is_train=False)\n",
    "        \n",
    "        # Remove rows with NaN values (from lag features)\n",
    "        train_prepared = train_prepared.dropna()\n",
    "        \n",
    "        return train_prepared, test_prepared\n",
    "    \n",
    "    def create_datasets(self, train_data, val_split=0.2):\n",
    "        \"\"\"Create train and validation datasets\"\"\"\n",
    "        print(\"Creating datasets...\")\n",
    "        \n",
    "        # Split data\n",
    "        train_size = int(len(train_data) * (1 - val_split))\n",
    "        train_df = train_data.iloc[:train_size]\n",
    "        val_df = train_data.iloc[train_size:]\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = WalmartTimeSeriesDataset(train_df, self.seq_len, self.pred_len)\n",
    "        val_dataset = WalmartTimeSeriesDataset(val_df, self.seq_len, self.pred_len)\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "    \n",
    "    def build_model(self, input_dim):\n",
    "        \"\"\"Build DLinear model\"\"\"\n",
    "        print(\"Building DLinear model...\")\n",
    "        \n",
    "        self.model = DLinear(\n",
    "            seq_len=self.seq_len,\n",
    "            pred_len=self.pred_len,\n",
    "            enc_in=input_dim,\n",
    "            individual=False,\n",
    "            kernel_size=25\n",
    "        )\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def train_model(self, train_dataset, val_dataset, batch_size=32, epochs=100, lr=0.001):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        print(\"Training DLinear model...\")\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Get input dimension from first batch\n",
    "        sample_batch = next(iter(train_loader))\n",
    "        input_dim = sample_batch[0].shape[-1]\n",
    "        \n",
    "        # Build model\n",
    "        self.model = self.build_model(input_dim)\n",
    "        \n",
    "        # Initialize trainer\n",
    "        self.trainer = DLinearTrainer(self.model)\n",
    "        \n",
    "        # Train model\n",
    "        self.model = self.trainer.fit(train_loader, val_loader, epochs, lr)\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def predict(self, test_data):\n",
    "        \"\"\"Make predictions on test data\"\"\"\n",
    "        print(\"Making predictions...\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        \n",
    "        # Create test dataset\n",
    "        test_dataset = WalmartTimeSeriesDataset(test_data, self.seq_len, self.pred_len)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, _, store, dept in test_loader:\n",
    "                x = x.to(self.trainer.device)\n",
    "                output = self.model(x)\n",
    "                pred = output.squeeze().cpu().numpy()\n",
    "                predictions.append({\n",
    "                    'Store': store,\n",
    "                    'Dept': dept,\n",
    "                    'Prediction': pred\n",
    "                })\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def run_experiment(self, train_path, test_path, stores_path, features_path, \n",
    "                      experiment_name=\"DLinear_Training\"):\n",
    "        \"\"\"Run complete experiment with MLflow tracking\"\"\"\n",
    "        \n",
    "        # Start MLflow run\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        \n",
    "        with mlflow.start_run(run_name=\"DLinear_Full_Pipeline\"):\n",
    "            # Log parameters\n",
    "            mlflow.log_param(\"seq_len\", self.seq_len)\n",
    "            mlflow.log_param(\"pred_len\", self.pred_len)\n",
    "            mlflow.log_param(\"model_type\", \"DLinear\")\n",
    "            \n",
    "            # Step 1: Load and prepare data\n",
    "            train_data, test_data = self.load_and_prepare_data(\n",
    "                train_path, test_path, stores_path, features_path\n",
    "            )\n",
    "            \n",
    "            # Step 2: Create datasets\n",
    "            train_dataset, val_dataset = self.create_datasets(train_data)\n",
    "            \n",
    "            # Step 3: Train model\n",
    "            model = self.train_model(train_dataset, val_dataset)\n",
    "            \n",
    "            # Step 4: Log model\n",
    "            mlflow.pytorch.log_model(model, \"dlinear_model\")\n",
    "            \n",
    "            # Step 5: Make predictions\n",
    "            predictions = self.predict(test_data)\n",
    "            \n",
    "            print(\"Experiment completed successfully!\")\n",
    "            \n",
    "            return model, predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd709b",
   "metadata": {},
   "source": [
    "# 6. USAGE EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25e3bf4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the DLinear pipeline\n",
    "    \"\"\"\n",
    "    # Initialize pipeline\n",
    "    pipeline = DLinearPipeline(seq_len=52, pred_len=1)\n",
    "    \n",
    "    # Define file paths\n",
    "    train_path = \"train.csv\"\n",
    "    test_path = \"test.csv\"\n",
    "    stores_path = \"stores.csv\"\n",
    "    features_path = \"features.csv\"\n",
    "    \n",
    "    # Run experiment\n",
    "    model, predictions = pipeline.run_experiment(\n",
    "        train_path, test_path, stores_path, features_path\n",
    "    )\n",
    "    \n",
    "    # Save predictions\n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "    pred_df.to_csv(\"dlinear_predictions.csv\", index=False)\n",
    "    \n",
    "    print(\"Pipeline completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
