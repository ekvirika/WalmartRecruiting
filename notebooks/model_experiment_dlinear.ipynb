{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekvirika/WalmartRecruiting/blob/main/notebooks/model_experiment_dlinear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "999760df",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "999760df",
        "outputId": "194d280a-2f0c-48cc-ec69-e7dd3d7509d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cc3ab412",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "cc3ab412",
        "outputId": "5355f033-82d0-4618-ef8b-e9ab6a7ad3e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (24.2)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: mlflow-skinny==3.1.1 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.16.2)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.11/dist-packages (from mlflow) (23.0.0)\n",
            "Requirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.41)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.1)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.57.0)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.115.14)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (1.34.1)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.5.3)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.35.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.4.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.6)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==3.1.1->mlflow) (0.46.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.1->mlflow) (3.23.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow) (0.55b1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==3.1.1->mlflow) (0.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.6.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn mlflow\n",
        "\n",
        "# Set up Kaggle API\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "159efafd",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "159efafd"
      },
      "outputs": [],
      "source": [
        "# Upload your kaggle.json to Colab and run:\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "56603c2f",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "56603c2f",
        "outputId": "5339ffbd-9cc6-4025-8f23-b00b3750962f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "walmart-recruiting-store-sales-forecasting.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "replace features.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace sampleSubmission.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace stores.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace test.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace train.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "39259775",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "39259775",
        "outputId": "6e55419d-6d42-49c4-9280-1dbb6d8baa97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "unzip:  cannot find or open stores.csv.zip, stores.csv.zip.zip or stores.csv.zip.ZIP.\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace features.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ],
      "source": [
        "!unzip -q train.csv.zip\n",
        "!unzip -q stores.csv.zip\n",
        "!unzip -q test.csv.zip\n",
        "!unzip -q features.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2625074f",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "2625074f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b91041f",
      "metadata": {
        "id": "2b91041f"
      },
      "source": [
        "# 1. DATA PREPROCESSING CLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "692f7b61",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "692f7b61"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class WalmartDataPreprocessor:\n",
        "    \"\"\"\n",
        "    Data preprocessing class for Walmart sales data\n",
        "    Handles cleaning, feature engineering, and time series preparation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.feature_columns = None\n",
        "        self.target_column = 'Weekly_Sales'\n",
        "\n",
        "    def load_data(self, train_path, test_path, stores_path, features_path):\n",
        "        \"\"\"Load all datasets and merge them\"\"\"\n",
        "        # Load datasets\n",
        "        train = pd.read_csv(train_path)\n",
        "        test = pd.read_csv(test_path)\n",
        "        stores = pd.read_csv(stores_path)\n",
        "        features = pd.read_csv(features_path)\n",
        "\n",
        "        # Merge datasets\n",
        "        train = train.merge(stores, on='Store', how='left')\n",
        "        train = train.merge(features, on=['Store', 'Date'], how='left')\n",
        "\n",
        "        test = test.merge(stores, on='Store', how='left')\n",
        "        test = test.merge(features, on=['Store', 'Date'], how='left')\n",
        "\n",
        "        # Fix IsHoliday duplicates after merge\n",
        "        for df in [train, test]:\n",
        "            if 'IsHoliday_x' in df.columns and 'IsHoliday_y' in df.columns:\n",
        "                df['IsHoliday'] = df['IsHoliday_x']  # or 'IsHoliday_y' if preferred\n",
        "                df.drop(['IsHoliday_x', 'IsHoliday_y'], axis=1, inplace=True)\n",
        "\n",
        "        return train, test\n",
        "\n",
        "    def create_time_features(self, df):\n",
        "        \"\"\"Create time-based features from Date column\"\"\"\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        df['Year'] = df['Date'].dt.year\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Week'] = df['Date'].dt.isocalendar().week.astype(int)\n",
        "        df['Day'] = df['Date'].dt.day\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "\n",
        "        # Ensure IsHoliday is binary\n",
        "        df['IsHoliday'] = df['IsHoliday'].apply(lambda x: 1 if x in [True, 1, 'True', '1'] else 0)\n",
        "\n",
        "        # Create cyclical features\n",
        "        df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
        "        df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
        "        df['Week_sin'] = np.sin(2 * np.pi * df['Week'] / 52)\n",
        "        df['Week_cos'] = np.cos(2 * np.pi * df['Week'] / 52)\n",
        "        df['DayOfWeek_sin'] = np.sin(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "        df['DayOfWeek_cos'] = np.cos(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def handle_missing_values(self, df):\n",
        "        \"\"\"Handle missing values in the dataset\"\"\"\n",
        "        # Fill numeric columns with median\n",
        "        numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
        "        for col in numeric_columns:\n",
        "            if col != self.target_column:\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "        # Fill categorical columns with mode\n",
        "        categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "        for col in categorical_columns:\n",
        "            if df[col].mode().empty:\n",
        "                df[col] = df[col].fillna('Unknown')\n",
        "            else:\n",
        "                df[col] = df[col].fillna(df[col].mode()[0])\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_lag_features(self, df, lags=[1, 2, 4]):\n",
        "        \"\"\"Create lag features for time series\"\"\"\n",
        "        df = df.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "        # Lag features\n",
        "        for lag in lags:\n",
        "            df[f'Sales_lag_{lag}'] = df.groupby(['Store', 'Dept'])[self.target_column].shift(lag)\n",
        "\n",
        "        # Rolling features (use transform to keep index aligned)\n",
        "        for window in [4, 8]:\n",
        "            df[f'Sales_rolling_mean_{window}'] = df.groupby(['Store', 'Dept'])[self.target_column].transform(lambda x: x.shift(1).rolling(window).mean())\n",
        "            df[f'Sales_rolling_std_{window}'] = df.groupby(['Store', 'Dept'])[self.target_column].transform(lambda x: x.shift(1).rolling(window).std())\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "    def prepare_features(self, df, is_train=True):\n",
        "        \"\"\"Prepare final feature set\"\"\"\n",
        "        # Time features\n",
        "        df = self.create_time_features(df)\n",
        "\n",
        "        # Missing values\n",
        "        df = self.handle_missing_values(df)\n",
        "\n",
        "        # Lag features\n",
        "        if is_train:\n",
        "            df = self.create_lag_features(df)\n",
        "\n",
        "        # Initial feature set\n",
        "        feature_cols = [\n",
        "            'Store', 'Dept', 'Size', 'Type',\n",
        "            'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5',\n",
        "            'CPI', 'Unemployment', 'IsHoliday',\n",
        "            'Year', 'Month', 'Week', 'Day', 'DayOfWeek',\n",
        "            'Month_sin', 'Month_cos', 'Week_sin', 'Week_cos', 'DayOfWeek_sin', 'DayOfWeek_cos'\n",
        "        ]\n",
        "\n",
        "        # Add lag/rolling features\n",
        "        lag_features = [col for col in df.columns if 'lag' in col or 'rolling' in col]\n",
        "        feature_cols.extend(lag_features)\n",
        "\n",
        "        # One-hot encode Type\n",
        "        df = pd.get_dummies(df, columns=['Type'], prefix='Type')\n",
        "\n",
        "        # Replace 'Type' with dummy column names\n",
        "        feature_cols = [col for col in feature_cols if col != 'Type']\n",
        "        feature_cols.extend([col for col in df.columns if col.startswith('Type_')])\n",
        "\n",
        "        # Retain only existing columns\n",
        "        feature_cols = [col for col in feature_cols if col in df.columns]\n",
        "\n",
        "        # Save selected features\n",
        "        self.feature_columns = feature_cols\n",
        "\n",
        "        # ✅ Always retain 'Date', 'Store', and 'Dept' for dataset construction\n",
        "        essential_cols = ['Date', 'Store', 'Dept']\n",
        "        for col in essential_cols:\n",
        "            if col not in feature_cols:\n",
        "                feature_cols.insert(0, col)\n",
        "\n",
        "        # Final return\n",
        "        if is_train:\n",
        "            return df[feature_cols + [self.target_column]]\n",
        "        else:\n",
        "            return df[feature_cols]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "824262b9",
      "metadata": {
        "id": "824262b9"
      },
      "source": [
        "# 2. DLINEAR MODEL IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8dfd4f7d",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "8dfd4f7d"
      },
      "outputs": [],
      "source": [
        "class DLinear(nn.Module):\n",
        "    \"\"\"\n",
        "    DLinear: Decomposition Linear Model for Time Series Forecasting\n",
        "\n",
        "    The model decomposes the time series into trend and seasonal components\n",
        "    and applies linear layers to each component separately.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, seq_len, pred_len, enc_in, individual=False, kernel_size=25):\n",
        "        super(DLinear, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.individual = individual\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        # Decomposition\n",
        "        self.decomposition = SeriesDecomposition(kernel_size)\n",
        "\n",
        "        if self.individual:\n",
        "            self.Linear_Seasonal = nn.ModuleList([\n",
        "                nn.Linear(self.seq_len, self.pred_len) for _ in range(enc_in)\n",
        "            ])\n",
        "            self.Linear_Trend = nn.ModuleList([\n",
        "                nn.Linear(self.seq_len, self.pred_len) for _ in range(enc_in)\n",
        "            ])\n",
        "        else:\n",
        "            self.Linear_Seasonal = nn.Linear(self.seq_len, self.pred_len)\n",
        "            self.Linear_Trend = nn.Linear(self.seq_len, self.pred_len)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [Batch, Input length, Channel]\n",
        "        seasonal_init, trend_init = self.decomposition(x)\n",
        "\n",
        "        seasonal_init = seasonal_init.permute(0, 2, 1)  # [Batch, Channel, Input length]\n",
        "        trend_init = trend_init.permute(0, 2, 1)\n",
        "\n",
        "        if self.individual:\n",
        "            seasonal_output = torch.zeros([seasonal_init.size(0), seasonal_init.size(1), self.pred_len],\n",
        "                                        dtype=seasonal_init.dtype).to(seasonal_init.device)\n",
        "            trend_output = torch.zeros([trend_init.size(0), trend_init.size(1), self.pred_len],\n",
        "                                     dtype=trend_init.dtype).to(trend_init.device)\n",
        "\n",
        "            for i in range(seasonal_init.size(1)):\n",
        "                seasonal_output[:, i, :] = self.Linear_Seasonal[i](seasonal_init[:, i, :])\n",
        "                trend_output[:, i, :] = self.Linear_Trend[i](trend_init[:, i, :])\n",
        "        else:\n",
        "            seasonal_output = self.Linear_Seasonal(seasonal_init)\n",
        "            trend_output = self.Linear_Trend(trend_init)\n",
        "\n",
        "        x = seasonal_output + trend_output\n",
        "        return x.permute(0, 2, 1)  # [Batch, Output length, Channel]\n",
        "\n",
        "\n",
        "class SeriesDecomposition(nn.Module):\n",
        "    \"\"\"\n",
        "    Series decomposition block for separating trend and seasonal components\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kernel_size):\n",
        "        super(SeriesDecomposition, self).__init__()\n",
        "        self.moving_avg = MovingAverage(kernel_size, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        moving_mean = self.moving_avg(x)\n",
        "        res = x - moving_mean\n",
        "        return res, moving_mean\n",
        "\n",
        "\n",
        "class MovingAverage(nn.Module):\n",
        "    \"\"\"\n",
        "    Moving average block for trend extraction\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kernel_size, stride):\n",
        "        super(MovingAverage, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch, seq_len, features]\n",
        "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
        "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
        "        x = torch.cat([front, x, end], dim=1)\n",
        "        x = self.avg(x.permute(0, 2, 1))\n",
        "        x = x.permute(0, 2, 1)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cbb2111",
      "metadata": {
        "id": "6cbb2111"
      },
      "source": [
        "# 3. DATASET CLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "986c5d1e",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "986c5d1e"
      },
      "outputs": [],
      "source": [
        "class WalmartTimeSeriesDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset class for Walmart time series data\n",
        "    Creates sequences for training DLinear model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, seq_len=52, pred_len=1, target_col='Weekly_Sales'):\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.target_col = target_col\n",
        "\n",
        "        # Prepare sequences\n",
        "        self.sequences = self._create_sequences()\n",
        "\n",
        "    def _create_sequences(self):\n",
        "        sequences = []\n",
        "\n",
        "        # Group by Store and Dept for creating sequences\n",
        "        for (store, dept), group in self.data.groupby(['Store', 'Dept']):\n",
        "            group = group.sort_values('Date')\n",
        "\n",
        "            # Skip if not enough data\n",
        "            if len(group) < self.seq_len + self.pred_len:\n",
        "                continue\n",
        "\n",
        "            # Create sequences\n",
        "            for i in range(len(group) - self.seq_len - self.pred_len + 1):\n",
        "                seq_data = group.iloc[i:i+self.seq_len]\n",
        "                target_data = group.iloc[i+self.seq_len:i+self.seq_len+self.pred_len]\n",
        "\n",
        "                # Extract features (excluding target column)\n",
        "                feature_cols = [col for col in group.columns if col != self.target_col and col not in ['Date', 'Store', 'Dept']]\n",
        "\n",
        "                x = seq_data[feature_cols].values\n",
        "                y = target_data[self.target_col].values\n",
        "\n",
        "                sequences.append((x, y, store, dept))\n",
        "\n",
        "        return sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y, store, dept = self.sequences[idx]\n",
        "        return torch.FloatTensor(x), torch.FloatTensor(y), store, dept\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e7bd4a7",
      "metadata": {
        "id": "8e7bd4a7"
      },
      "source": [
        "# 4. TRAINING CLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "53e4e2cc",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "53e4e2cc"
      },
      "outputs": [],
      "source": [
        "class DLinearTrainer:\n",
        "    \"\"\"\n",
        "    Training class for DLinear model with MLflow logging\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.best_loss = float('inf')\n",
        "\n",
        "    def train_epoch(self, dataloader, optimizer, criterion):\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, (x, y, _, _) in enumerate(dataloader):\n",
        "            x, y = x.to(self.device), y.to(self.device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = self.model(x)\n",
        "\n",
        "            # Reshape output to match target\n",
        "            output = output.squeeze(-1)  # Remove last dimension if it's 1\n",
        "            loss = criterion(output, y)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "    def validate(self, dataloader, criterion):\n",
        "        \"\"\"Validate the model\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y, _, _ in dataloader:\n",
        "                x, y = x.to(self.device), y.to(self.device)\n",
        "                output = self.model(x)\n",
        "                output = output.squeeze(-1)\n",
        "                loss = criterion(output, y)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "    def fit(self, train_loader, val_loader, epochs=100, lr=0.001):\n",
        "        \"\"\"Train the model with MLflow logging\"\"\"\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        # MLflow logging\n",
        "        mlflow.log_param(\"learning_rate\", lr)\n",
        "        mlflow.log_param(\"epochs\", epochs)\n",
        "        mlflow.log_param(\"optimizer\", \"Adam\")\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            train_loss = self.train_epoch(train_loader, optimizer, criterion)\n",
        "            val_loss = self.validate(val_loader, criterion)\n",
        "\n",
        "            # Log metrics\n",
        "            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
        "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
        "\n",
        "            # Save best model\n",
        "            if val_loss < self.best_loss:\n",
        "                self.best_loss = val_loss\n",
        "                torch.save(self.model.state_dict(), 'best_dlinear_model.pth')\n",
        "                mlflow.log_metric(\"best_val_loss\", val_loss, step=epoch)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "        return self.model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fa95722",
      "metadata": {
        "id": "0fa95722"
      },
      "source": [
        "# 5. MAIN PIPELINE CLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d4b92e87",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "d4b92e87"
      },
      "outputs": [],
      "source": [
        "class DLinearPipeline:\n",
        "    \"\"\"\n",
        "    Main pipeline class that orchestrates the entire DLinear workflow\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, seq_len=52, pred_len=1):\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.preprocessor = WalmartDataPreprocessor()\n",
        "        self.model = None\n",
        "        self.trainer = None\n",
        "\n",
        "    def load_and_prepare_data(self, train_path, test_path, stores_path, features_path):\n",
        "        \"\"\"Load and prepare data for training\"\"\"\n",
        "        print(\"Loading and preparing data...\")\n",
        "\n",
        "        # Load data\n",
        "        train_data, test_data = self.preprocessor.load_data(\n",
        "            train_path, test_path, stores_path, features_path\n",
        "        )\n",
        "\n",
        "        # Prepare features\n",
        "        train_prepared = self.preprocessor.prepare_features(train_data, is_train=True)\n",
        "        test_prepared = self.preprocessor.prepare_features(test_data, is_train=False)\n",
        "\n",
        "        # Remove rows with NaN values (from lag features)\n",
        "        train_prepared = train_prepared.dropna()\n",
        "\n",
        "        return train_prepared, test_prepared\n",
        "\n",
        "    def create_datasets(self, train_data, val_split=0.2):\n",
        "        \"\"\"Create train and validation datasets\"\"\"\n",
        "        print(\"Creating datasets...\")\n",
        "\n",
        "        # Split data\n",
        "        train_size = int(len(train_data) * (1 - val_split))\n",
        "        train_df = train_data.iloc[:train_size]\n",
        "        val_df = train_data.iloc[train_size:]\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = WalmartTimeSeriesDataset(train_df, self.seq_len, self.pred_len)\n",
        "        val_dataset = WalmartTimeSeriesDataset(val_df, self.seq_len, self.pred_len)\n",
        "\n",
        "        return train_dataset, val_dataset\n",
        "\n",
        "    def build_model(self, input_dim):\n",
        "        \"\"\"Build DLinear model\"\"\"\n",
        "        print(\"Building DLinear model...\")\n",
        "\n",
        "        self.model = DLinear(\n",
        "            seq_len=self.seq_len,\n",
        "            pred_len=self.pred_len,\n",
        "            enc_in=input_dim,\n",
        "            individual=False,\n",
        "            kernel_size=25\n",
        "        )\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def train_model(self, train_dataset, val_dataset, batch_size=32, epochs=100, lr=0.001):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "        print(\"Training DLinear model...\")\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Get input dimension from first batch\n",
        "        sample_batch = next(iter(train_loader))\n",
        "        input_dim = sample_batch[0].shape[-1]\n",
        "\n",
        "        # Build model\n",
        "        self.model = self.build_model(input_dim)\n",
        "\n",
        "        # Initialize trainer\n",
        "        self.trainer = DLinearTrainer(self.model)\n",
        "\n",
        "        # Train model\n",
        "        self.model = self.trainer.fit(train_loader, val_loader, epochs, lr)\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def predict(self, test_data):\n",
        "        \"\"\"Make predictions on test data\"\"\"\n",
        "        print(\"Making predictions...\")\n",
        "\n",
        "        self.model.eval()\n",
        "        predictions = []\n",
        "\n",
        "        # Create test dataset\n",
        "        test_dataset = WalmartTimeSeriesDataset(test_data, self.seq_len, self.pred_len)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, _, store, dept in test_loader:\n",
        "                x = x.to(self.trainer.device)\n",
        "                output = self.model(x)\n",
        "                pred = output.squeeze().cpu().numpy()\n",
        "                predictions.append({\n",
        "                    'Store': store,\n",
        "                    'Dept': dept,\n",
        "                    'Prediction': pred\n",
        "                })\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def run_experiment(self, train_path, test_path, stores_path, features_path,\n",
        "                      experiment_name=\"DLinear_Training\"):\n",
        "        \"\"\"Run complete experiment with MLflow tracking\"\"\"\n",
        "\n",
        "        # Start MLflow run\n",
        "        mlflow.set_experiment(experiment_name)\n",
        "\n",
        "        with mlflow.start_run(run_name=\"DLinear_Full_Pipeline\"):\n",
        "            # Log parameters\n",
        "            mlflow.log_param(\"seq_len\", self.seq_len)\n",
        "            mlflow.log_param(\"pred_len\", self.pred_len)\n",
        "            mlflow.log_param(\"model_type\", \"DLinear\")\n",
        "\n",
        "            # Step 1: Load and prepare data\n",
        "            train_data, test_data = self.load_and_prepare_data(\n",
        "                train_path, test_path, stores_path, features_path\n",
        "            )\n",
        "\n",
        "            # Step 2: Create datasets\n",
        "            train_dataset, val_dataset = self.create_datasets(train_data)\n",
        "\n",
        "            # Step 3: Train model\n",
        "            model = self.train_model(train_dataset, val_dataset)\n",
        "\n",
        "            # Step 4: Log model\n",
        "            mlflow.pytorch.log_model(model, \"dlinear_model\")\n",
        "\n",
        "            # Step 5: Make predictions\n",
        "            predictions = self.predict(test_data)\n",
        "\n",
        "            print(\"Experiment completed successfully!\")\n",
        "\n",
        "            return model, predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90dd709b",
      "metadata": {
        "id": "90dd709b"
      },
      "source": [
        "# 6. USAGE EXAMPLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a25e3bf4",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "a25e3bf4",
        "outputId": "bf3f41bb-6a28-460d-eff0-45bd23d4bc3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing data...\n",
            "Creating datasets...\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the DLinear pipeline\n",
        "    \"\"\"\n",
        "    # Initialize pipeline\n",
        "    pipeline = DLinearPipeline(seq_len=52, pred_len=1)\n",
        "\n",
        "    # Define file paths\n",
        "    train_path = \"train.csv\"\n",
        "    test_path = \"test.csv\"\n",
        "    stores_path = \"stores.csv\"\n",
        "    features_path = \"features.csv\"\n",
        "\n",
        "    # Run experiment\n",
        "    model, predictions = pipeline.run_experiment(\n",
        "        train_path, test_path, stores_path, features_path\n",
        "    )\n",
        "\n",
        "    # Save predictions\n",
        "    pred_df = pd.DataFrame(predictions)\n",
        "    pred_df.to_csv(\"dlinear_predictions.csv\", index=False)\n",
        "\n",
        "    print(\"Pipeline completed successfully!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}