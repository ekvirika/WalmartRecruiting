{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekvirika/WalmartRecruiting/blob/main/notebooks/model_experiment_dlinear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "999760df",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "999760df",
        "outputId": "a6977263-be87-47f7-bf9f-0cd8dc1eec81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cc3ab412",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "cc3ab412",
        "outputId": "214023d9-3faa-47c0-d2de-16896caab7ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.11/dist-packages (3.1.4)\n",
            "Requirement already satisfied: neuralforecast in /usr/local/lib/python3.11/dist-packages (3.0.2)\n",
            "Requirement already satisfied: dagshub in /usr/local/lib/python3.11/dist-packages (0.6.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Collecting sympy==1.13.1 (from torch)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: mlflow-skinny==3.1.4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.4)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.16.4)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.11/dist-packages (from mlflow) (23.0.0)\n",
            "Requirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.41)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (3.1.1)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.61.0)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.116.1)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (1.36.0)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.5.3)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.35.0)\n",
            "Requirement already satisfied: coreforecast>=0.0.6 in /usr/local/lib/python3.11/dist-packages (from neuralforecast) (0.0.16)\n",
            "Requirement already satisfied: pytorch-lightning>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from neuralforecast) (2.5.2)\n",
            "Requirement already satisfied: ray>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from ray[tune]>=2.2.0->neuralforecast) (2.48.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (from neuralforecast) (4.4.0)\n",
            "Requirement already satisfied: utilsforecast>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from neuralforecast) (0.2.12)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.4.4)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.28.1)\n",
            "Requirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (13.9.4)\n",
            "Requirement already satisfied: dacite~=1.6.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.6.0)\n",
            "Requirement already satisfied: tenacity>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from dagshub) (8.5.0)\n",
            "Requirement already satisfied: gql[requests] in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.5.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.6.7)\n",
            "Requirement already satisfied: treelib>=1.6.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.8.0)\n",
            "Requirement already satisfied: pathvalidate>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.3.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.40.1)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.0.4)\n",
            "Requirement already satisfied: dagshub-annotation-converter>=0.1.5 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.1.11)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (5.4.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.6)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->dagshub) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning>=2.0.0->neuralforecast) (4.67.1)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning>=2.0.0->neuralforecast) (1.8.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning>=2.0.0->neuralforecast) (0.15.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray>=2.2.0->ray[tune]>=2.2.0->neuralforecast) (4.25.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray>=2.2.0->ray[tune]>=2.2.0->neuralforecast) (1.1.1)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.11/dist-packages (from ray[tune]>=2.2.0->neuralforecast) (2.6.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
            "Requirement already satisfied: botocore<1.41.0,>=1.40.1 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.40.1)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (0.13.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (0.9.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.20.1)\n",
            "Requirement already satisfied: backoff<3.0,>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (2.2.1)\n",
            "Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.0.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna->neuralforecast) (6.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->dagshub) (1.3.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==3.1.4->mlflow) (0.47.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralforecast) (3.12.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.4->mlflow) (3.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning>=2.0.0->neuralforecast) (75.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->dagshub) (0.1.2)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.4->mlflow) (0.57b0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub) (1.1.0)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (0.3.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray>=2.2.0->ray[tune]>=2.2.0->neuralforecast) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray>=2.2.0->ray[tune]>=2.2.0->neuralforecast) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray>=2.2.0->ray[tune]>=2.2.0->neuralforecast) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray>=2.2.0->ray[tune]>=2.2.0->neuralforecast) (0.26.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralforecast) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralforecast) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.0.0->neuralforecast) (1.7.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (4.9.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (0.6.1)\n",
            "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.11.1\n",
            "    Uninstalling sympy-1.11.1:\n",
            "      Successfully uninstalled sympy-1.11.1\n",
            "Successfully installed sympy-1.13.1\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.7.14)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn mlflow neuralforecast dagshub\n",
        "\n",
        "# Set up Kaggle API\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "159efafd",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "159efafd"
      },
      "outputs": [],
      "source": [
        "# Upload your kaggle.json to Colab and run:\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "56603c2f",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "56603c2f",
        "outputId": "551e94df-8603-465a-ef83-700507224800",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "walmart-recruiting-store-sales-forecasting.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "replace features.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "39259775",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "39259775",
        "outputId": "911e4e9f-b8ca-4277-8678-4c014e9ed28e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "unzip:  cannot find or open stores.csv.zip, stores.csv.zip.zip or stores.csv.zip.ZIP.\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "replace features.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ],
      "source": [
        "!unzip -q train.csv.zip\n",
        "!unzip -q stores.csv.zip\n",
        "!unzip -q test.csv.zip\n",
        "!unzip -q features.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from dagshub import dagshub_logger\n",
        "import os\n",
        "import torch\n",
        "\n",
        "\n",
        "# Set tracking URI manually\n",
        "mlflow.set_tracking_uri(\"https://dagshub.com/ekvirika/WalmartRecruiting.mlflow\")\n",
        "\n",
        "# Use your DagsHub credentials\n",
        "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"ekvirika\"\n",
        "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"0adb1004ddd4221395353efea2d8ead625e26197\"\n",
        "\n",
        "# Optional: set registry if you're using model registry\n",
        "mlflow.set_registry_uri(\"https://dagshub.com/ekvirika/WalmartRecruiting.mlflow\")\n",
        "mlflow.set_experiment(\"NBeats_Training\")\n",
        "\n",
        "# Detect GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# W&B setup\n",
        "wandb_project = 'WalmartRecruiting'\n",
        "wandb_entity = None  # Replace with your W&B entity if using teams\n"
      ],
      "metadata": {
        "id": "s0QKXqkFPFXC",
        "outputId": "54d17821-1b50-43c0-c40e-e60f4a8c6639",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "s0QKXqkFPFXC",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "wlXjsRptPHNe",
        "outputId": "5f15609b-9012-4ab2-8064-39236424aef3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wlXjsRptPHNe",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mellekvirikashvili\u001b[0m (\u001b[33mellekvirikashvili-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2625074f",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "2625074f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import wandb\n",
        "import joblib\n",
        "import mlflow\n",
        "import os\n",
        "from itertools import product\n",
        "from neuralforecast.models import PatchTST\n",
        "from neuralforecast import NeuralForecast\n",
        "from datetime import datetime\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.WARNING)\n",
        "for lib in [\"neuralforecast\", \"pytorch_lightning\", \"lightning_fabric\"]:\n",
        "    logging.getLogger(lib).setLevel(logging.WARNING)\n",
        "\n",
        "# --- Data loading ---\n",
        "STORES_PATH = \"stores.csv\"\n",
        "FEATURES_PATH = \"features.csv\"\n",
        "TRAIN_PATH = \"train.csv\"\n",
        "TEST_PATH = \"test.csv\"\n",
        "\n",
        "stores = pd.read_csv(STORES_PATH)\n",
        "features = pd.read_csv(FEATURES_PATH)\n",
        "train = pd.read_csv(TRAIN_PATH)\n",
        "test = pd.read_csv(TEST_PATH)\n",
        "\n",
        "class NeuralForecastModels:\n",
        "    def __init__(self, models, model_names=None, freq='W-FRI', one_model=False):\n",
        "        self.freq = freq\n",
        "        self.one_model = one_model\n",
        "        self.models = models\n",
        "        self.model_names = model_names if model_names else [f\"model_{i}\" for i in range(len(models))]\n",
        "        self.nf = NeuralForecast(models=self.models, freq=self.freq)\n",
        "        self.fitted_df = None\n",
        "\n",
        "    def fit(self, df):\n",
        "        self.fitted_df = df.copy()\n",
        "        self.nf.fit(df=df)\n",
        "\n",
        "    def predict(self, h=None):\n",
        "        if h is None:\n",
        "            h = self.models[0].h\n",
        "        return self.nf.predict(h=h)\n",
        "\n",
        "    def cross_validation(self, df, n_windows=1):\n",
        "        return self.nf.cross_validation(df=df, n_windows=n_windows)\n",
        "\n",
        "def preprocess(df):\n",
        "    df = df.copy()\n",
        "    df['unique_id'] = df['Store'].astype(str) + \"_\" + df['Dept'].astype(str)\n",
        "    df.rename(columns={'Date': 'ds', 'Weekly_Sales': 'y'}, inplace=True)\n",
        "    df['ds'] = pd.to_datetime(df['ds'])\n",
        "    return df.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
        "\n",
        "def prepare_data_for_cv(df, n_windows=1, h=53):\n",
        "    min_length = h * (n_windows + 1) + 10\n",
        "    series_lengths = df.groupby('unique_id').size()\n",
        "    valid_series = series_lengths[series_lengths >= min_length].index\n",
        "    filtered_df = df[df['unique_id'].isin(valid_series)].copy()\n",
        "    print(f\"Original series: {len(series_lengths)}, Valid series for CV: {len(valid_series)}\")\n",
        "    print(f\"Original data points: {len(df)}, Filtered data points: {len(filtered_df)}\")\n",
        "    return filtered_df\n",
        "\n",
        "def compute_wmae(y_true, y_pred, is_holiday=None, holiday_weight=5, non_holiday_weight=1):\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    if y_true.shape != y_pred.shape:\n",
        "        min_len = min(len(y_true), len(y_pred))\n",
        "        y_true = y_true[:min_len]\n",
        "        y_pred = y_pred[:min_len]\n",
        "        if is_holiday is not None:\n",
        "            is_holiday = np.array(is_holiday)[:min_len]\n",
        "    if is_holiday is not None:\n",
        "        weights = np.where(np.array(is_holiday), holiday_weight, non_holiday_weight)\n",
        "    else:\n",
        "        weights = np.ones_like(y_true)\n",
        "    abs_errors = np.abs(y_true - y_pred)\n",
        "    weighted_errors = weights * abs_errors\n",
        "    return weighted_errors.sum() / weights.sum()\n",
        "\n",
        "def create_run_name(params, wmae_score=None):\n",
        "    \"\"\"Create descriptive run name for MLflow\"\"\"\n",
        "    key_params = ['input_size', 'patch_len', 'hidden_size', 'n_heads']\n",
        "    name_parts = []\n",
        "\n",
        "    for param in key_params:\n",
        "        if param in params:\n",
        "            name_parts.append(f\"{param[:3]}{params[param]}\")\n",
        "\n",
        "    if wmae_score is not None:\n",
        "        name_parts.append(f\"wmae{wmae_score:.4f}\")\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%H%M\")\n",
        "    return f\"PatchTST_{'_'.join(name_parts)}_{timestamp}\"\n",
        "\n",
        "def run_patchtst_cv(df, param_grid, fixed_params, experiment_name=\"PatchTST_Hyperparam_Tuning\"):\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "    results = []\n",
        "    keys, values = zip(*param_grid.items())\n",
        "\n",
        "    total_combinations = len(list(product(*values)))\n",
        "    print(f\"Starting hyperparameter tuning with {total_combinations} combinations...\")\n",
        "\n",
        "    for i, vals in enumerate(product(*values), 1):\n",
        "        params = dict(zip(keys, vals))\n",
        "        params.update(fixed_params)\n",
        "        params.update({'enable_progress_bar': False, 'enable_model_summary': False})\n",
        "\n",
        "        # Create descriptive run name\n",
        "        run_name = f\"Trial_{i:02d}_inp{params['input_size']}_patch{params['patch_len']}_hid{params.get('hidden_size', 'def')}\"\n",
        "\n",
        "        try:\n",
        "            with mlflow.start_run(nested=True, run_name=run_name):\n",
        "                # Log parameters with better organization\n",
        "                mlflow.log_params({\n",
        "                    \"model_type\": \"PatchTST\",\n",
        "                    \"trial_number\": i,\n",
        "                    **{f\"param_{k}\": str(v) for k, v in params.items()}\n",
        "                })\n",
        "\n",
        "                # Add tags for better organization\n",
        "                mlflow.set_tag(\"stage\", \"hyperparameter_tuning\")\n",
        "                mlflow.set_tag(\"model_family\", \"transformer\")\n",
        "                mlflow.set_tag(\"trial_id\", f\"trial_{i:02d}\")\n",
        "\n",
        "                model = PatchTST(**params)\n",
        "                nf_model = NeuralForecastModels(models=[model], model_names=['PatchTST'], freq='W-FRI', one_model=True)\n",
        "                cv_df = nf_model.cross_validation(df, n_windows=1)\n",
        "\n",
        "                y_true = cv_df['y'].values\n",
        "                y_pred = cv_df['PatchTST'].values\n",
        "                is_holiday = None\n",
        "\n",
        "                if 'IsHoliday' in df.columns:\n",
        "                    cv_df_with_holiday = cv_df.merge(df[['unique_id', 'ds', 'IsHoliday']], on=['unique_id', 'ds'], how='left')\n",
        "                    is_holiday = cv_df_with_holiday['IsHoliday'].fillna(False).values\n",
        "\n",
        "                score = compute_wmae(y_true, y_pred, is_holiday)\n",
        "\n",
        "                # Log metrics with more context\n",
        "                mlflow.log_metric(\"val_wmae\", score)\n",
        "                mlflow.log_metric(\"data_points\", len(y_true))\n",
        "                mlflow.log_metric(\"progress_pct\", (i / total_combinations) * 100)\n",
        "\n",
        "                # Update run name with score\n",
        "                final_run_name = f\"{run_name}_wmae{score:.4f}\"\n",
        "                mlflow.set_tag(\"final_run_name\", final_run_name)\n",
        "\n",
        "                param_summary = \" → \".join(f\"{k}={v}\" for k, v in {\n",
        "                    'input_size': params['input_size'],\n",
        "                    'patch_len': params['patch_len'],\n",
        "                    'hidden_size': params.get('hidden_size', 'default'),\n",
        "                    'n_heads': params.get('n_heads', 'default')\n",
        "                }.items())\n",
        "\n",
        "                print(f\"[{i:2d}/{total_combinations}] {param_summary} → WMAE={score:.4f}\")\n",
        "                results.append({'wmae': score, 'trial': i, **params})\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            print(f\"[{i:2d}/{total_combinations}] ERROR: {error_msg}\")\n",
        "\n",
        "            # Log failed runs too for debugging\n",
        "            with mlflow.start_run(nested=True, run_name=f\"FAILED_{run_name}\"):\n",
        "                mlflow.log_params({f\"param_{k}\": str(v) for k, v in params.items()})\n",
        "                mlflow.set_tag(\"stage\", \"hyperparameter_tuning\")\n",
        "                mlflow.set_tag(\"status\", \"failed\")\n",
        "                mlflow.log_param(\"error_message\", error_msg)\n",
        "            continue\n",
        "\n",
        "    if not results:\n",
        "        raise ValueError(\"No successful runs completed\")\n",
        "\n",
        "    # Find and log best result\n",
        "    best_result = min(results, key=lambda r: r['wmae'])\n",
        "    print(f\"\\nBest result from trial {best_result['trial']}: WMAE = {best_result['wmae']:.4f}\")\n",
        "\n",
        "    return best_result\n",
        "\n",
        "# Preprocess and prepare data\n",
        "df = preprocess(train)\n",
        "df_cv = prepare_data_for_cv(df, n_windows=1, h=53)\n",
        "\n",
        "print(\"Tuning PatchTST hyperparameters...\")\n",
        "\n",
        "# Updated parameter grid with correct PatchTST parameters\n",
        "param_grid = {\n",
        "    'input_size': [20, 40],\n",
        "    'patch_len': [8, 16],\n",
        "    'hidden_size': [64, 128],  # Changed from d_model to hidden_size\n",
        "    'n_heads': [4, 8],         # Number of attention heads\n",
        "    'dropout': [0.0, 0.1],\n",
        "    'learning_rate': [1e-3, 5e-4]\n",
        "}\n",
        "\n",
        "fixed_params = {\n",
        "    'h': 53,\n",
        "    'max_steps': 25 * 104,\n",
        "    'random_seed': 42,\n",
        "    'batch_size': 64\n",
        "}\n",
        "\n",
        "# Run hyperparameter tuning\n",
        "experiment_name = f\"PatchTST_Walmart_Tuning_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
        "best_result = run_patchtst_cv(df_cv, param_grid, fixed_params, experiment_name)\n",
        "print(f\"\\nBest hyperparameters: {best_result}\")\n",
        "\n",
        "# Train final model with enhanced naming\n",
        "print(\"\\nTraining final PatchTST model...\")\n",
        "final_run_name = f\"PatchTST_Final_Model_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "with mlflow.start_run(run_name=final_run_name):\n",
        "    # Remove trial-specific keys for final model\n",
        "    final_params = {k: v for k, v in best_result.items() if k not in ['wmae', 'trial']}\n",
        "\n",
        "    # Enhanced logging for final model\n",
        "    mlflow.log_params({\n",
        "        \"model_type\": \"PatchTST\",\n",
        "        \"stage\": \"final_training\",\n",
        "        **{f\"final_{k}\": str(v) for k, v in final_params.items()}\n",
        "    })\n",
        "\n",
        "    # Add comprehensive tags\n",
        "    mlflow.set_tag(\"stage\", \"final_model\")\n",
        "    mlflow.set_tag(\"model_family\", \"transformer\")\n",
        "    mlflow.set_tag(\"dataset\", \"walmart_sales\")\n",
        "    mlflow.set_tag(\"validation_method\", \"cross_validation\")\n",
        "    mlflow.set_tag(\"best_trial\", str(best_result.get('trial', 'unknown')))\n",
        "\n",
        "    final_model = PatchTST(**final_params)\n",
        "    nf_model = NeuralForecastModels(models=[final_model], model_names=['PatchTST'], freq='W-FRI', one_model=True)\n",
        "    nf_model.fit(df_cv)\n",
        "\n",
        "    # Cross-validation evaluation\n",
        "    final_cv_df = nf_model.cross_validation(df_cv, n_windows=1)\n",
        "    y_true = final_cv_df['y'].values\n",
        "    y_pred = final_cv_df['PatchTST'].values\n",
        "    is_holiday = None\n",
        "\n",
        "    if 'IsHoliday' in df.columns:\n",
        "        final_cv_df_with_holiday = final_cv_df.merge(df[['unique_id', 'ds', 'IsHoliday']], on=['unique_id', 'ds'], how='left')\n",
        "        is_holiday = final_cv_df_with_holiday['IsHoliday'].fillna(False).values\n",
        "\n",
        "    wmae_score = compute_wmae(y_true, y_pred, is_holiday)\n",
        "\n",
        "    # Enhanced metrics logging\n",
        "    mlflow.log_metrics({\n",
        "        \"final_wmae\": wmae_score,\n",
        "        \"val_wmae_final\": wmae_score,\n",
        "        \"horizon\": final_params['h'],\n",
        "        \"batch_size\": final_params['batch_size'],\n",
        "        \"training_series_count\": len(df_cv['unique_id'].unique()),\n",
        "        \"training_data_points\": len(df_cv),\n",
        "        \"cv_data_points\": len(y_true)\n",
        "    })\n",
        "\n",
        "    # Save and log model\n",
        "    model_file = f\"patchtst_final_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl\"\n",
        "    joblib.dump(nf_model, model_file)\n",
        "    mlflow.log_artifact(model_file)\n",
        "\n",
        "    print(f\"Final model WMAE: {wmae_score:.4f}\")\n",
        "    print(\"Model saved and logged to MLflow.\")\n",
        "\n",
        "    # W&B logging with enhanced naming\n",
        "    wandb_run_name = f\"PatchTST_Final_WMAE{wmae_score:.4f}_{datetime.now().strftime('%m%d_%H%M')}\"\n",
        "    wandb.init(\n",
        "        project=\"Walmart Recruiting - Store Sales Forecasting\",\n",
        "        name=wandb_run_name,\n",
        "        tags=[\"final_model\", \"patchtst\", \"transformer\", \"walmart_sales\"]\n",
        "    )\n",
        "    wandb.config.update(final_params)\n",
        "    wandb.log({'val_wmae': wmae_score, 'final_wmae': wmae_score})\n",
        "\n",
        "    artifact = wandb.Artifact(\n",
        "        name=f\"patchtst_final_model_{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
        "        type=\"model\",\n",
        "        description=f\"Final PatchTST model with WMAE: {wmae_score:.4f}\"\n",
        "    )\n",
        "    artifact.add_file(model_file)\n",
        "    wandb.log_artifact(artifact)\n",
        "    wandb.finish()\n",
        "\n",
        "    # Cleanup\n",
        "    if os.path.exists(model_file):\n",
        "        os.remove(model_file)\n",
        "\n",
        "    print(\"Model saved and logged to W&B.\")\n",
        "\n",
        "# --- INFERENCE AND SUBMISSION GENERATION ---\n",
        "print(\"\\nGenerating predictions for submission...\")\n",
        "\n",
        "# Preprocess test data to match training format\n",
        "def preprocess_test_data(test_df, train_df):\n",
        "    \"\"\"Preprocess test data to match training format\"\"\"\n",
        "    test_processed = test_df.copy()\n",
        "    test_processed['unique_id'] = test_processed['Store'].astype(str) + \"_\" + test_processed['Dept'].astype(str)\n",
        "    test_processed.rename(columns={'Date': 'ds'}, inplace=True)\n",
        "    test_processed['ds'] = pd.to_datetime(test_processed['ds'])\n",
        "\n",
        "    # Add dummy y column for prediction (required by neuralforecast)\n",
        "    test_processed['y'] = 0\n",
        "\n",
        "    # Get the last date from training data for each series\n",
        "    train_last_dates = train_df.groupby('unique_id')['ds'].max().reset_index()\n",
        "    train_last_dates.columns = ['unique_id', 'last_train_date']\n",
        "\n",
        "    # Merge to get context\n",
        "    test_with_context = test_processed.merge(train_last_dates, on='unique_id', how='left')\n",
        "\n",
        "    return test_processed.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
        "\n",
        "# Prepare full dataset for prediction (train + test structure)\n",
        "def prepare_full_dataset_for_prediction(train_df, test_df):\n",
        "    \"\"\"Combine train and test data for prediction\"\"\"\n",
        "    # Get unique series from test that exist in train\n",
        "    train_series = set(train_df['unique_id'].unique())\n",
        "    test_series = set(test_df['unique_id'].unique())\n",
        "    common_series = train_series.intersection(test_series)\n",
        "\n",
        "    print(f\"Series in train: {len(train_series)}\")\n",
        "    print(f\"Series in test: {len(test_series)}\")\n",
        "    print(f\"Common series: {len(common_series)}\")\n",
        "\n",
        "    # Filter both datasets to common series\n",
        "    train_filtered = train_df[train_df['unique_id'].isin(common_series)].copy()\n",
        "    test_processed_filtered = test_df[test_df['unique_id'].isin(common_series)].copy()\n",
        "\n",
        "    # Combine for prediction context\n",
        "    full_dataset = pd.concat([train_filtered, test_processed_filtered], ignore_index=True)\n",
        "    full_dataset = full_dataset.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
        "\n",
        "    return full_dataset\n",
        "\n",
        "# Process test data\n",
        "test_processed = preprocess_test_data(test, df)\n",
        "full_dataset = prepare_full_dataset_for_prediction(df, test_processed)\n",
        "\n",
        "print(f\"Full dataset shape: {full_dataset.shape}\")\n",
        "print(f\"Test data shape: {test_processed.shape}\")\n",
        "\n",
        "# Generate predictions using the trained model\n",
        "inference_run_name = f\"PatchTST_Inference_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "with mlflow.start_run(run_name=inference_run_name):\n",
        "    mlflow.log_params({\n",
        "        \"stage\": \"inference\",\n",
        "        \"model_type\": \"PatchTST\",\n",
        "        \"test_series_count\": len(test_filtered['unique_id'].unique()),\n",
        "        \"test_data_points\": len(test_filtered),\n",
        "        \"prediction_horizon\": final_params['h']\n",
        "    })\n",
        "\n",
        "    mlflow.set_tag(\"stage\", \"inference\")\n",
        "    mlflow.set_tag(\"model_family\", \"transformer\")\n",
        "    mlflow.set_tag(\"dataset\", \"walmart_sales\")\n",
        "\n",
        "    print(\"Generating predictions...\")\n",
        "\n",
        "    try:\n",
        "        # Make predictions\n",
        "        predictions_df = nf_model.predict(h=53)\n",
        "\n",
        "        # Process predictions to match submission format\n",
        "        def create_submission_file(predictions_df, test_df, output_file=\"submission.csv\"):\n",
        "            \"\"\"Create submission file in the required format\"\"\"\n",
        "\n",
        "            # Merge predictions with test data to get Store, Dept, Date information\n",
        "            # First, extract unique_id components back to Store and Dept\n",
        "            predictions_processed = predictions_df.copy()\n",
        "            predictions_processed[['Store', 'Dept']] = predictions_processed['unique_id'].str.split('_', expand=True)\n",
        "            predictions_processed['Store'] = predictions_processed['Store'].astype(int)\n",
        "            predictions_processed['Dept'] = predictions_processed['Dept'].astype(int)\n",
        "            predictions_processed.rename(columns={'ds': 'Date', 'PatchTST': 'Weekly_Sales'}, inplace=True)\n",
        "\n",
        "            # Merge with original test data to get the exact rows needed\n",
        "            test_for_merge = test.copy()\n",
        "            test_for_merge['Date'] = pd.to_datetime(test_for_merge['Date'])\n",
        "\n",
        "            submission = test_for_merge.merge(\n",
        "                predictions_processed[['Store', 'Dept', 'Date', 'Weekly_Sales']],\n",
        "                on=['Store', 'Dept', 'Date'],\n",
        "                how='left'\n",
        "            )\n",
        "\n",
        "            # Handle any missing predictions (fill with median or last known value)\n",
        "            if submission['Weekly_Sales'].isna().any():\n",
        "                print(f\"Warning: {submission['Weekly_Sales'].isna().sum()} missing predictions filled with median\")\n",
        "                median_sales = predictions_processed['Weekly_Sales'].median()\n",
        "                submission['Weekly_Sales'].fillna(median_sales, inplace=True)\n",
        "\n",
        "            # Ensure no negative predictions\n",
        "            submission['Weekly_Sales'] = submission['Weekly_Sales'].clip(lower=0)\n",
        "\n",
        "            # Create final submission with required columns\n",
        "            final_submission = submission[['Id', 'Weekly_Sales']].copy()\n",
        "            final_submission.to_csv(output_file, index=False)\n",
        "\n",
        "            print(f\"Submission file saved: {output_file}\")\n",
        "            print(f\"Submission shape: {final_submission.shape}\")\n",
        "            print(f\"Sample predictions:\")\n",
        "            print(final_submission.head(10))\n",
        "\n",
        "            # Log statistics\n",
        "            stats = {\n",
        "                \"prediction_count\": len(final_submission),\n",
        "                \"mean_prediction\": float(final_submission['Weekly_Sales'].mean()),\n",
        "                \"median_prediction\": float(final_submission['Weekly_Sales'].median()),\n",
        "                \"std_prediction\": float(final_submission['Weekly_Sales'].std()),\n",
        "                \"min_prediction\": float(final_submission['Weekly_Sales'].min()),\n",
        "                \"max_prediction\": float(final_submission['Weekly_Sales'].max()),\n",
        "                \"zero_predictions\": int((final_submission['Weekly_Sales'] == 0).sum())\n",
        "            }\n",
        "\n",
        "            return final_submission, stats\n",
        "\n",
        "        # Create submission file\n",
        "        submission_filename = f\"patchtst_submission_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "        submission_df, prediction_stats = create_submission_file(predictions_df, test, submission_filename)\n",
        "\n",
        "        # Log prediction statistics\n",
        "        mlflow.log_metrics(prediction_stats)\n",
        "        mlflow.log_artifact(submission_filename)\n",
        "\n",
        "        print(\"\\nPrediction Statistics:\")\n",
        "        for key, value in prediction_stats.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "\n",
        "        # Log to W&B\n",
        "        wandb_inference_name = f\"PatchTST_Inference_{datetime.now().strftime('%m%d_%H%M')}\"\n",
        "        wandb.init(\n",
        "            project=\"Walmart Recruiting - Store Sales Forecasting\",\n",
        "            name=wandb_inference_name,\n",
        "            tags=[\"inference\", \"patchtst\", \"submission\"]\n",
        "        )\n",
        "        wandb.log(prediction_stats)\n",
        "\n",
        "        # Create W&B artifact for submission\n",
        "        submission_artifact = wandb.Artifact(\n",
        "            name=f\"patchtst_submission_{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
        "            type=\"submission\",\n",
        "            description=f\"PatchTST submission with {prediction_stats['prediction_count']} predictions\"\n",
        "        )\n",
        "        submission_artifact.add_file(submission_filename)\n",
        "        wandb.log_artifact(submission_artifact)\n",
        "        wandb.finish()\n",
        "\n",
        "        print(f\"\\nSubmission file '{submission_filename}' created successfully!\")\n",
        "        print(\"File logged to both MLflow and W&B.\")\n",
        "\n",
        "        # Cleanup\n",
        "        if os.path.exists(submission_filename):\n",
        "            print(f\"Submission file ready for upload: {submission_filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error during inference: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        mlflow.log_param(\"inference_error\", error_msg)\n",
        "        mlflow.set_tag(\"status\", \"failed\")\n",
        "        raise\n",
        "\n",
        "print(\"\\nComplete pipeline finished! Hyperparameter tuning, training, and inference completed successfully!\")"
      ],
      "metadata": {
        "id": "NVYr9BTbN857",
        "outputId": "3c4ad89b-ecf9-4596-b817-f5b83ec85717",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "NVYr9BTbN857",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/08/03 06:27:41 INFO mlflow.tracking.fluent: Experiment with name 'PatchTST_Walmart_Tuning_20250803_0627' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original series: 3331, Valid series for CV: 2827\n",
            "Original data points: 421570, Filtered data points: 402458\n",
            "Tuning PatchTST hyperparameters...\n",
            "Starting hyperparameter tuning with 64 combinations...\n",
            "[ 1/64] input_size=20 → patch_len=8 → hidden_size=64 → n_heads=4 → WMAE=2936.6718\n",
            "[ 2/64] input_size=20 → patch_len=8 → hidden_size=64 → n_heads=4 → WMAE=2946.3911\n",
            "[ 3/64] input_size=20 → patch_len=8 → hidden_size=64 → n_heads=4 → WMAE=2950.8570\n",
            "[ 4/64] input_size=20 → patch_len=8 → hidden_size=64 → n_heads=4 → WMAE=2951.7497\n",
            "[ 5/64] input_size=20 → patch_len=8 → hidden_size=64 → n_heads=8 → WMAE=2928.5935\n",
            "[ 6/64] input_size=20 → patch_len=8 → hidden_size=64 → n_heads=8 → WMAE=2947.9398\n",
            "[ 7/64] input_size=20 → patch_len=8 → hidden_size=64 → n_heads=8 → WMAE=2952.4183\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}