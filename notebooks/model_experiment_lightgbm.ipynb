{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekvirika/WalmartRecruiting/blob/main/notebooks/model_experiment_lightgbm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d37ffccc",
      "metadata": {
        "id": "d37ffccc",
        "outputId": "389812f9-21e5-4a7c-ecb2-41b85b027ea5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a9a063c4",
      "metadata": {
        "id": "a9a063c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7cdf7fe-37eb-446a-ea17-c1fe743eb95e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.9/981.9 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m680.6/680.6 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m261.2/261.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install required packages\n",
        "!pip install -q wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn mlflow pytorch_lightning pytorch_forecasting\n",
        "\n",
        "# Set up Kaggle API\n",
        "!pip install -q kaggle  dagshub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "caaec57a",
      "metadata": {
        "id": "caaec57a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Upload your kaggle.json to Colab and run:\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1c101c9a",
      "metadata": {
        "id": "1c101c9a",
        "outputId": "db30f083-0122-443c-81e4-a972223dde1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 577MB/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Download the dataset\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9ea4e8ff",
      "metadata": {
        "id": "9ea4e8ff",
        "outputId": "15bebb05-3588-49a3-ff96-05eb337ad114",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open stores.csv.zip, stores.csv.zip.zip or stores.csv.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!unzip -q train.csv.zip\n",
        "!unzip -q stores.csv.zip\n",
        "!unzip -q test.csv.zip\n",
        "!unzip -q features.csv.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dagshub mlflow --quiet\n",
        "! pip install optuna"
      ],
      "metadata": {
        "id": "ki4OFcT8Namn",
        "outputId": "af269c1e-7cc7-4fec-c7a1-2bb935cdcfc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ki4OFcT8Namn",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.4)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.42)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.9.0 optuna-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "import lightgbm as lgb\n",
        "import mlflow\n",
        "import mlflow.lightgbm\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "U1iCKIG-NgG9"
      },
      "id": "U1iCKIG-NgG9",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from dagshub import dagshub_logger\n",
        "import os\n",
        "\n",
        "# Set tracking URI manually\n",
        "mlflow.set_tracking_uri(\"https://dagshub.com/ekvirika/WalmartRecruiting.mlflow\")\n",
        "\n",
        "# Use your DagsHub credentials\n",
        "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"ekvirika\"\n",
        "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"0adb1004ddd4221395353efea2d8ead625e26197\"\n",
        "\n",
        "# Optional: set registry if you're using model registry\n",
        "mlflow.set_registry_uri(\"https://dagshub.com/ekvirika/WalmartRecruiting.mlflow\")"
      ],
      "metadata": {
        "id": "meCDQ-eMNjwa"
      },
      "id": "meCDQ-eMNjwa",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import mlflow.sklearn\n",
        "# import mlflow.lightgbm\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from lightgbm import LGBMRegressor\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "# from sklearn.model_selection import RandomizedSearchCV\n",
        "# import matplotlib.pyplot as plt\n",
        "# import joblib\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # ----------------------\n",
        "# # Evaluation Metric (WMAE)\n",
        "# # ----------------------\n",
        "# def weighted_mae(y_true, y_pred, is_holiday):\n",
        "#     weights = np.where(is_holiday, 5, 1)\n",
        "#     return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "# # ----------------------\n",
        "# # Data Loader\n",
        "# # ----------------------\n",
        "# def load_data():\n",
        "#     with mlflow.start_run(run_name=\"LGBM_Data_Loading\", nested=True):\n",
        "#         train_df = pd.read_csv('train.csv')\n",
        "#         test_df = pd.read_csv('test.csv')\n",
        "#         stores_df = pd.read_csv('stores.csv')\n",
        "#         features_df = pd.read_csv('features.csv')\n",
        "\n",
        "#         df = train_df.merge(features_df, on=['Store', 'Date'], how='inner') \\\n",
        "#                      .merge(stores_df, on='Store', how='inner')\n",
        "\n",
        "#         mlflow.log_params({\n",
        "#             \"train_shape\": train_df.shape,\n",
        "#             \"test_shape\": test_df.shape,\n",
        "#             \"missing_values_train\": train_df.isnull().sum().sum(),\n",
        "#             \"missing_values_test\": test_df.isnull().sum().sum()\n",
        "#         })\n",
        "\n",
        "#         return df\n",
        "\n",
        "# # ----------------------\n",
        "# # Preprocessing\n",
        "# # ----------------------\n",
        "# def preprocess(df):\n",
        "#     with mlflow.start_run(run_name=\"LGBM_Cleaning\", nested=True):\n",
        "\n",
        "\n",
        "#       df = df.copy()\n",
        "\n",
        "#       # Fix column naming issues\n",
        "#       df[\"IsHoliday\"] = df.pop(\"IsHoliday_x\") if \"IsHoliday_x\" in df else df[\"IsHoliday\"]\n",
        "#       df.drop(columns=[\"IsHoliday_y\"], errors='ignore', inplace=True)\n",
        "\n",
        "#       # Filter and sort\n",
        "#       df = df[df[\"Weekly_Sales\"] > 0]\n",
        "#       df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "#       df = df.sort_values(by=\"Date\")\n",
        "\n",
        "#       # Remove outliers\n",
        "#       q_low = df[\"Weekly_Sales\"].quantile(0.01)\n",
        "#       q_high = df[\"Weekly_Sales\"].quantile(0.99)\n",
        "#       df = df[(df[\"Weekly_Sales\"] >= q_low) & (df[\"Weekly_Sales\"] <= q_high)]\n",
        "\n",
        "#       # Lag features\n",
        "#       for lag in [1, 2, 4, 52]:\n",
        "#           df[f\"lag_{lag}\"] = df.groupby(\"Store\")[\"Weekly_Sales\"].shift(lag)\n",
        "\n",
        "#       # Rolling statistics\n",
        "#       df[\"rolling_mean_4\"] = df.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(window=4).mean()\n",
        "#       df[\"rolling_std_4\"] = df.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(window=4).std()\n",
        "\n",
        "#       # Fill missing MarkDowns\n",
        "#       markdown_cols = [col for col in df.columns if \"MarkDown\" in col]\n",
        "#       df[markdown_cols] = df[markdown_cols].fillna(0)\n",
        "\n",
        "#       # Date features\n",
        "#       df[\"Year\"] = df[\"Date\"].dt.year\n",
        "#       df[\"Month\"] = df[\"Date\"].dt.month\n",
        "#       df[\"Week\"] = df[\"Date\"].dt.isocalendar().week\n",
        "#       df[\"DayOfWeek\"] = df[\"Date\"].dt.dayofweek\n",
        "#       df[\"IsMonthStart\"] = df[\"Date\"].dt.is_month_start.astype(int)\n",
        "#       df[\"IsMonthEnd\"] = df[\"Date\"].dt.is_month_end.astype(int)\n",
        "#       df[\"Quarter\"] = df[\"Date\"].dt.quarter\n",
        "#       df.drop(columns=[\"Date\"], inplace=True)\n",
        "\n",
        "#       mlflow.log_params({\"droppped_cols\": \"IsHoliday_x, IsHoliday_y, Date\", \"fill_markdown_NaNs\": \"0\"})\n",
        "\n",
        "#       # Drop rows with missing lag/rolling values\n",
        "#       df = df.dropna()\n",
        "\n",
        "#       return df\n",
        "\n",
        "# # ----------------------\n",
        "# # Dynamic Pipeline Builder\n",
        "# # ----------------------\n",
        "# def build_pipeline(X, model=None):\n",
        "#     with mlflow.start_run(run_name=\"LGBM_Feature_Engineering\", nested=True):\n",
        "\n",
        "#       numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "#       categorical_cols = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "\n",
        "#       preprocessor = ColumnTransformer(transformers=[\n",
        "#           (\"num\", SimpleImputer(strategy='mean'), numeric_cols),\n",
        "#           (\"cat\", OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)\n",
        "#       ])\n",
        "\n",
        "#       if model is None:\n",
        "#           model = LGBMRegressor(\n",
        "#               n_estimators=100,\n",
        "#               max_depth=6,\n",
        "#               learning_rate=0.1,\n",
        "#               random_state=42,\n",
        "#               n_jobs=-1\n",
        "#           )\n",
        "\n",
        "#       pipeline = Pipeline([\n",
        "#           (\"preprocessor\", preprocessor),\n",
        "#           (\"model\", model)\n",
        "#       ])\n",
        "\n",
        "#       mlflow.log_params({\"num_cols\": numeric_cols, \"cat_cols\": categorical_cols,\n",
        "#                         \"simple_imputer\": \"mean\", \"cat_encoder\": \"one_hot\", \"pipeline\": pipeline})\n",
        "\n",
        "#       return pipeline\n",
        "\n",
        "# # ----------------------\n",
        "# # Runner\n",
        "# # ----------------------\n",
        "# def run():\n",
        "#     # with mlflow.start_run(run_name=\"XGBoost_Training\"):\n",
        "\n",
        "#       df = load_data()\n",
        "#       df = preprocess(df)\n",
        "\n",
        "#       # Define features and target\n",
        "#       X = df.drop(columns=[\"Weekly_Sales\"])\n",
        "#       y = df[\"Weekly_Sales\"]\n",
        "\n",
        "#       # Split\n",
        "#       X_train, X_test, y_train, y_test = train_test_split(\n",
        "#           X, y, test_size=0.2, random_state=42\n",
        "#       )\n",
        "\n",
        "#       # Store IsHoliday for WMAE\n",
        "#       is_holiday_test = X_test[\"IsHoliday\"].astype(bool).values if \"IsHoliday\" in X_test.columns else np.zeros_like(y_test)\n",
        "\n",
        "#       # Train\n",
        "#       pipeline = build_pipeline(X_train)\n",
        "\n",
        "#       # Hyperparameter search space\n",
        "#       param_dist = {\n",
        "#           \"model__n_estimators\": [100, 200, 300, 500, 800, 1000],\n",
        "#           \"model__max_depth\": [3, 6, 9, -1],  # -1 means no limit\n",
        "#           \"model__learning_rate\": [0.01, 0.1],\n",
        "#           \"model__subsample\": [0.6, 0.8, 1.0],  # still valid\n",
        "#           \"model__colsample_bytree\": [0.6, 0.8, 1.0],  # also valid\n",
        "#           \"model__num_leaves\": [31, 63, 127, 255, 511]\n",
        "#       }\n",
        "\n",
        "\n",
        "#       search = RandomizedSearchCV(\n",
        "#           pipeline,\n",
        "#           param_distributions=param_dist,\n",
        "#           n_iter=20,\n",
        "#           cv=3,\n",
        "#           scoring=\"neg_mean_absolute_error\",  # you could also define custom scorer using WMAE if needed\n",
        "#           verbose=2,\n",
        "#           n_jobs=-1,\n",
        "#           random_state=42\n",
        "#       )\n",
        "\n",
        "#       search.fit(X_train, y_train)\n",
        "\n",
        "#       best_model = search.best_estimator_\n",
        "#       y_pred = best_model.predict(X_test)\n",
        "\n",
        "#       # WMAE\n",
        "#       wmae_score = weighted_mae(y_test.values, y_pred, is_holiday_test)\n",
        "#       print(f\"âœ… WMAE: {wmae_score:.2f}\")\n",
        "\n",
        "#       mlflow.log_params(search.best_params_)\n",
        "#       mlflow.log_metric(\"WMAE\", wmae_score)\n",
        "#       # mlflow.sklearn.log_model(best_model.named_steps[\"model\"], artifact_path=\"lgbm_model\")\n",
        "#       joblib.dump(best_model.named_steps[\"model\"], \"lgbm_model.pkl\")\n",
        "#       mlflow.log_artifact(\"lgbm_model.pkl\")\n",
        "\n",
        "#       joblib.dump(best_model, \"lgbm_pipeline.pkl\")\n",
        "#       mlflow.log_artifact(\"lgbm_pipeline.pkl\")\n",
        "\n",
        "\n",
        "#       # Save feature importance plot as artifact\n",
        "#       fig, ax = plt.subplots()\n",
        "#       lgb_model = best_model.named_steps[\"model\"]\n",
        "#       importances = lgb_model.feature_importances_\n",
        "#       ax.bar(range(len(importances)), importances)\n",
        "#       plt.savefig(\"feature_importance.png\")\n",
        "#       mlflow.log_artifact(\"feature_importance.png\")\n",
        "\n",
        "\n",
        "#       return best_model\n",
        "\n",
        "# # ----------------------\n",
        "# # Entry Point\n",
        "# # ----------------------\n",
        "# if __name__ == \"__main__\":\n",
        "#     try:\n",
        "#         with mlflow.start_run(run_name=\"LGBM_Training\"):\n",
        "#             model = run()\n",
        "#         print(\"âœ… Pipeline executed successfully!\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"âŒ Pipeline failed: {e}\")\n"
      ],
      "metadata": {
        "id": "kCFU6zraNr9N",
        "collapsed": true
      },
      "id": "kCFU6zraNr9N",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "stores_df = pd.read_csv('stores.csv')\n",
        "features_df = pd.read_csv('features.csv')"
      ],
      "metadata": {
        "id": "Jv3UkGSNiMZL"
      },
      "id": "Jv3UkGSNiMZL",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import mlflow.lightgbm\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "\n",
        "# ----------------------\n",
        "# Evaluation Metric (WMAE)\n",
        "# ----------------------\n",
        "def weighted_mae(y_true, y_pred, is_holiday):\n",
        "    weights = np.where(is_holiday, 5, 1)\n",
        "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "# ----------------------\n",
        "# Custom Transformers\n",
        "# ----------------------\n",
        "class DataMerger(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, features_df, stores_df):\n",
        "        self.features_df = features_df\n",
        "        self.stores_df = stores_df\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X = X.merge(self.features_df, on=['Store', 'Date'], how='inner')\n",
        "        X = X.merge(self.stores_df, on='Store', how='inner')\n",
        "        return X\n",
        "\n",
        "class DataCleaner(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, is_train=True):\n",
        "        self.q_low = None\n",
        "        self.q_high = None\n",
        "        self.is_train = is_train\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if self.is_train and 'Weekly_Sales' in X.columns:\n",
        "            self.q_low = X[\"Weekly_Sales\"].quantile(0.01)\n",
        "            self.q_high = X[\"Weekly_Sales\"].quantile(0.99)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        if \"IsHoliday_x\" in X.columns:\n",
        "            X[\"IsHoliday\"] = X.pop(\"IsHoliday_x\")\n",
        "        X.drop(columns=[\"IsHoliday_y\"], errors='ignore', inplace=True)\n",
        "\n",
        "        X[\"Date\"] = pd.to_datetime(X[\"Date\"])\n",
        "        X = X.sort_values(by=\"Date\")\n",
        "\n",
        "        if self.is_train and 'Weekly_Sales' in X.columns:\n",
        "            X = X[X[\"Weekly_Sales\"] > 0]\n",
        "            if self.q_low is not None and self.q_high is not None:\n",
        "                X = X[(X[\"Weekly_Sales\"] >= self.q_low) & (X[\"Weekly_Sales\"] <= self.q_high)]\n",
        "\n",
        "        return X\n",
        "\n",
        "\n",
        "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, is_train=True):\n",
        "        self.is_train = is_train\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        if self.is_train and 'Weekly_Sales' in X.columns:\n",
        "            for lag in [1, 2, 4, 52]:\n",
        "                X[f\"lag_{lag}\"] = X.groupby(\"Store\")[\"Weekly_Sales\"].shift(lag)\n",
        "            X[\"rolling_mean_4\"] = X.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(window=4).mean()\n",
        "            X[\"rolling_std_4\"] = X.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(window=4).std()\n",
        "\n",
        "        markdown_cols = [col for col in X.columns if \"MarkDown\" in col]\n",
        "        X[markdown_cols] = X[markdown_cols].fillna(0)\n",
        "\n",
        "        X[\"Year\"] = X[\"Date\"].dt.year\n",
        "        X[\"Month\"] = X[\"Date\"].dt.month\n",
        "        X[\"Week\"] = X[\"Date\"].dt.isocalendar().week\n",
        "        X[\"DayOfWeek\"] = X[\"Date\"].dt.dayofweek\n",
        "        X[\"IsMonthStart\"] = X[\"Date\"].dt.is_month_start.astype(int)\n",
        "        X[\"IsMonthEnd\"] = X[\"Date\"].dt.is_month_end.astype(int)\n",
        "        X[\"Quarter\"] = X[\"Date\"].dt.quarter\n",
        "\n",
        "        X.drop(columns=[\"Date\"], inplace=True)\n",
        "        return X\n",
        "\n",
        "\n",
        "class MissingValueHandler(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.fill_values = {}\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Store mean values for numerical columns\n",
        "        numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "        for col in numeric_cols:\n",
        "            if X[col].isnull().any():\n",
        "                self.fill_values[col] = X[col].mean()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Fill numerical columns with stored means\n",
        "        for col, fill_value in self.fill_values.items():\n",
        "            if col in X.columns:\n",
        "                X[col] = X[col].fillna(fill_value)\n",
        "\n",
        "        # Drop rows with any remaining missing values (from lag features)\n",
        "        X = X.dropna()\n",
        "\n",
        "        return X\n",
        "\n",
        "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.numeric_cols = []\n",
        "        self.categorical_cols = []\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "        self.categorical_cols = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # This transformer just passes through the data but stores column info\n",
        "        return X\n",
        "\n",
        "    def get_feature_names_out(self):\n",
        "        return self.numeric_cols, self.categorical_cols"
      ],
      "metadata": {
        "id": "XVJa-QxZf4w0"
      },
      "id": "XVJa-QxZf4w0",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.pipeline import Pipeline\n",
        "# from lightgbm import LGBMRegressor\n",
        "\n",
        "# full_pipeline = Pipeline([\n",
        "#     ('merger', DataMerger(features_df, stores_df)),\n",
        "#     ('cleaner', DataCleaner()),\n",
        "#     ('feature_engineer', FeatureEngineer()),\n",
        "#     ('missing_handler', MissingValueHandler()),\n",
        "#     ('feature_selector', FeatureSelector()),\n",
        "#     ('model', LGBMRegressor(\n",
        "#         n_estimators=800,\n",
        "#         max_depth=-1,\n",
        "#         learning_rate=0.1,\n",
        "#         random_state=42,\n",
        "#         n_jobs=-1,\n",
        "#         num_leaves=63\n",
        "#     ))\n",
        "# ])\n",
        "\n",
        "# Define preprocessing after FeatureSelector\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", \"passthrough\", []),  # to be filled after fit\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [])\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Full pipeline with preprocessing + model\n",
        "full_pipeline = Pipeline([\n",
        "    ('merger', DataMerger(features_df, stores_df)),\n",
        "    ('cleaner', DataCleaner(is_train=False)),\n",
        "    ('feature_engineer', FeatureEngineer(is_train=False)),\n",
        "    ('missing_handler', MissingValueHandler()),\n",
        "    ('feature_selector', FeatureSelector()),  # No transform here\n",
        "    ('preprocessor', preprocessor),          # Youâ€™ll configure this dynamically\n",
        "    ('model', LGBMRegressor(\n",
        "        n_estimators=800,\n",
        "        max_depth=-1,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        num_leaves=63\n",
        "    ))\n",
        "])\n"
      ],
      "metadata": {
        "id": "qTTlCDtrgvE8"
      },
      "id": "qTTlCDtrgvE8",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Training and Results\n",
        "# ----------------------\n",
        "def train_and_evaluate(full_pipeline, train_df, test_df, features_df, stores_df):\n",
        "    \"\"\"Complete training pipeline with results\"\"\"\n",
        "\n",
        "    with mlflow.start_run(run_name=\"Walmart_Pipeline_Training\"):\n",
        "\n",
        "        print(\"ğŸ”„ Starting pipeline training...\")\n",
        "\n",
        "        # Step 1: Process all training data first\n",
        "        temp_pipeline = Pipeline([\n",
        "            ('merger', DataMerger(features_df, stores_df)),\n",
        "            ('cleaner', DataCleaner()),\n",
        "            ('feature_engineer', FeatureEngineer()),\n",
        "            ('missing_handler', MissingValueHandler()),\n",
        "            ('feature_selector', FeatureSelector())\n",
        "        ])\n",
        "\n",
        "        # Get processed data\n",
        "        X_processed = temp_pipeline.fit_transform(train_df)\n",
        "\n",
        "        # Configure preprocessor columns\n",
        "        numeric_cols = X_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        categorical_cols = X_processed.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "\n",
        "        # Remove target from numeric columns if present\n",
        "        if 'Weekly_Sales' in numeric_cols:\n",
        "            numeric_cols.remove('Weekly_Sales')\n",
        "\n",
        "        # Update preprocessor\n",
        "        full_pipeline.named_steps['preprocessor'].transformers = [\n",
        "            (\"num\", \"passthrough\", numeric_cols),\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols)\n",
        "        ]\n",
        "\n",
        "        print(f\"ğŸ“ˆ Numeric columns: {len(numeric_cols)}\")\n",
        "        print(f\"ğŸ“Š Categorical columns: {len(categorical_cols)}\")\n",
        "\n",
        "        # Step 2: Separate features and target from processed data\n",
        "        X_features = X_processed.drop(columns=[\"Weekly_Sales\"]) if \"Weekly_Sales\" in X_processed.columns else X_processed\n",
        "        y = X_processed[\"Weekly_Sales\"] if \"Weekly_Sales\" in X_processed.columns else None\n",
        "\n",
        "        print(f\"ğŸ”„ Total processed samples: {len(X_features)}\")\n",
        "\n",
        "        # Step 3: Train-test split on processed data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_features, y, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        print(f\"ğŸ”„ Training set size: {len(X_train)}\")\n",
        "        print(f\"ğŸ”„ Test set size: {len(X_test)}\")\n",
        "\n",
        "        # Step 4: Create a simplified pipeline for final preprocessing + model\n",
        "        # Since we already did most preprocessing, we just need the final steps\n",
        "        final_pipeline = Pipeline([\n",
        "            ('preprocessor', full_pipeline.named_steps['preprocessor']),\n",
        "            ('model', full_pipeline.named_steps['model'])\n",
        "        ])\n",
        "\n",
        "        # Step 5: Fit the final pipeline\n",
        "        print(\"ğŸš€ Training final pipeline...\")\n",
        "        final_pipeline.fit(X_train, y_train)\n",
        "\n",
        "        # Step 6: Make predictions\n",
        "        print(\"ğŸ”® Making predictions...\")\n",
        "        y_pred_train = final_pipeline.predict(X_train)\n",
        "        y_pred_test = final_pipeline.predict(X_test)\n",
        "\n",
        "        # test = pd.read_csv('test.csv')\n",
        "\n",
        "        # test[\"Date\"] = pd.to_datetime(test[\"Date\"]).dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        # test[\"Id\"] = test[\"Store\"].astype(str) + \"_\" + test[\"Dept\"].astype(str) + \"_\" + test[\"Date\"]\n",
        "\n",
        "        # submission = test[[\"Id\"]].copy()\n",
        "        # submission[\"Weekly_Sales\"] = y_pred_test\n",
        "\n",
        "        # submission.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "\n",
        "        # Step 7: Get IsHoliday for WMAE calculation\n",
        "        is_holiday_test = X_test[\"IsHoliday\"].astype(bool).values if \"IsHoliday\" in X_test.columns else np.zeros_like(y_test, dtype=bool)\n",
        "\n",
        "        # Step 8: Calculate metrics\n",
        "        train_wmae = weighted_mae(y_train.values, y_pred_train, np.zeros_like(y_train, dtype=bool))\n",
        "        test_wmae = weighted_mae(y_test.values, y_pred_test, is_holiday_test)\n",
        "\n",
        "        train_mae = mean_absolute_error(y_train, y_pred_train)\n",
        "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
        "\n",
        "        # Step 9: Print results\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ğŸ“Š TRAINING RESULTS\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"ğŸ¯ Training WMAE: {train_wmae:.2f}\")\n",
        "        print(f\"ğŸ¯ Test WMAE: {test_wmae:.2f}\")\n",
        "        print(f\"ğŸ“ˆ Training MAE: {train_mae:.2f}\")\n",
        "        print(f\"ğŸ“ˆ Test MAE: {test_mae:.2f}\")\n",
        "        print(f\"ğŸ” Holiday samples in test: {is_holiday_test.sum()}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Step 10: Log to MLflow\n",
        "        mlflow.log_params({\n",
        "            \"model_type\": \"LGBMRegressor\",\n",
        "            \"n_estimators\": 800,\n",
        "            \"max_depth\": -1,\n",
        "            \"learning_rate\": 0.1,\n",
        "            \"num_leaves\": 63,\n",
        "            \"train_size\": len(X_train),\n",
        "            \"test_size\": len(X_test),\n",
        "            \"numeric_features\": len(numeric_cols),\n",
        "            \"categorical_features\": len(categorical_cols)\n",
        "        })\n",
        "\n",
        "        mlflow.log_metrics({\n",
        "            \"train_wmae\": train_wmae,\n",
        "            \"test_wmae\": test_wmae,\n",
        "            \"train_mae\": train_mae,\n",
        "            \"test_mae\": test_mae\n",
        "        })\n",
        "\n",
        "        # Step 11: Feature importance\n",
        "        lgb_model = final_pipeline.named_steps['model']\n",
        "        feature_names = [f\"feature_{i}\" for i in range(len(lgb_model.feature_importances_))]\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'importance': lgb_model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        print(f\"\\nğŸ” Top 10 Most Important Features:\")\n",
        "        print(\"-\" * 40)\n",
        "        for i, (_, row) in enumerate(importance_df.head(10).iterrows()):\n",
        "            print(f\"{i+1:2d}. {row['feature']:<20} {row['importance']:.4f}\")\n",
        "\n",
        "        # Step 12: Create complete pipeline for saving/prediction\n",
        "        complete_pipeline = Pipeline([\n",
        "            ('preprocessing', temp_pipeline),\n",
        "            ('final_model', final_pipeline)\n",
        "        ])\n",
        "\n",
        "        # Step 13: Save models\n",
        "        joblib.dump(complete_pipeline, \"walmart_complete_pipeline.pkl\")\n",
        "        mlflow.log_artifact(\"walmart_complete_pipeline.pkl\")\n",
        "\n",
        "        # mlflow.sklearn.log_model(\n",
        "        #     complete_pipeline,\n",
        "        #     artifact_path=\"complete_pipeline\",\n",
        "        #     registered_model_name=\"walmart_sales_pipeline\"\n",
        "        # )\n",
        "        # mlflow.sklearn.log_model(complete_pipeline, artifact_path=\"pipeline\")\n",
        "        joblib.dump(complete_pipeline, \"walmart_sales_pipeline\")\n",
        "\n",
        "        print(\"âœ… Training completed successfully!\")\n",
        "        print(f\"ğŸ¯ Final Test WMAE: {test_wmae:.2f}\")\n",
        "\n",
        "        return complete_pipeline, {\n",
        "            'train_wmae': train_wmae,\n",
        "            'test_wmae': test_wmae,\n",
        "            'train_mae': train_mae,\n",
        "            'test_mae': test_mae\n",
        "        }\n",
        "\n",
        "# def make_test_predictions(pipeline, test_df):\n",
        "#     \"\"\"Make predictions on test data\"\"\"\n",
        "#     print(\"ğŸ”® Making predictions on test data...\")\n",
        "\n",
        "#     predictions = pipeline.predict(test_df)\n",
        "\n",
        "#     submission_df = pd.DataFrame({\n",
        "#         'Id': test_df['Id'] if 'Id' in test_df.columns else range(len(predictions)),\n",
        "#         'Weekly_Sales': predictions\n",
        "#     })\n",
        "\n",
        "#     submission_df.to_csv(\"walmart_submission.csv\", index=False)\n",
        "\n",
        "#     print(f\"ğŸ“Š Generated {len(predictions)} predictions\")\n",
        "#     print(f\"ğŸ“Š Prediction range: {predictions.min():.2f} to {predictions.max():.2f}\")\n",
        "#     print(\"ğŸ’¾ Submission saved as 'walmart_submission.csv'\")\n",
        "\n",
        "#     return submission_df\n",
        "\n",
        "# def make_test_predictions(pipeline, test_df):\n",
        "#     \"\"\"Make predictions on test data\"\"\"\n",
        "#     print(\"ğŸ”® Making predictions on test data...\")\n",
        "\n",
        "#     # Load required files for merging and historical context\n",
        "#     stores_df = pd.read_csv(\"stores.csv\")\n",
        "#     features_df = pd.read_csv(\"features.csv\")\n",
        "#     train_df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "#     # Concatenate train and test for proper lag/rolling feature computation\n",
        "#     combined_df = pd.concat([train_df, test_df], sort=False)\n",
        "#     combined_df.sort_values([\"Store\", \"Dept\", \"Date\"], inplace=True)\n",
        "\n",
        "#     # Mark test rows\n",
        "#     combined_df[\"is_test\"] = combined_df[\"Weekly_Sales\"].isna()\n",
        "\n",
        "#     # Apply only preprocessing steps (must be same order as in training)\n",
        "#     temp_pipeline = Pipeline([\n",
        "#         ('merger', DataMerger(features_df, stores_df)),\n",
        "#         ('cleaner', DataCleaner()),\n",
        "#         ('feature_engineer', FeatureEngineer()),\n",
        "#         ('missing_handler', MissingValueHandler()),\n",
        "#         ('feature_selector', FeatureSelector())\n",
        "#     ])\n",
        "\n",
        "#     full_processed = temp_pipeline.fit_transform(combined_df)\n",
        "\n",
        "#     # Separate test data after transformation\n",
        "#     test_processed = full_processed[full_processed[\"is_test\"] == True].drop(columns=[\"is_test\", \"Weekly_Sales\"], errors='ignore')\n",
        "\n",
        "#     # Drop Id if needed\n",
        "#     ids = test_processed[\"Id\"] if \"Id\" in test_processed.columns else range(len(test_processed))\n",
        "#     if \"Id\" in test_processed.columns:\n",
        "#         test_processed = test_processed.drop(columns=[\"Id\"])\n",
        "\n",
        "#     # Get the final model step\n",
        "#     model_pipeline = pipeline.named_steps[\"final_model\"]\n",
        "\n",
        "#     # Make predictions\n",
        "#     predictions = model_pipeline.predict(test_processed)\n",
        "\n",
        "#     submission_df = pd.DataFrame({\n",
        "#         \"Id\": ids,\n",
        "#         \"Weekly_Sales\": predictions\n",
        "#     })\n",
        "\n",
        "#     submission_df.to_csv(\"walmart_submission.csv\", index=False)\n",
        "\n",
        "#     print(f\"ğŸ“Š Generated {len(predictions)} predictions\")\n",
        "#     print(f\"ğŸ“Š Prediction range: {predictions.min():.2f} to {predictions.max():.2f}\")\n",
        "#     print(\"ğŸ’¾ Submission saved as 'walmart_submission.csv'\")\n",
        "\n",
        "#     return submission_df\n",
        "\n",
        "\n",
        "# ----------------------\n",
        "# Main execution\n",
        "# ----------------------\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Load data\n",
        "        train_df = pd.read_csv('train.csv')\n",
        "        test_df = pd.read_csv('test.csv')\n",
        "        stores_df = pd.read_csv('stores.csv')\n",
        "        features_df = pd.read_csv('features.csv')\n",
        "\n",
        "        print(\"ğŸš€ Starting Walmart Sales Forecasting Pipeline\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Train the model\n",
        "        pipeline, results = train_and_evaluate(full_pipeline, train_df, test_df, features_df, stores_df)\n",
        "\n",
        "        # # Make test predictions\n",
        "        # print(\"\\n\" + \"=\"*50)\n",
        "        # print(\"ğŸ“ MAKING TEST PREDICTIONS\")\n",
        "        # print(\"=\"*50)\n",
        "\n",
        "        # submission_df = make_test_predictions(pipeline, test_df)\n",
        "\n",
        "        # Final summary\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ğŸ‰ PIPELINE EXECUTION COMPLETE!\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"âœ… Best Test WMAE: {results['test_wmae']:.2f}\")\n",
        "        # print(f\"âœ… Best Test MAE: {results['test_mae']:.2f}\")\n",
        "        print(f\"ğŸ’¾ Pipeline saved: walmart_complete_pipeline.pkl\")\n",
        "        # print(f\"ğŸ“ Submission file: walmart_submission.csv\")\n",
        "        print(\"ğŸ¯ Ready for Kaggle submission!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Pipeline failed: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "GNlvSVUmmpGi",
        "outputId": "cb2f3172-7d23-447f-f93c-1114d7fee755"
      },
      "id": "GNlvSVUmmpGi",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Starting Walmart Sales Forecasting Pipeline\n",
            "============================================================\n",
            "ğŸ”„ Starting pipeline training...\n",
            "ğŸ“ˆ Numeric columns: 25\n",
            "ğŸ“Š Categorical columns: 2\n",
            "ğŸ”„ Total processed samples: 413212\n",
            "ğŸ”„ Training set size: 330569\n",
            "ğŸ”„ Test set size: 82643\n",
            "ğŸš€ Training final pipeline...\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044242 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4072\n",
            "[LightGBM] [Info] Number of data points in the train set: 330569, number of used features: 29\n",
            "[LightGBM] [Info] Start training from score 14887.537073\n",
            "ğŸ”® Making predictions...\n",
            "ğŸƒ View run Walmart_Pipeline_Training at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/aee94eaffa0145a387a5641505143591\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "âŒ Pipeline failed: Length of values (82643) does not match length of index (115064)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Length of values (82643) does not match length of index (115064)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2555432535.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstores_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;31m# # Make test predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2555432535.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(full_pipeline, train_df, test_df, features_df, stores_df)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0msubmission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Weekly_Sales\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"submission.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4309\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4310\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4311\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4522\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4523\u001b[0m         \"\"\"\n\u001b[0;32m-> 4524\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4526\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5266\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5267\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5268\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \"\"\"\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    574\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of values (82643) does not match length of index (115064)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "stores_df = pd.read_csv('stores.csv')\n",
        "features_df = pd.read_csv('features.csv')"
      ],
      "metadata": {
        "id": "Noy3y_R4Do83"
      },
      "id": "Noy3y_R4Do83",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "import mlflow\n",
        "\n",
        "model_uri = \"runs:/376129df52d046e2a1253f88df49f4c9/mlflow-artifacts:/4ccb92214fac4ea3a39cf3ba55b4b875/376129df52d046e2a1253f88df49f4c9/artifacts/walmart_complete_pipeline.pkl\"  # e.g., runs:/1234abcd/xgb_model\n",
        "model = mlflow.sklearn.load_model(model_uri)\n",
        "\n",
        "\n",
        "# Load your trained model\n",
        "# model = joblib.load(\"model.pkl\")  # Or use the model directly if already in memory\n",
        "\n",
        "# Make predictions on test set\n",
        "preds = model.predict(test_df)\n",
        "\n",
        "# Construct the Id column â€” assumes you have these columns in your test set\n",
        "X_test_with_id = test_df.copy()\n",
        "X_test_with_id[\"Id\"] = X_test_with_id[\"Store\"].astype(str) + \"_\" + \\\n",
        "                       X_test_with_id[\"Dept\"].astype(str) + \"_\" + \\\n",
        "                       X_test_with_id[\"Date\"].astype(str)\n",
        "\n",
        "# Build submission DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    \"Id\": X_test_with_id[\"Id\"],\n",
        "    \"Weekly_Sales\": preds\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "submission.to_csv(\"submission.csv\", index=False)\n",
        "print(\"Submission file saved to 'submission.csv'\")\n"
      ],
      "metadata": {
        "id": "w2OUe5GPDWRo",
        "outputId": "9223d710-c613-47c4-e8bc-6ccc47b67af9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405,
          "referenced_widgets": [
            "e981efcdb5d149ad972b3d3ac15c0831",
            "a31cec29f8e0474bb0a395a1ffc813cb",
            "7d67b5cf247647c1a0fa1599917bebc5",
            "2d2fccb035d344a495d274d45517c9e0",
            "d66b0a14e7fb44dab3544e9c2acd50e4",
            "ba29c2bfb3bd4a2595fdc8025f6be1f4",
            "46fe49fc9a0d433595bd6178868c5aa3",
            "255491a32dec4290bf99f0fce67fa464",
            "51b51b258b6943c69b7578db90c8f428",
            "19061ffbf74240798d7d8a6d4cc3c667",
            "59711acdaa534cbc8ea751d8aa094b89"
          ]
        }
      },
      "id": "w2OUe5GPDWRo",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e981efcdb5d149ad972b3d3ac15c0831"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "MlflowException",
          "evalue": "Failed to download artifacts from path 'walmart_complete_pipeline.pkl', please ensure that the path is correct.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2269872691.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"runs:/376129df52d046e2a1253f88df49f4c9/mlflow-artifacts:/4ccb92214fac4ea3a39cf3ba55b4b875/376129df52d046e2a1253f88df49f4c9/artifacts/walmart_complete_pipeline.pkl\"\u001b[0m  \u001b[0;31m# e.g., runs:/1234abcd/xgb_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/sklearn/__init__.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_uri, dst_path)\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msk_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpandas_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m     \"\"\"\n\u001b[0;32m--> 652\u001b[0;31m     \u001b[0mlocal_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_download_artifact_from_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdst_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m     \u001b[0mflavor_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_flavor_configuration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflavor_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAVOR_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     \u001b[0m_add_code_from_conf_to_system_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflavor_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracking/artifact_utils.py\u001b[0m in \u001b[0;36m_download_artifact_from_uri\u001b[0;34m(artifact_uri, output_path, lineage_header_info, tracking_uri)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mlineage_header_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineage_header_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             )\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrepo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0martifact_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0martifact_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"m-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/store/artifact/runs_artifact_repo.py\u001b[0m in \u001b[0;36mdownload_artifacts\u001b[0;34m(self, artifact_path, dst_path)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_out_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmodel_out_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             raise MlflowException(\n\u001b[0m\u001b[1;32m    227\u001b[0m                 \u001b[0;34mf\"Failed to download artifacts from path {artifact_path!r}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0;34m\"please ensure that the path is correct.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMlflowException\u001b[0m: Failed to download artifacts from path 'walmart_complete_pipeline.pkl', please ensure that the path is correct."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('submission.csv')\n"
      ],
      "metadata": {
        "id": "H_BLVHAdYnZ3"
      },
      "id": "H_BLVHAdYnZ3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e981efcdb5d149ad972b3d3ac15c0831": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a31cec29f8e0474bb0a395a1ffc813cb",
              "IPY_MODEL_7d67b5cf247647c1a0fa1599917bebc5",
              "IPY_MODEL_2d2fccb035d344a495d274d45517c9e0"
            ],
            "layout": "IPY_MODEL_d66b0a14e7fb44dab3544e9c2acd50e4"
          }
        },
        "a31cec29f8e0474bb0a395a1ffc813cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba29c2bfb3bd4a2595fdc8025f6be1f4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_46fe49fc9a0d433595bd6178868c5aa3",
            "value": "Downloadingâ€‡artifacts:â€‡â€‡â€‡0%"
          }
        },
        "7d67b5cf247647c1a0fa1599917bebc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_255491a32dec4290bf99f0fce67fa464",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51b51b258b6943c69b7578db90c8f428",
            "value": 0
          }
        },
        "2d2fccb035d344a495d274d45517c9e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19061ffbf74240798d7d8a6d4cc3c667",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_59711acdaa534cbc8ea751d8aa094b89",
            "value": "â€‡0/1â€‡[04:08&lt;?,â€‡?it/s]"
          }
        },
        "d66b0a14e7fb44dab3544e9c2acd50e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba29c2bfb3bd4a2595fdc8025f6be1f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46fe49fc9a0d433595bd6178868c5aa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "255491a32dec4290bf99f0fce67fa464": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51b51b258b6943c69b7578db90c8f428": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19061ffbf74240798d7d8a6d4cc3c667": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59711acdaa534cbc8ea751d8aa094b89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}