{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37ffccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a063c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required packages\n",
    "!pip install wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn mlflow pytorch_lightning pytorch_forecasting\n",
    "\n",
    "# Set up Kaggle API\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaec57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Upload your kaggle.json to Colab and run:\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
    "! chmod 600 ~/.kaggle/kaggle.json\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c101c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the dataset\n",
    "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
    "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea4e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!unzip -q train.csv.zip\n",
    "!unzip -q stores.csv.zip\n",
    "!unzip -q test.csv.zip\n",
    "!unzip -q features.csv.zip\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee4f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walmart Store Sales Forecasting - LightGBM Model Experiment\n",
    "# ================================================================\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226838dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up MLflow\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"LightGBM_Training\")\n",
    "\n",
    "print(\"ბიბლიოთეკები წარმატებით ჩაიტვირთა!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78074b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =================================================================\n",
    "# 1. Data Loading and Initial Exploration\n",
    "# =================================================================\n",
    "\n",
    "def load_walmart_data():\n",
    "    \"\"\"Load Walmart competition data\"\"\"\n",
    "    try:\n",
    "        # Load the datasets\n",
    "        train_df = pd.read_csv('train.csv')\n",
    "        test_df = pd.read_csv('test.csv')\n",
    "        features_df = pd.read_csv('features.csv')\n",
    "        stores_df = pd.read_csv('stores.csv')\n",
    "        \n",
    "        print(f\"Train shape: {train_df.shape}\")\n",
    "        print(f\"Test shape: {test_df.shape}\")\n",
    "        print(f\"Features shape: {features_df.shape}\")\n",
    "        print(f\"Stores shape: {stores_df.shape}\")\n",
    "        \n",
    "        return train_df, test_df, features_df, stores_df\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please ensure Walmart competition data files are in the working directory\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# Load data\n",
    "train_df, test_df, features_df, stores_df = load_walmart_data()\n",
    "\n",
    "if train_df is not None:\n",
    "    # Display basic info about the data\n",
    "    print(\"\\n=== Train Data Info ===\")\n",
    "    print(train_df.info())\n",
    "    print(\"\\n=== Train Data Sample ===\")\n",
    "    print(train_df.head())\n",
    "    \n",
    "    print(\"\\n=== Target Variable Statistics ===\")\n",
    "    print(train_df['Weekly_Sales'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b670dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =================================================================\n",
    "# 2. Custom Preprocessing Pipeline\n",
    "# =================================================================\n",
    "\n",
    "class WalmartPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom preprocessor for Walmart data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.label_encoders = {}\n",
    "        self.scalers = {}\n",
    "        self.fitted = False\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the preprocessor\"\"\"\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Convert Date column\n",
    "        if 'Date' in X.columns:\n",
    "            X['Date'] = pd.to_datetime(X['Date'])\n",
    "        \n",
    "        # Fit label encoders for categorical variables\n",
    "        categorical_cols = ['Store', 'Dept', 'Type']\n",
    "        for col in categorical_cols:\n",
    "            if col in X.columns:\n",
    "                self.label_encoders[col] = LabelEncoder()\n",
    "                self.label_encoders[col].fit(X[col].astype(str))\n",
    "        \n",
    "        # Fit scalers for numerical variables\n",
    "        numerical_cols = ['Size', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
    "        for col in numerical_cols:\n",
    "            if col in X.columns:\n",
    "                self.scalers[col] = StandardScaler()\n",
    "                self.scalers[col].fit(X[[col]])\n",
    "        \n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform the data\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Preprocessor must be fitted before transform\")\n",
    "        \n",
    "        X = X.copy()\n",
    "        \n",
    "        # Convert Date and create time features\n",
    "        if 'Date' in X.columns:\n",
    "            X['Date'] = pd.to_datetime(X['Date'])\n",
    "            X['Year'] = X['Date'].dt.year\n",
    "            X['Month'] = X['Date'].dt.month\n",
    "            X['Week'] = X['Date'].dt.isocalendar().week\n",
    "            X['Day'] = X['Date'].dt.day\n",
    "            X['DayOfWeek'] = X['Date'].dt.dayofweek\n",
    "            X['IsWeekend'] = (X['DayOfWeek'] >= 5).astype(int)\n",
    "            X['Quarter'] = X['Date'].dt.quarter\n",
    "            \n",
    "            # Create cyclical features\n",
    "            X['Month_sin'] = np.sin(2 * np.pi * X['Month'] / 12)\n",
    "            X['Month_cos'] = np.cos(2 * np.pi * X['Month'] / 12)\n",
    "            X['Week_sin'] = np.sin(2 * np.pi * X['Week'] / 52)\n",
    "            X['Week_cos'] = np.cos(2 * np.pi * X['Week'] / 52)\n",
    "        \n",
    "        # Handle holidays\n",
    "        if 'IsHoliday' in X.columns:\n",
    "            X['IsHoliday'] = X['IsHoliday'].astype(int)\n",
    "        \n",
    "        # Apply label encoders\n",
    "        for col, encoder in self.label_encoders.items():\n",
    "            if col in X.columns:\n",
    "                X[col] = encoder.transform(X[col].astype(str))\n",
    "        \n",
    "        # Apply scalers\n",
    "        for col, scaler in self.scalers.items():\n",
    "            if col in X.columns:\n",
    "                X[col] = scaler.transform(X[[col]]).flatten()\n",
    "        \n",
    "        # Fill missing values\n",
    "        X = X.fillna(0)\n",
    "        \n",
    "        return X\n",
    "\n",
    "def create_features(train_df, test_df, features_df, stores_df):\n",
    "    \"\"\"Create comprehensive feature set\"\"\"\n",
    "    \n",
    "    # Merge all datasets\n",
    "    train_full = train_df.merge(features_df, on=['Store', 'Date'], how='left')\n",
    "    train_full = train_full.merge(stores_df, on='Store', how='left')\n",
    "    \n",
    "    test_full = test_df.merge(features_df, on=['Store', 'Date'], how='left')\n",
    "    test_full = test_full.merge(stores_df, on='Store', how='left')\n",
    "    \n",
    "    # Create lag features for train data\n",
    "    train_full = train_full.sort_values(['Store', 'Dept', 'Date'])\n",
    "    \n",
    "    # Create lag features (previous weeks sales)\n",
    "    for lag in [1, 2, 4, 8, 12]:\n",
    "        train_full[f'Sales_Lag_{lag}'] = train_full.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(lag)\n",
    "    \n",
    "    # Create rolling window features\n",
    "    for window in [4, 8, 12]:\n",
    "        train_full[f'Sales_RollingMean_{window}'] = train_full.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        train_full[f'Sales_RollingStd_{window}'] = train_full.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).std()\n",
    "        )\n",
    "    \n",
    "    # Create statistical features by Store-Dept\n",
    "    dept_stats = train_full.groupby(['Store', 'Dept'])['Weekly_Sales'].agg({\n",
    "        'mean': 'mean',\n",
    "        'std': 'std',\n",
    "        'min': 'min',\n",
    "        'max': 'max',\n",
    "        'median': 'median'\n",
    "    }).reset_index()\n",
    "    \n",
    "    dept_stats.columns = ['Store', 'Dept', 'Dept_Sales_Mean', 'Dept_Sales_Std', \n",
    "                         'Dept_Sales_Min', 'Dept_Sales_Max', 'Dept_Sales_Median']\n",
    "    \n",
    "    train_full = train_full.merge(dept_stats, on=['Store', 'Dept'], how='left')\n",
    "    test_full = test_full.merge(dept_stats, on=['Store', 'Dept'], how='left')\n",
    "    \n",
    "    return train_full, test_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39127b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =================================================================\n",
    "# 3. MLflow Data Preprocessing Run\n",
    "# =================================================================\n",
    "\n",
    "def run_data_preprocessing():\n",
    "    \"\"\"Run data preprocessing with MLflow tracking\"\"\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"LightGBM_Data_Preprocessing\"):\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"preprocessing_method\", \"custom_walmart_preprocessor\")\n",
    "        mlflow.log_param(\"feature_engineering\", True)\n",
    "        mlflow.log_param(\"lag_features\", [1, 2, 4, 8, 12])\n",
    "        mlflow.log_param(\"rolling_windows\", [4, 8, 12])\n",
    "        \n",
    "        # Create features\n",
    "        train_full, test_full = create_features(train_df, test_df, features_df, stores_df)\n",
    "        \n",
    "        # Log dataset info\n",
    "        mlflow.log_metric(\"train_samples\", len(train_full))\n",
    "        mlflow.log_metric(\"test_samples\", len(test_full))\n",
    "        mlflow.log_metric(\"total_features\", len(train_full.columns))\n",
    "        \n",
    "        # Save processed data\n",
    "        train_full.to_csv('train_processed.csv', index=False)\n",
    "        test_full.to_csv('test_processed.csv', index=False)\n",
    "        \n",
    "        print(\"მონაცემთა დამუშავება წარმატებით დასრულდა!\")\n",
    "        \n",
    "        return train_full, test_full\n",
    "\n",
    "# Run preprocessing\n",
    "train_processed, test_processed = run_data_preprocessing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e85f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =================================================================\n",
    "# 4. Feature Selection and Engineering\n",
    "# =================================================================\n",
    "\n",
    "def run_feature_selection(train_data):\n",
    "    \"\"\"Run feature selection with MLflow tracking\"\"\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"LightGBM_Feature_Selection\"):\n",
    "        \n",
    "        # Prepare features\n",
    "        feature_cols = [col for col in train_data.columns if col not in ['Weekly_Sales', 'Date']]\n",
    "        \n",
    "        # Handle missing values and prepare data\n",
    "        X = train_data[feature_cols].copy()\n",
    "        y = train_data['Weekly_Sales'].copy()\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        preprocessor = WalmartPreprocessor()\n",
    "        X_processed = preprocessor.fit_transform(X)\n",
    "        \n",
    "        # Feature importance using LightGBM\n",
    "        lgb_model = lgb.LGBMRegressor(\n",
    "            objective='regression',\n",
    "            metric='rmse',\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            verbose=-1\n",
    "        )\n",
    "        \n",
    "        lgb_model.fit(X_processed, y)\n",
    "        \n",
    "        # Get feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': lgb_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Select top features\n",
    "        top_features = feature_importance.head(20)['feature'].tolist()\n",
    "        \n",
    "        # Log parameters and metrics\n",
    "        mlflow.log_param(\"total_features\", len(feature_cols))\n",
    "        mlflow.log_param(\"selected_features\", len(top_features))\n",
    "        mlflow.log_param(\"feature_selection_method\", \"lightgbm_importance\")\n",
    "        \n",
    "        # Save feature importance plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.barplot(data=feature_importance.head(20), x='importance', y='feature')\n",
    "        plt.title('Top 20 Feature Importance (LightGBM)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "        mlflow.log_artifact('feature_importance.png')\n",
    "        plt.show()\n",
    "        \n",
    "        return top_features, preprocessor\n",
    "\n",
    "# Run feature selection\n",
    "selected_features, preprocessor = run_feature_selection(train_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd3b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =================================================================\n",
    "# 5. Model Training with Cross-Validation\n",
    "# =================================================================\n",
    "\n",
    "def weighted_mean_absolute_error(y_true, y_pred, weights):\n",
    "    \"\"\"Calculate weighted MAE (Walmart competition metric)\"\"\"\n",
    "    return np.average(np.abs(y_true - y_pred), weights=weights)\n",
    "\n",
    "def run_lightgbm_training(train_data, selected_features):\n",
    "    \"\"\"Run LightGBM training with cross-validation\"\"\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"LightGBM_CrossValidation\"):\n",
    "        \n",
    "        # Prepare data\n",
    "        X = train_data[selected_features].copy()\n",
    "        y = train_data['Weekly_Sales'].copy()\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        X_processed = preprocessor.transform(X)\n",
    "        \n",
    "        # Create weights for holiday weeks (as per Walmart competition)\n",
    "        weights = np.where(train_data['IsHoliday'] == 1, 5, 1)\n",
    "        \n",
    "        # Time series cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "        \n",
    "        # LightGBM parameters\n",
    "        lgb_params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1,\n",
    "            'random_state': 42\n",
    "        }\n",
    "        \n",
    "        # Log parameters\n",
    "        for param, value in lgb_params.items():\n",
    "            mlflow.log_param(f\"lgb_{param}\", value)\n",
    "        \n",
    "        # Cross-validation results\n",
    "        cv_scores = []\n",
    "        cv_wmae_scores = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_processed)):\n",
    "            \n",
    "            X_train_fold, X_val_fold = X_processed[train_idx], X_processed[val_idx]\n",
    "            y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            weights_train = weights[train_idx]\n",
    "            weights_val = weights[val_idx]\n",
    "            \n",
    "            # Train model\n",
    "            lgb_model = lgb.LGBMRegressor(**lgb_params, n_estimators=1000)\n",
    "            lgb_model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                eval_set=[(X_val_fold, y_val_fold)],\n",
    "                eval_metric='rmse',\n",
    "                early_stopping_rounds=50,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = lgb_model.predict(X_val_fold)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n",
    "            mae = mean_absolute_error(y_val_fold, y_pred)\n",
    "            wmae = weighted_mean_absolute_error(y_val_fold, y_pred, weights_val)\n",
    "            \n",
    "            cv_scores.append(rmse)\n",
    "            cv_wmae_scores.append(wmae)\n",
    "            \n",
    "            print(f\"Fold {fold + 1}: RMSE = {rmse:.4f}, MAE = {mae:.4f}, WMAE = {wmae:.4f}\")\n",
    "        \n",
    "        # Calculate mean CV scores\n",
    "        mean_cv_rmse = np.mean(cv_scores)\n",
    "        mean_cv_wmae = np.mean(cv_wmae_scores)\n",
    "        std_cv_rmse = np.std(cv_scores)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"cv_rmse_mean\", mean_cv_rmse)\n",
    "        mlflow.log_metric(\"cv_rmse_std\", std_cv_rmse)\n",
    "        mlflow.log_metric(\"cv_wmae_mean\", mean_cv_wmae)\n",
    "        mlflow.log_metric(\"n_folds\", len(cv_scores))\n",
    "        \n",
    "        print(f\"\\nCross-Validation Results:\")\n",
    "        print(f\"Mean RMSE: {mean_cv_rmse:.4f} (+/- {std_cv_rmse:.4f})\")\n",
    "        print(f\"Mean WMAE: {mean_cv_wmae:.4f}\")\n",
    "        \n",
    "        return mean_cv_rmse, mean_cv_wmae\n",
    "\n",
    "# Run cross-validation\n",
    "cv_rmse, cv_wmae = run_lightgbm_training(train_processed, selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a6285",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =================================================================\n",
    "# 6. Final Model Training and Pipeline Creation\n",
    "# =================================================================\n",
    "\n",
    "def create_final_pipeline(train_data, selected_features):\n",
    "    \"\"\"Create final LightGBM pipeline\"\"\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"LightGBM_Final_Model\"):\n",
    "        \n",
    "        # Prepare data\n",
    "        X = train_data[selected_features].copy()\n",
    "        y = train_data['Weekly_Sales'].copy()\n",
    "        \n",
    "        # Best parameters (you can tune these further)\n",
    "        best_params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1,\n",
    "            'random_state': 42,\n",
    "            'n_estimators': 1000\n",
    "        }\n",
    "        \n",
    "        # Create pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', lgb.LGBMRegressor(**best_params))\n",
    "        ])\n",
    "        \n",
    "        # Train pipeline\n",
    "        pipeline.fit(X, y)\n",
    "        \n",
    "        # Log model parameters\n",
    "        for param, value in best_params.items():\n",
    "            mlflow.log_param(f\"final_{param}\", value)\n",
    "        \n",
    "        # Log final metrics\n",
    "        y_pred = pipeline.predict(X)\n",
    "        final_rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "        final_mae = mean_absolute_error(y, y_pred)\n",
    "        \n",
    "        mlflow.log_metric(\"final_train_rmse\", final_rmse)\n",
    "        mlflow.log_metric(\"final_train_mae\", final_mae)\n",
    "        \n",
    "        # Log the model\n",
    "        mlflow.sklearn.log_model(\n",
    "            pipeline,\n",
    "            \"lightgbm_pipeline\",\n",
    "            registered_model_name=\"WalmartSales_LightGBM\"\n",
    "        )\n",
    "        \n",
    "        print(f\"Final Model Training Complete!\")\n",
    "        print(f\"Training RMSE: {final_rmse:.4f}\")\n",
    "        print(f\"Training MAE: {final_mae:.4f}\")\n",
    "        \n",
    "        return pipeline\n",
    "\n",
    "# Create final pipeline\n",
    "final_pipeline = create_final_pipeline(train_processed, selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed8e877",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =================================================================\n",
    "# 7. Model Evaluation and Validation\n",
    "# =================================================================\n",
    "\n",
    "def evaluate_model_performance(pipeline, train_data, selected_features):\n",
    "    \"\"\"Evaluate model performance with various metrics\"\"\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"LightGBM_Model_Evaluation\"):\n",
    "        \n",
    "        # Prepare data\n",
    "        X = train_data[selected_features].copy()\n",
    "        y = train_data['Weekly_Sales'].copy()\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = pipeline.predict(X)\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        mape = np.mean(np.abs((y - y_pred) / y)) * 100\n",
    "        \n",
    "        # Holiday vs non-holiday performance\n",
    "        holiday_mask = train_data['IsHoliday'] == 1\n",
    "        \n",
    "        if holiday_mask.sum() > 0:\n",
    "            holiday_rmse = np.sqrt(mean_squared_error(y[holiday_mask], y_pred[holiday_mask]))\n",
    "            non_holiday_rmse = np.sqrt(mean_squared_error(y[~holiday_mask], y_pred[~holiday_mask]))\n",
    "            \n",
    "            mlflow.log_metric(\"holiday_rmse\", holiday_rmse)\n",
    "            mlflow.log_metric(\"non_holiday_rmse\", non_holiday_rmse)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"evaluation_rmse\", rmse)\n",
    "        mlflow.log_metric(\"evaluation_mae\", mae)\n",
    "        mlflow.log_metric(\"evaluation_mape\", mape)\n",
    "        \n",
    "        # Create residuals plot\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(y, y_pred, alpha=0.5)\n",
    "        plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
    "        plt.xlabel('Actual Sales')\n",
    "        plt.ylabel('Predicted Sales')\n",
    "        plt.title('Actual vs Predicted Sales')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        residuals = y - y_pred\n",
    "        plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "        plt.axhline(y=0, color='r', linestyle='--')\n",
    "        plt.xlabel('Predicted Sales')\n",
    "        plt.ylabel('Residuals')\n",
    "        plt.title('Residuals Plot')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('model_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "        mlflow.log_artifact('model_evaluation.png')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Model Evaluation Complete!\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"MAPE: {mape:.2f}%\")\n",
    "        \n",
    "        return rmse, mae, mape\n",
    "\n",
    "# Evaluate model\n",
    "eval_rmse, eval_mae, eval_mape = evaluate_model_performance(final_pipeline, train_processed, selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0bbb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =================================================================\n",
    "# 8. Hyperparameter Tuning (Optional)\n",
    "# =================================================================\n",
    "\n",
    "def run_hyperparameter_tuning(train_data, selected_features, n_trials=20):\n",
    "    \"\"\"Run hyperparameter tuning for LightGBM\"\"\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"LightGBM_Hyperparameter_Tuning\"):\n",
    "        \n",
    "        # Prepare data\n",
    "        X = train_data[selected_features].copy()\n",
    "        y = train_data['Weekly_Sales'].copy()\n",
    "        X_processed = preprocessor.transform(X)\n",
    "        \n",
    "        # Define parameter grid\n",
    "        param_grid = {\n",
    "            'num_leaves': [20, 31, 40],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'feature_fraction': [0.8, 0.9, 1.0],\n",
    "            'bagging_fraction': [0.7, 0.8, 0.9],\n",
    "            'min_child_samples': [20, 30, 40]\n",
    "        }\n",
    "        \n",
    "        best_score = float('inf')\n",
    "        best_params = None\n",
    "        \n",
    "        # Simple grid search (you can use optuna for more advanced tuning)\n",
    "        from itertools import product\n",
    "        \n",
    "        param_combinations = list(product(\n",
    "            param_grid['num_leaves'],\n",
    "            param_grid['learning_rate'],\n",
    "            param_grid['feature_fraction'],\n",
    "            param_grid['bagging_fraction'],\n",
    "            param_grid['min_child_samples']\n",
    "        ))\n",
    "        \n",
    "        # Sample a subset for demonstration\n",
    "        np.random.seed(42)\n",
    "        sampled_combinations = np.random.choice(\n",
    "            len(param_combinations), \n",
    "            size=min(n_trials, len(param_combinations)), \n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        for i, idx in enumerate(sampled_combinations):\n",
    "            num_leaves, lr, feat_frac, bag_frac, min_child = param_combinations[idx]\n",
    "            \n",
    "            params = {\n",
    "                'objective': 'regression',\n",
    "                'metric': 'rmse',\n",
    "                'num_leaves': num_leaves,\n",
    "                'learning_rate': lr,\n",
    "                'feature_fraction': feat_frac,\n",
    "                'bagging_fraction': bag_frac,\n",
    "                'min_child_samples': min_child,\n",
    "                'verbose': -1,\n",
    "                'random_state': 42,\n",
    "                'n_estimators': 500\n",
    "            }\n",
    "            \n",
    "            # Cross-validation\n",
    "            tscv = TimeSeriesSplit(n_splits=3)\n",
    "            cv_scores = []\n",
    "            \n",
    "            for train_idx, val_idx in tscv.split(X_processed):\n",
    "                X_train_fold, X_val_fold = X_processed[train_idx], X_processed[val_idx]\n",
    "                y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "                \n",
    "                lgb_model = lgb.LGBMRegressor(**params)\n",
    "                lgb_model.fit(X_train_fold, y_train_fold)\n",
    "                y_pred = lgb_model.predict(X_val_fold)\n",
    "                \n",
    "                rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n",
    "                cv_scores.append(rmse)\n",
    "            \n",
    "            mean_cv_score = np.mean(cv_scores)\n",
    "            \n",
    "            if mean_cv_score < best_score:\n",
    "                best_score = mean_cv_score\n",
    "                best_params = params\n",
    "            \n",
    "            print(f\"Trial {i+1}/{len(sampled_combinations)}: CV RMSE = {mean_cv_score:.4f}\")\n",
    "        \n",
    "        # Log best parameters\n",
    "        mlflow.log_param(\"tuning_trials\", len(sampled_combinations))\n",
    "        mlflow.log_metric(\"best_cv_rmse\", best_score)\n",
    "        \n",
    "        for param, value in best_params.items():\n",
    "            mlflow.log_param(f\"best_{param}\", value)\n",
    "        \n",
    "        print(f\"\\nBest Parameters Found:\")\n",
    "        for param, value in best_params.items():\n",
    "            print(f\"{param}: {value}\")\n",
    "        print(f\"Best CV RMSE: {best_score:.4f}\")\n",
    "        \n",
    "        return best_params\n",
    "\n",
    "# Run hyperparameter tuning (optional - can be time-consuming)\n",
    "best_params = run_hyperparameter_tuning(train_processed, selected_features, n_trials=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43f3efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =================================================================\n",
    "# 9. Final Model Registration\n",
    "# =================================================================\n",
    "\n",
    "def register_best_model(pipeline):\n",
    "    \"\"\"Register the best model in MLflow Model Registry\"\"\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=\"LightGBM_Model_Registration\"):\n",
    "        \n",
    "        # Log final model info\n",
    "        mlflow.log_param(\"model_type\", \"LightGBM_Pipeline\")\n",
    "        mlflow.log_param(\"framework\", \"scikit-learn + LightGBM\")\n",
    "        mlflow.log_param(\"purpose\", \"Walmart Sales Forecasting\")\n",
    "        \n",
    "        # Register model\n",
    "        model_uri = f\"runs:/{mlflow.active_run().info.run_id}/lightgbm_pipeline\"\n",
    "        \n",
    "        # Register in Model Registry\n",
    "        mlflow.register_model(\n",
    "            model_uri=model_uri,\n",
    "            name=\"WalmartSales_LightGBM_Final\",\n",
    "            description=\"Final LightGBM model for Walmart Sales Forecasting competition\"\n",
    "        )\n",
    "        \n",
    "        print(\"Model successfully registered in MLflow Model Registry!\")\n",
    "        print(\"Model name: WalmartSales_LightGBM_Final\")\n",
    "\n",
    "# Register the model\n",
    "register_best_model(final_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c792648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =================================================================\n",
    "# 10. Summary and Next Steps\n",
    "# =================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LIGHTGBM MODEL EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"✓ Data preprocessing completed\")\n",
    "print(f\"✓ Feature engineering and selection completed\")\n",
    "print(f\"✓ Cross-validation RMSE: {cv_rmse:.4f}\")\n",
    "print(f\"✓ Final model evaluation RMSE: {eval_rmse:.4f}\")\n",
    "print(f\"✓ Final model evaluation MAE: {eval_mae:.4f}\")\n",
    "print(f\"✓ Final model evaluation MAPE: {eval_mape:.2f}%\")\n",
    "print(f\"✓ Model registered in MLflow Model Registry\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Run model_inference.ipynb to make predictions on test set\")\n",
    "print(\"2. Compare with other model architectures\")\n",
    "print(\"3. Submit predictions to Kaggle competition\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save the final pipeline for inference\n",
    "import joblib\n",
    "joblib.dump(final_pipeline, 'lightgbm_final_pipeline.pkl')\n",
    "print(\"Pipeline saved as 'lightgbm_final_pipeline.pkl'\")\n",
    "\n",
    "# Create a summary report\n",
    "summary_report = {\n",
    "    'Model': 'LightGBM',\n",
    "    'CV_RMSE': cv_rmse,\n",
    "    'CV_WMAE': cv_wmae,\n",
    "    'Final_RMSE': eval_rmse,\n",
    "    'Final_MAE': eval_mae,\n",
    "    'Final_MAPE': eval_mape,\n",
    "    'Selected_Features': len(selected_features),\n",
    "    'Model_Registered': True\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('lightgbm_summary.json', 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2)\n",
    "\n",
    "print(\"Summary report saved as 'lightgbm_summary.json'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
