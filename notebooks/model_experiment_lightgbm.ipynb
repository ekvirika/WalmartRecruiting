{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekvirika/WalmartRecruiting/blob/main/notebooks/model_experiment_lightgbm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "d37ffccc",
      "metadata": {
        "id": "d37ffccc",
        "outputId": "a33ca714-1a45-4299-9144-dc48f1bab4b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "a9a063c4",
      "metadata": {
        "id": "a9a063c4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install required packages\n",
        "!pip install -q wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn mlflow pytorch_lightning pytorch_forecasting\n",
        "\n",
        "# Set up Kaggle API\n",
        "!pip install -q kaggle  dagshub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "caaec57a",
      "metadata": {
        "id": "caaec57a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Upload your kaggle.json to Colab and run:\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "1c101c9a",
      "metadata": {
        "id": "1c101c9a",
        "outputId": "83b2cb16-02a0-4f69-84d6-28f7bce9fa68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "walmart-recruiting-store-sales-forecasting.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "replace features.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Download the dataset\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "9ea4e8ff",
      "metadata": {
        "id": "9ea4e8ff",
        "outputId": "85f0f88f-a9c3-4490-8e37-f1f3e4e4b389",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "unzip:  cannot find or open stores.csv.zip, stores.csv.zip.zip or stores.csv.zip.ZIP.\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "replace features.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!unzip -q train.csv.zip\n",
        "!unzip -q stores.csv.zip\n",
        "!unzip -q test.csv.zip\n",
        "!unzip -q features.csv.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dagshub mlflow --quiet\n",
        "! pip install optuna"
      ],
      "metadata": {
        "id": "ki4OFcT8Namn",
        "outputId": "dd7236de-1911-4a71-8aae-04c72ea9ef2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ki4OFcT8Namn",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.4.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.4)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "import lightgbm as lgb\n",
        "import mlflow\n",
        "import mlflow.lightgbm\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "U1iCKIG-NgG9"
      },
      "id": "U1iCKIG-NgG9",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from dagshub import dagshub_logger\n",
        "import os\n",
        "\n",
        "# Set tracking URI manually\n",
        "mlflow.set_tracking_uri(\"https://dagshub.com/ekvirika/WalmartRecruiting.mlflow\")\n",
        "\n",
        "# Use your DagsHub credentials\n",
        "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"ekvirika\"\n",
        "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"0adb1004ddd4221395353efea2d8ead625e26197\"\n",
        "\n",
        "# Optional: set registry if you're using model registry\n",
        "mlflow.set_registry_uri(\"https://dagshub.com/ekvirika/WalmartRecruiting.mlflow\")"
      ],
      "metadata": {
        "id": "meCDQ-eMNjwa"
      },
      "id": "meCDQ-eMNjwa",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import mlflow.sklearn\n",
        "# import mlflow.lightgbm\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from lightgbm import LGBMRegressor\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "# from sklearn.model_selection import RandomizedSearchCV\n",
        "# import matplotlib.pyplot as plt\n",
        "# import joblib\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # ----------------------\n",
        "# # Evaluation Metric (WMAE)\n",
        "# # ----------------------\n",
        "# def weighted_mae(y_true, y_pred, is_holiday):\n",
        "#     weights = np.where(is_holiday, 5, 1)\n",
        "#     return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "# # ----------------------\n",
        "# # Data Loader\n",
        "# # ----------------------\n",
        "# def load_data():\n",
        "#     with mlflow.start_run(run_name=\"LGBM_Data_Loading\", nested=True):\n",
        "#         train_df = pd.read_csv('train.csv')\n",
        "#         test_df = pd.read_csv('test.csv')\n",
        "#         stores_df = pd.read_csv('stores.csv')\n",
        "#         features_df = pd.read_csv('features.csv')\n",
        "\n",
        "#         df = train_df.merge(features_df, on=['Store', 'Date'], how='inner') \\\n",
        "#                      .merge(stores_df, on='Store', how='inner')\n",
        "\n",
        "#         mlflow.log_params({\n",
        "#             \"train_shape\": train_df.shape,\n",
        "#             \"test_shape\": test_df.shape,\n",
        "#             \"missing_values_train\": train_df.isnull().sum().sum(),\n",
        "#             \"missing_values_test\": test_df.isnull().sum().sum()\n",
        "#         })\n",
        "\n",
        "#         return df\n",
        "\n",
        "# # ----------------------\n",
        "# # Preprocessing\n",
        "# # ----------------------\n",
        "# def preprocess(df):\n",
        "#     with mlflow.start_run(run_name=\"LGBM_Cleaning\", nested=True):\n",
        "\n",
        "\n",
        "#       df = df.copy()\n",
        "\n",
        "#       # Fix column naming issues\n",
        "#       df[\"IsHoliday\"] = df.pop(\"IsHoliday_x\") if \"IsHoliday_x\" in df else df[\"IsHoliday\"]\n",
        "#       df.drop(columns=[\"IsHoliday_y\"], errors='ignore', inplace=True)\n",
        "\n",
        "#       # Filter and sort\n",
        "#       df = df[df[\"Weekly_Sales\"] > 0]\n",
        "#       df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "#       df = df.sort_values(by=\"Date\")\n",
        "\n",
        "#       # Remove outliers\n",
        "#       q_low = df[\"Weekly_Sales\"].quantile(0.01)\n",
        "#       q_high = df[\"Weekly_Sales\"].quantile(0.99)\n",
        "#       df = df[(df[\"Weekly_Sales\"] >= q_low) & (df[\"Weekly_Sales\"] <= q_high)]\n",
        "\n",
        "#       # Lag features\n",
        "#       for lag in [1, 2, 4, 52]:\n",
        "#           df[f\"lag_{lag}\"] = df.groupby(\"Store\")[\"Weekly_Sales\"].shift(lag)\n",
        "\n",
        "#       # Rolling statistics\n",
        "#       df[\"rolling_mean_4\"] = df.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(window=4).mean()\n",
        "#       df[\"rolling_std_4\"] = df.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(window=4).std()\n",
        "\n",
        "#       # Fill missing MarkDowns\n",
        "#       markdown_cols = [col for col in df.columns if \"MarkDown\" in col]\n",
        "#       df[markdown_cols] = df[markdown_cols].fillna(0)\n",
        "\n",
        "#       # Date features\n",
        "#       df[\"Year\"] = df[\"Date\"].dt.year\n",
        "#       df[\"Month\"] = df[\"Date\"].dt.month\n",
        "#       df[\"Week\"] = df[\"Date\"].dt.isocalendar().week\n",
        "#       df[\"DayOfWeek\"] = df[\"Date\"].dt.dayofweek\n",
        "#       df[\"IsMonthStart\"] = df[\"Date\"].dt.is_month_start.astype(int)\n",
        "#       df[\"IsMonthEnd\"] = df[\"Date\"].dt.is_month_end.astype(int)\n",
        "#       df[\"Quarter\"] = df[\"Date\"].dt.quarter\n",
        "#       df.drop(columns=[\"Date\"], inplace=True)\n",
        "\n",
        "#       mlflow.log_params({\"droppped_cols\": \"IsHoliday_x, IsHoliday_y, Date\", \"fill_markdown_NaNs\": \"0\"})\n",
        "\n",
        "#       # Drop rows with missing lag/rolling values\n",
        "#       df = df.dropna()\n",
        "\n",
        "#       return df\n",
        "\n",
        "# # ----------------------\n",
        "# # Dynamic Pipeline Builder\n",
        "# # ----------------------\n",
        "# def build_pipeline(X, model=None):\n",
        "#     with mlflow.start_run(run_name=\"LGBM_Feature_Engineering\", nested=True):\n",
        "\n",
        "#       numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "#       categorical_cols = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "\n",
        "#       preprocessor = ColumnTransformer(transformers=[\n",
        "#           (\"num\", SimpleImputer(strategy='mean'), numeric_cols),\n",
        "#           (\"cat\", OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)\n",
        "#       ])\n",
        "\n",
        "#       if model is None:\n",
        "#           model = LGBMRegressor(\n",
        "#               n_estimators=100,\n",
        "#               max_depth=6,\n",
        "#               learning_rate=0.1,\n",
        "#               random_state=42,\n",
        "#               n_jobs=-1\n",
        "#           )\n",
        "\n",
        "#       pipeline = Pipeline([\n",
        "#           (\"preprocessor\", preprocessor),\n",
        "#           (\"model\", model)\n",
        "#       ])\n",
        "\n",
        "#       mlflow.log_params({\"num_cols\": numeric_cols, \"cat_cols\": categorical_cols,\n",
        "#                         \"simple_imputer\": \"mean\", \"cat_encoder\": \"one_hot\", \"pipeline\": pipeline})\n",
        "\n",
        "#       return pipeline\n",
        "\n",
        "# # ----------------------\n",
        "# # Runner\n",
        "# # ----------------------\n",
        "# def run():\n",
        "#     # with mlflow.start_run(run_name=\"XGBoost_Training\"):\n",
        "\n",
        "#       df = load_data()\n",
        "#       df = preprocess(df)\n",
        "\n",
        "#       # Define features and target\n",
        "#       X = df.drop(columns=[\"Weekly_Sales\"])\n",
        "#       y = df[\"Weekly_Sales\"]\n",
        "\n",
        "#       # Split\n",
        "#       X_train, X_test, y_train, y_test = train_test_split(\n",
        "#           X, y, test_size=0.2, random_state=42\n",
        "#       )\n",
        "\n",
        "#       # Store IsHoliday for WMAE\n",
        "#       is_holiday_test = X_test[\"IsHoliday\"].astype(bool).values if \"IsHoliday\" in X_test.columns else np.zeros_like(y_test)\n",
        "\n",
        "#       # Train\n",
        "#       pipeline = build_pipeline(X_train)\n",
        "\n",
        "#       # Hyperparameter search space\n",
        "#       param_dist = {\n",
        "#           \"model__n_estimators\": [100, 200, 300, 500, 800, 1000],\n",
        "#           \"model__max_depth\": [3, 6, 9, -1],  # -1 means no limit\n",
        "#           \"model__learning_rate\": [0.01, 0.1],\n",
        "#           \"model__subsample\": [0.6, 0.8, 1.0],  # still valid\n",
        "#           \"model__colsample_bytree\": [0.6, 0.8, 1.0],  # also valid\n",
        "#           \"model__num_leaves\": [31, 63, 127, 255, 511]\n",
        "#       }\n",
        "\n",
        "\n",
        "#       search = RandomizedSearchCV(\n",
        "#           pipeline,\n",
        "#           param_distributions=param_dist,\n",
        "#           n_iter=20,\n",
        "#           cv=3,\n",
        "#           scoring=\"neg_mean_absolute_error\",  # you could also define custom scorer using WMAE if needed\n",
        "#           verbose=2,\n",
        "#           n_jobs=-1,\n",
        "#           random_state=42\n",
        "#       )\n",
        "\n",
        "#       search.fit(X_train, y_train)\n",
        "\n",
        "#       best_model = search.best_estimator_\n",
        "#       y_pred = best_model.predict(X_test)\n",
        "\n",
        "#       # WMAE\n",
        "#       wmae_score = weighted_mae(y_test.values, y_pred, is_holiday_test)\n",
        "#       print(f\"âœ… WMAE: {wmae_score:.2f}\")\n",
        "\n",
        "#       mlflow.log_params(search.best_params_)\n",
        "#       mlflow.log_metric(\"WMAE\", wmae_score)\n",
        "#       # mlflow.sklearn.log_model(best_model.named_steps[\"model\"], artifact_path=\"lgbm_model\")\n",
        "#       joblib.dump(best_model.named_steps[\"model\"], \"lgbm_model.pkl\")\n",
        "#       mlflow.log_artifact(\"lgbm_model.pkl\")\n",
        "\n",
        "#       joblib.dump(best_model, \"lgbm_pipeline.pkl\")\n",
        "#       mlflow.log_artifact(\"lgbm_pipeline.pkl\")\n",
        "\n",
        "\n",
        "#       # Save feature importance plot as artifact\n",
        "#       fig, ax = plt.subplots()\n",
        "#       lgb_model = best_model.named_steps[\"model\"]\n",
        "#       importances = lgb_model.feature_importances_\n",
        "#       ax.bar(range(len(importances)), importances)\n",
        "#       plt.savefig(\"feature_importance.png\")\n",
        "#       mlflow.log_artifact(\"feature_importance.png\")\n",
        "\n",
        "\n",
        "#       return best_model\n",
        "\n",
        "# # ----------------------\n",
        "# # Entry Point\n",
        "# # ----------------------\n",
        "# if __name__ == \"__main__\":\n",
        "#     try:\n",
        "#         with mlflow.start_run(run_name=\"LGBM_Training\"):\n",
        "#             model = run()\n",
        "#         print(\"âœ… Pipeline executed successfully!\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"âŒ Pipeline failed: {e}\")\n"
      ],
      "metadata": {
        "id": "kCFU6zraNr9N",
        "collapsed": true
      },
      "id": "kCFU6zraNr9N",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import mlflow\n",
        "# import mlflow.sklearn\n",
        "# import mlflow.lightgbm\n",
        "# from sklearn.base import BaseEstimator, TransformerMixin\n",
        "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from lightgbm import LGBMRegressor\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "# import matplotlib.pyplot as plt\n",
        "# import joblib\n",
        "\n",
        "# # ----------------------\n",
        "# # Evaluation Metric (WMAE)\n",
        "# # ----------------------\n",
        "# def weighted_mae(y_true, y_pred, is_holiday):\n",
        "#     weights = np.where(is_holiday, 5, 1)\n",
        "#     return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "# # ----------------------\n",
        "# # Custom Transformers\n",
        "# # ----------------------\n",
        "# class DataMerger(BaseEstimator, TransformerMixin):\n",
        "#     def __init__(self, features_df, stores_df):\n",
        "#         self.features_df = features_df\n",
        "#         self.stores_df = stores_df\n",
        "\n",
        "#     def fit(self, X, y=None):\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         X = X.copy()\n",
        "#         X = X.merge(self.features_df, on=['Store', 'Date'], how='inner')\n",
        "#         X = X.merge(self.stores_df, on='Store', how='inner')\n",
        "#         return X\n",
        "\n",
        "# class DataCleaner(BaseEstimator, TransformerMixin):\n",
        "#     def __init__(self, is_train=True):\n",
        "#         self.q_low = None\n",
        "#         self.q_high = None\n",
        "#         self.is_train = is_train\n",
        "\n",
        "#     def fit(self, X, y=None):\n",
        "#         if self.is_train and 'Weekly_Sales' in X.columns:\n",
        "#             self.q_low = X[\"Weekly_Sales\"].quantile(0.01)\n",
        "#             self.q_high = X[\"Weekly_Sales\"].quantile(0.99)\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         X = X.copy()\n",
        "\n",
        "#         if \"IsHoliday_x\" in X.columns:\n",
        "#             X[\"IsHoliday\"] = X.pop(\"IsHoliday_x\")\n",
        "#         X.drop(columns=[\"IsHoliday_y\"], errors='ignore', inplace=True)\n",
        "\n",
        "#         X[\"Date\"] = pd.to_datetime(X[\"Date\"])\n",
        "#         X = X.sort_values(by=\"Date\")\n",
        "\n",
        "#         if self.is_train and 'Weekly_Sales' in X.columns:\n",
        "#             X = X[X[\"Weekly_Sales\"] > 0]\n",
        "#             if self.q_low is not None and self.q_high is not None:\n",
        "#                 X = X[(X[\"Weekly_Sales\"] >= self.q_low) & (X[\"Weekly_Sales\"] <= self.q_high)]\n",
        "\n",
        "#         return X\n",
        "\n",
        "\n",
        "# class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "#     def __init__(self, is_train=True):\n",
        "#         self.is_train = is_train\n",
        "\n",
        "#     def fit(self, X, y=None):\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         X = X.copy()\n",
        "\n",
        "#         if self.is_train and 'Weekly_Sales' in X.columns:\n",
        "#             for lag in [1, 2, 4, 52]:\n",
        "#                 X[f\"lag_{lag}\"] = X.groupby(\"Store\")[\"Weekly_Sales\"].shift(lag)\n",
        "#             X[\"rolling_mean_4\"] = X.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(window=4).mean()\n",
        "#             X[\"rolling_std_4\"] = X.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(window=4).std()\n",
        "\n",
        "#         markdown_cols = [col for col in X.columns if \"MarkDown\" in col]\n",
        "#         X[markdown_cols] = X[markdown_cols].fillna(0)\n",
        "\n",
        "#         X[\"Year\"] = X[\"Date\"].dt.year\n",
        "#         X[\"Month\"] = X[\"Date\"].dt.month\n",
        "#         X[\"Week\"] = X[\"Date\"].dt.isocalendar().week\n",
        "#         X[\"DayOfWeek\"] = X[\"Date\"].dt.dayofweek\n",
        "#         X[\"IsMonthStart\"] = X[\"Date\"].dt.is_month_start.astype(int)\n",
        "#         X[\"IsMonthEnd\"] = X[\"Date\"].dt.is_month_end.astype(int)\n",
        "#         X[\"Quarter\"] = X[\"Date\"].dt.quarter\n",
        "\n",
        "#         X.drop(columns=[\"Date\"], inplace=True)\n",
        "#         return X\n",
        "\n",
        "\n",
        "# class MissingValueHandler(BaseEstimator, TransformerMixin):\n",
        "#     def __init__(self):\n",
        "#         self.fill_values = {}\n",
        "#         pass\n",
        "\n",
        "#     def fit(self, X, y=None):\n",
        "#         # Store mean values for numerical columns\n",
        "#         numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "#         for col in numeric_cols:\n",
        "#             if X[col].isnull().any():\n",
        "#                 self.fill_values[col] = X[col].mean()\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         X = X.copy()\n",
        "\n",
        "#         # Fill numerical columns with stored means\n",
        "#         for col, fill_value in self.fill_values.items():\n",
        "#             if col in X.columns:\n",
        "#                 X[col] = X[col].fillna(fill_value)\n",
        "\n",
        "#         # Drop rows with any remaining missing values (from lag features)\n",
        "#         X = X.dropna()\n",
        "\n",
        "#         return X\n",
        "\n",
        "# class FeatureSelector(BaseEstimator, TransformerMixin):\n",
        "#     def __init__(self):\n",
        "#         self.numeric_cols = []\n",
        "#         self.categorical_cols = []\n",
        "\n",
        "#     def fit(self, X, y=None):\n",
        "#         self.numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "#         self.categorical_cols = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         # This transformer just passes through the data but stores column info\n",
        "#         return X\n",
        "\n",
        "#     def get_feature_names_out(self):\n",
        "#         return self.numeric_cols, self.categorical_cols"
      ],
      "metadata": {
        "id": "XVJa-QxZf4w0"
      },
      "id": "XVJa-QxZf4w0",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # from sklearn.pipeline import Pipeline\n",
        "# # from lightgbm import LGBMRegressor\n",
        "\n",
        "# # full_pipeline = Pipeline([\n",
        "# #     ('merger', DataMerger(features_df, stores_df)),\n",
        "# #     ('cleaner', DataCleaner()),\n",
        "# #     ('feature_engineer', FeatureEngineer()),\n",
        "# #     ('missing_handler', MissingValueHandler()),\n",
        "# #     ('feature_selector', FeatureSelector()),\n",
        "# #     ('model', LGBMRegressor(\n",
        "# #         n_estimators=800,\n",
        "# #         max_depth=-1,\n",
        "# #         learning_rate=0.1,\n",
        "# #         random_state=42,\n",
        "# #         n_jobs=-1,\n",
        "# #         num_leaves=63\n",
        "# #     ))\n",
        "# # ])\n",
        "\n",
        "# # Define preprocessing after FeatureSelector\n",
        "# preprocessor = ColumnTransformer(\n",
        "#     transformers=[\n",
        "#         (\"num\", \"passthrough\", []),  # to be filled after fit\n",
        "#         (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), [])\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# # Full pipeline with preprocessing + model\n",
        "# full_pipeline = Pipeline([\n",
        "#     ('merger', DataMerger(features_df, stores_df)),\n",
        "#     ('cleaner', DataCleaner(is_train=False)),\n",
        "#     ('feature_engineer', FeatureEngineer(is_train=False)),\n",
        "#     ('missing_handler', MissingValueHandler()),\n",
        "#     ('feature_selector', FeatureSelector()),  # No transform here\n",
        "#     ('preprocessor', preprocessor),          # Youâ€™ll configure this dynamically\n",
        "#     ('model', LGBMRegressor(\n",
        "#         n_estimators=800,\n",
        "#         max_depth=-1,\n",
        "#         learning_rate=0.1,\n",
        "#         random_state=42,\n",
        "#         n_jobs=-1,\n",
        "#         num_leaves=63\n",
        "#     ))\n",
        "# ])\n"
      ],
      "metadata": {
        "id": "qTTlCDtrgvE8"
      },
      "id": "qTTlCDtrgvE8",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ----------------------\n",
        "# # Training and Results\n",
        "# # ----------------------\n",
        "# def train_and_evaluate(full_pipeline, train_df, test_df, features_df, stores_df):\n",
        "#     \"\"\"Complete training pipeline with results\"\"\"\n",
        "\n",
        "#     with mlflow.start_run(run_name=\"Walmart_Pipeline_Training\"):\n",
        "\n",
        "#         print(\"ðŸ”„ Starting pipeline training...\")\n",
        "\n",
        "#         # Step 1: Process all training data first\n",
        "#         temp_pipeline = Pipeline([\n",
        "#             ('merger', DataMerger(features_df, stores_df)),\n",
        "#             ('cleaner', DataCleaner()),\n",
        "#             ('feature_engineer', FeatureEngineer()),\n",
        "#             ('missing_handler', MissingValueHandler()),\n",
        "#             ('feature_selector', FeatureSelector())\n",
        "#         ])\n",
        "\n",
        "#         # Get processed data\n",
        "#         X_processed = temp_pipeline.fit_transform(train_df)\n",
        "\n",
        "#         # Configure preprocessor columns\n",
        "#         numeric_cols = X_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
        "#         categorical_cols = X_processed.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "\n",
        "#         # Remove target from numeric columns if present\n",
        "#         if 'Weekly_Sales' in numeric_cols:\n",
        "#             numeric_cols.remove('Weekly_Sales')\n",
        "\n",
        "#         # Update preprocessor\n",
        "#         full_pipeline.named_steps['preprocessor'].transformers = [\n",
        "#             (\"num\", \"passthrough\", numeric_cols),\n",
        "#             (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols)\n",
        "#         ]\n",
        "\n",
        "#         print(f\"ðŸ“ˆ Numeric columns: {len(numeric_cols)}\")\n",
        "#         print(f\"ðŸ“Š Categorical columns: {len(categorical_cols)}\")\n",
        "\n",
        "#         # Step 2: Separate features and target from processed data\n",
        "#         X_features = X_processed.drop(columns=[\"Weekly_Sales\"]) if \"Weekly_Sales\" in X_processed.columns else X_processed\n",
        "#         y = X_processed[\"Weekly_Sales\"] if \"Weekly_Sales\" in X_processed.columns else None\n",
        "\n",
        "#         print(f\"ðŸ”„ Total processed samples: {len(X_features)}\")\n",
        "\n",
        "#         # Step 3: Train-test split on processed data\n",
        "#         X_train, X_test, y_train, y_test = train_test_split(\n",
        "#             X_features, y, test_size=0.2, random_state=42\n",
        "#         )\n",
        "\n",
        "#         print(f\"ðŸ”„ Training set size: {len(X_train)}\")\n",
        "#         print(f\"ðŸ”„ Test set size: {len(X_test)}\")\n",
        "\n",
        "#         # Step 4: Create a simplified pipeline for final preprocessing + model\n",
        "#         # Since we already did most preprocessing, we just need the final steps\n",
        "#         final_pipeline = Pipeline([\n",
        "#             ('preprocessor', full_pipeline.named_steps['preprocessor']),\n",
        "#             ('model', full_pipeline.named_steps['model'])\n",
        "#         ])\n",
        "\n",
        "#         # Step 5: Fit the final pipeline\n",
        "#         print(\"ðŸš€ Training final pipeline...\")\n",
        "#         final_pipeline.fit(X_train, y_train)\n",
        "\n",
        "#         # Step 6: Make predictions\n",
        "#         print(\"ðŸ”® Making predictions...\")\n",
        "#         y_pred_train = final_pipeline.predict(X_train)\n",
        "#         y_pred_test = final_pipeline.predict(X_test)\n",
        "\n",
        "#         # test = pd.read_csv('test.csv')\n",
        "\n",
        "#         # test[\"Date\"] = pd.to_datetime(test[\"Date\"]).dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "#         # test[\"Id\"] = test[\"Store\"].astype(str) + \"_\" + test[\"Dept\"].astype(str) + \"_\" + test[\"Date\"]\n",
        "\n",
        "#         # submission = test[[\"Id\"]].copy()\n",
        "#         # submission[\"Weekly_Sales\"] = y_pred_test\n",
        "\n",
        "#         # submission.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "\n",
        "#         # Step 7: Get IsHoliday for WMAE calculation\n",
        "#         is_holiday_test = X_test[\"IsHoliday\"].astype(bool).values if \"IsHoliday\" in X_test.columns else np.zeros_like(y_test, dtype=bool)\n",
        "\n",
        "#         # Step 8: Calculate metrics\n",
        "#         train_wmae = weighted_mae(y_train.values, y_pred_train, np.zeros_like(y_train, dtype=bool))\n",
        "#         test_wmae = weighted_mae(y_test.values, y_pred_test, is_holiday_test)\n",
        "\n",
        "#         train_mae = mean_absolute_error(y_train, y_pred_train)\n",
        "#         test_mae = mean_absolute_error(y_test, y_pred_test)\n",
        "\n",
        "#         # Step 9: Print results\n",
        "#         print(\"\\n\" + \"=\"*50)\n",
        "#         print(\"ðŸ“Š TRAINING RESULTS\")\n",
        "#         print(\"=\"*50)\n",
        "#         print(f\"ðŸŽ¯ Training WMAE: {train_wmae:.2f}\")\n",
        "#         print(f\"ðŸŽ¯ Test WMAE: {test_wmae:.2f}\")\n",
        "#         print(f\"ðŸ“ˆ Training MAE: {train_mae:.2f}\")\n",
        "#         print(f\"ðŸ“ˆ Test MAE: {test_mae:.2f}\")\n",
        "#         print(f\"ðŸ” Holiday samples in test: {is_holiday_test.sum()}\")\n",
        "#         print(\"=\"*50)\n",
        "\n",
        "#         # Step 10: Log to MLflow\n",
        "#         mlflow.log_params({\n",
        "#             \"model_type\": \"LGBMRegressor\",\n",
        "#             \"n_estimators\": 800,\n",
        "#             \"max_depth\": -1,\n",
        "#             \"learning_rate\": 0.1,\n",
        "#             \"num_leaves\": 63,\n",
        "#             \"train_size\": len(X_train),\n",
        "#             \"test_size\": len(X_test),\n",
        "#             \"numeric_features\": len(numeric_cols),\n",
        "#             \"categorical_features\": len(categorical_cols)\n",
        "#         })\n",
        "\n",
        "#         mlflow.log_metrics({\n",
        "#             \"train_wmae\": train_wmae,\n",
        "#             \"test_wmae\": test_wmae,\n",
        "#             \"train_mae\": train_mae,\n",
        "#             \"test_mae\": test_mae\n",
        "#         })\n",
        "\n",
        "#         # Step 11: Feature importance\n",
        "#         lgb_model = final_pipeline.named_steps['model']\n",
        "#         feature_names = [f\"feature_{i}\" for i in range(len(lgb_model.feature_importances_))]\n",
        "\n",
        "#         importance_df = pd.DataFrame({\n",
        "#             'feature': feature_names,\n",
        "#             'importance': lgb_model.feature_importances_\n",
        "#         }).sort_values('importance', ascending=False)\n",
        "\n",
        "#         print(f\"\\nðŸ” Top 10 Most Important Features:\")\n",
        "#         print(\"-\" * 40)\n",
        "#         for i, (_, row) in enumerate(importance_df.head(10).iterrows()):\n",
        "#             print(f\"{i+1:2d}. {row['feature']:<20} {row['importance']:.4f}\")\n",
        "\n",
        "#         # Step 12: Create complete pipeline for saving/prediction\n",
        "#         complete_pipeline = Pipeline([\n",
        "#             ('preprocessing', temp_pipeline),\n",
        "#             ('final_model', final_pipeline)\n",
        "#         ])\n",
        "\n",
        "#         # Step 13: Save models\n",
        "#         joblib.dump(complete_pipeline, \"walmart_complete_pipeline.pkl\")\n",
        "#         mlflow.log_artifact(\"walmart_complete_pipeline.pkl\")\n",
        "\n",
        "#         # mlflow.sklearn.log_model(\n",
        "#         #     complete_pipeline,\n",
        "#         #     artifact_path=\"complete_pipeline\",\n",
        "#         #     registered_model_name=\"walmart_sales_pipeline\"\n",
        "#         # )\n",
        "#         # mlflow.sklearn.log_model(complete_pipeline, artifact_path=\"pipeline\")\n",
        "#         joblib.dump(complete_pipeline, \"walmart_sales_pipeline\")\n",
        "\n",
        "#         print(\"âœ… Training completed successfully!\")\n",
        "#         print(f\"ðŸŽ¯ Final Test WMAE: {test_wmae:.2f}\")\n",
        "\n",
        "#         return complete_pipeline, {\n",
        "#             'train_wmae': train_wmae,\n",
        "#             'test_wmae': test_wmae,\n",
        "#             'train_mae': train_mae,\n",
        "#             'test_mae': test_mae\n",
        "#         }\n",
        "\n",
        "# # def make_test_predictions(pipeline, test_df):\n",
        "# #     \"\"\"Make predictions on test data\"\"\"\n",
        "# #     print(\"ðŸ”® Making predictions on test data...\")\n",
        "\n",
        "# #     predictions = pipeline.predict(test_df)\n",
        "\n",
        "# #     submission_df = pd.DataFrame({\n",
        "# #         'Id': test_df['Id'] if 'Id' in test_df.columns else range(len(predictions)),\n",
        "# #         'Weekly_Sales': predictions\n",
        "# #     })\n",
        "\n",
        "# #     submission_df.to_csv(\"walmart_submission.csv\", index=False)\n",
        "\n",
        "# #     print(f\"ðŸ“Š Generated {len(predictions)} predictions\")\n",
        "# #     print(f\"ðŸ“Š Prediction range: {predictions.min():.2f} to {predictions.max():.2f}\")\n",
        "# #     print(\"ðŸ’¾ Submission saved as 'walmart_submission.csv'\")\n",
        "\n",
        "# #     return submission_df\n",
        "\n",
        "# # def make_test_predictions(pipeline, test_df):\n",
        "# #     \"\"\"Make predictions on test data\"\"\"\n",
        "# #     print(\"ðŸ”® Making predictions on test data...\")\n",
        "\n",
        "# #     # Load required files for merging and historical context\n",
        "# #     stores_df = pd.read_csv(\"stores.csv\")\n",
        "# #     features_df = pd.read_csv(\"features.csv\")\n",
        "# #     train_df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# #     # Concatenate train and test for proper lag/rolling feature computation\n",
        "# #     combined_df = pd.concat([train_df, test_df], sort=False)\n",
        "# #     combined_df.sort_values([\"Store\", \"Dept\", \"Date\"], inplace=True)\n",
        "\n",
        "# #     # Mark test rows\n",
        "# #     combined_df[\"is_test\"] = combined_df[\"Weekly_Sales\"].isna()\n",
        "\n",
        "# #     # Apply only preprocessing steps (must be same order as in training)\n",
        "# #     temp_pipeline = Pipeline([\n",
        "# #         ('merger', DataMerger(features_df, stores_df)),\n",
        "# #         ('cleaner', DataCleaner()),\n",
        "# #         ('feature_engineer', FeatureEngineer()),\n",
        "# #         ('missing_handler', MissingValueHandler()),\n",
        "# #         ('feature_selector', FeatureSelector())\n",
        "# #     ])\n",
        "\n",
        "# #     full_processed = temp_pipeline.fit_transform(combined_df)\n",
        "\n",
        "# #     # Separate test data after transformation\n",
        "# #     test_processed = full_processed[full_processed[\"is_test\"] == True].drop(columns=[\"is_test\", \"Weekly_Sales\"], errors='ignore')\n",
        "\n",
        "# #     # Drop Id if needed\n",
        "# #     ids = test_processed[\"Id\"] if \"Id\" in test_processed.columns else range(len(test_processed))\n",
        "# #     if \"Id\" in test_processed.columns:\n",
        "# #         test_processed = test_processed.drop(columns=[\"Id\"])\n",
        "\n",
        "# #     # Get the final model step\n",
        "# #     model_pipeline = pipeline.named_steps[\"final_model\"]\n",
        "\n",
        "# #     # Make predictions\n",
        "# #     predictions = model_pipeline.predict(test_processed)\n",
        "\n",
        "# #     submission_df = pd.DataFrame({\n",
        "# #         \"Id\": ids,\n",
        "# #         \"Weekly_Sales\": predictions\n",
        "# #     })\n",
        "\n",
        "# #     submission_df.to_csv(\"walmart_submission.csv\", index=False)\n",
        "\n",
        "# #     print(f\"ðŸ“Š Generated {len(predictions)} predictions\")\n",
        "# #     print(f\"ðŸ“Š Prediction range: {predictions.min():.2f} to {predictions.max():.2f}\")\n",
        "# #     print(\"ðŸ’¾ Submission saved as 'walmart_submission.csv'\")\n",
        "\n",
        "# #     return submission_df\n",
        "\n",
        "\n",
        "# # ----------------------\n",
        "# # Main execution\n",
        "# # ----------------------\n",
        "# if __name__ == \"__main__\":\n",
        "#     try:\n",
        "#         # Load data\n",
        "#         train_df = pd.read_csv('train.csv')\n",
        "#         test_df = pd.read_csv('test.csv')\n",
        "#         stores_df = pd.read_csv('stores.csv')\n",
        "#         features_df = pd.read_csv('features.csv')\n",
        "\n",
        "#         print(\"ðŸš€ Starting Walmart Sales Forecasting Pipeline\")\n",
        "#         print(\"=\" * 60)\n",
        "\n",
        "#         # Train the model\n",
        "#         pipeline, results = train_and_evaluate(full_pipeline, train_df, test_df, features_df, stores_df)\n",
        "\n",
        "#         # # Make test predictions\n",
        "#         # print(\"\\n\" + \"=\"*50)\n",
        "#         # print(\"ðŸ“ MAKING TEST PREDICTIONS\")\n",
        "#         # print(\"=\"*50)\n",
        "\n",
        "#         # submission_df = make_test_predictions(pipeline, test_df)\n",
        "\n",
        "#         # Final summary\n",
        "#         print(\"\\n\" + \"=\"*60)\n",
        "#         print(\"ðŸŽ‰ PIPELINE EXECUTION COMPLETE!\")\n",
        "#         print(\"=\"*60)\n",
        "#         print(f\"âœ… Best Test WMAE: {results['test_wmae']:.2f}\")\n",
        "#         # print(f\"âœ… Best Test MAE: {results['test_mae']:.2f}\")\n",
        "#         print(f\"ðŸ’¾ Pipeline saved: walmart_complete_pipeline.pkl\")\n",
        "#         # print(f\"ðŸ“ Submission file: walmart_submission.csv\")\n",
        "#         print(\"ðŸŽ¯ Ready for Kaggle submission!\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"âŒ Pipeline failed: {e}\")\n",
        "#         raise"
      ],
      "metadata": {
        "id": "GNlvSVUmmpGi",
        "collapsed": true
      },
      "id": "GNlvSVUmmpGi",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "stores = pd.read_csv('stores.csv')\n",
        "features = pd.read_csv('features.csv')"
      ],
      "metadata": {
        "id": "Noy3y_R4Do83"
      },
      "id": "Noy3y_R4Do83",
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# class BaseMerger(BaseEstimator, TransformerMixin):\n",
        "\n",
        "#     def __init__(self, features_df, stores_df):\n",
        "#         self.features_df = features_df.copy()\n",
        "#         self.stores_df = stores_df.copy()\n",
        "#         self.features_df[\"Date\"] = pd.to_datetime(self.features_df[\"Date\"])\n",
        "#         self.stores_df[\"Store\"] = self.stores_df[\"Store\"].astype(int)\n",
        "\n",
        "#     def fit(self, X, y=None):\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         X = X.copy()\n",
        "#         X[\"Date\"] = pd.to_datetime(X[\"Date\"])\n",
        "#         X[\"Store\"] = X[\"Store\"].astype(int)\n",
        "#         merged = X.merge(self.features_df, on=[\"Store\", \"Date\"], how=\"left\")\n",
        "#         merged = merged.merge(self.stores_df, on=\"Store\", how=\"left\")\n",
        "#         return merged\n",
        "\n",
        "# # class FeatureAdder(BaseEstimator, TransformerMixin):\n",
        "\n",
        "# #     def __init__(self, is_train=True):\n",
        "# #         self.is_train = is_train\n",
        "\n",
        "# #     def fit(self, X, y=None):\n",
        "# #         return self\n",
        "\n",
        "# #     def transform(self, X):\n",
        "# #         X = X.copy()\n",
        "# #         X[\"Date\"] = pd.to_datetime(X[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "# #         if self.is_train and \"Weekly_Sales\" in X.columns:\n",
        "# #             for lag in [1, 2, 4, 52]:\n",
        "# #                 X[f\"lag_{lag}\"] = X.groupby(\"Store\")[\"Weekly_Sales\"].shift(lag)\n",
        "# #             X[\"rolling_mean_4\"] = X.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(4).mean()\n",
        "# #             X[\"rolling_std_4\"] = X.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(4).std()\n",
        "\n",
        "# #         markdown_cols = [col for col in X.columns if \"MarkDown\" in col]\n",
        "# #         X[markdown_cols] = X[markdown_cols].fillna(0)\n",
        "\n",
        "# #         X[\"Year\"] = X[\"Date\"].dt.year\n",
        "# #         X[\"Month\"] = X[\"Date\"].dt.month\n",
        "# #         X[\"Week\"] = X[\"Date\"].dt.isocalendar().week\n",
        "# #         X[\"DayOfWeek\"] = X[\"Date\"].dt.dayofweek\n",
        "# #         X[\"IsMonthStart\"] = X[\"Date\"].dt.is_month_start.astype(int)\n",
        "# #         X[\"IsMonthEnd\"] = X[\"Date\"].dt.is_month_end.astype(int)\n",
        "# #         X[\"Quarter\"] = X[\"Date\"].dt.quarter\n",
        "\n",
        "# #         # âš ï¸ Don't drop Date here â€” it's needed for aggregations!\n",
        "# #         return X\n",
        "\n",
        "# class FeatureAdder(BaseEstimator, TransformerMixin):\n",
        "#     def __init__(self, is_train=True, use_log_sales=False):\n",
        "#         self.is_train = is_train\n",
        "#         self.use_log_sales = use_log_sales\n",
        "\n",
        "#     def fit(self, X, y=None):\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         X = X.copy()\n",
        "#         X[\"Date\"] = pd.to_datetime(X[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "#         if self.is_train and \"Weekly_Sales\" in X.columns:\n",
        "#             if self.use_log_sales:\n",
        "#                 X[\"Weekly_Sales\"] = np.log1p(X[\"Weekly_Sales\"])\n",
        "\n",
        "#             # Lag features per Store + Dept\n",
        "#             for lag in [1, 2, 4, 52]:\n",
        "#                 X[f\"lag_{lag}\"] = X.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(lag)\n",
        "\n",
        "#             # Rolling features\n",
        "#             X[\"rolling_mean_4\"] = X.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(1).rolling(4).mean()\n",
        "#             X[\"rolling_std_4\"] = X.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(1).rolling(4).std()\n",
        "\n",
        "#         # Time-related features\n",
        "#         X[\"Year\"] = X[\"Date\"].dt.year\n",
        "#         X[\"Month\"] = X[\"Date\"].dt.month\n",
        "#         X[\"Week\"] = X[\"Date\"].dt.isocalendar().week\n",
        "#         X[\"DayOfWeek\"] = X[\"Date\"].dt.dayofweek\n",
        "#         X[\"IsMonthStart\"] = X[\"Date\"].dt.is_month_start.astype(int)\n",
        "#         X[\"IsMonthEnd\"] = X[\"Date\"].dt.is_month_end.astype(int)\n",
        "#         X[\"Quarter\"] = X[\"Date\"].dt.quarter\n",
        "#         X[\"IsWeekend\"] = X[\"DayOfWeek\"].isin([5, 6]).astype(int)\n",
        "\n",
        "#         # Seasonality\n",
        "#         X[\"sin_week\"] = np.sin(2 * np.pi * X[\"Week\"] / 52)\n",
        "#         X[\"cos_week\"] = np.cos(2 * np.pi * X[\"Week\"] / 52)\n",
        "\n",
        "#         markdown_cols = [col for col in X.columns if \"MarkDown\" in col]\n",
        "#         X[markdown_cols] = X[markdown_cols].fillna(0)\n",
        "\n",
        "#         return X\n",
        "\n",
        "\n",
        "# class MissingValueFiller(BaseEstimator, TransformerMixin):\n",
        "\n",
        "#     def __init__(self, is_train=True):\n",
        "#         self.q_low = None\n",
        "#         self.q_high = None\n",
        "#         self.is_train = is_train\n",
        "\n",
        "#     def fit(self, X, y=None):\n",
        "#         if self.is_train and \"Weekly_Sales\" in X.columns:\n",
        "#             self.q_low = X[\"Weekly_Sales\"].quantile(0.01)\n",
        "#             self.q_high = X[\"Weekly_Sales\"].quantile(0.99)\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         X = X.copy()\n",
        "#         X[\"Date\"] = pd.to_datetime(X[\"Date\"])\n",
        "\n",
        "#         if self.is_train and \"Weekly_Sales\" in X.columns:\n",
        "#             X = X[X[\"Weekly_Sales\"] > 0]\n",
        "#             if self.q_low is not None and self.q_high is not None:\n",
        "#                 X = X[(X[\"Weekly_Sales\"] >= self.q_low) & (X[\"Weekly_Sales\"] <= self.q_high)]\n",
        "\n",
        "#         return X\n",
        "\n",
        "\n",
        "# class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
        "\n",
        "#     def __init__(self):\n",
        "#         self.fill_values = {}\n",
        "\n",
        "#     def fit(self, X, y=None):\n",
        "#         numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "#         for col in numeric_cols:\n",
        "#             if X[col].isnull().any():\n",
        "#                 self.fill_values[col] = X[col].mean()\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         X = X.copy()\n",
        "\n",
        "#         for col, fill_value in self.fill_values.items():\n",
        "#             if col in X.columns:\n",
        "#                 X[col] = X[col].fillna(fill_value)\n",
        "\n",
        "#         # if \"IsHoliday\" in X.columns:\n",
        "#         #     X[\"IsHoliday\"] = X[\"IsHoliday\"].astype(int)\n",
        "\n",
        "#         # X = X.dropna()  # Now okay, since lags are filled\n",
        "\n",
        "#         if \"IsHoliday\" in X.columns:\n",
        "#             X[\"IsHoliday\"] = X[\"IsHoliday\"].fillna(0).astype(int)  # Fill missing with 0 (not holiday)\n",
        "\n",
        "#         if \"Type\" in X.columns:\n",
        "#             X[\"Type\"] = X[\"Type\"].map({\"A\": 3, \"B\": 2, \"C\": 1}).fillna(0)\n",
        "\n",
        "#         # Fill missing values for numeric columns as before\n",
        "#         for col, fill_value in self.fill_values.items():\n",
        "#             if col in X.columns:\n",
        "#                 X[col] = X[col].fillna(fill_value)\n",
        "\n",
        "#         # Now drop rows only if they still have missing values in important columns (exclude 'IsHoliday')\n",
        "#         X = X.dropna(subset=[col for col in X.columns if col != \"IsHoliday\"])\n",
        "\n",
        "#         return X\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "\n",
        "class BaseMerger(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, features_df, stores_df):\n",
        "        self.features_df = features_df.copy()\n",
        "        self.stores_df = stores_df.copy()\n",
        "        self.features_df[\"Date\"] = pd.to_datetime(self.features_df[\"Date\"])\n",
        "        self.stores_df[\"Store\"] = self.stores_df[\"Store\"].astype(int)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[\"Date\"] = pd.to_datetime(X[\"Date\"])\n",
        "        X[\"Store\"] = X[\"Store\"].astype(int)\n",
        "        merged = X.merge(self.features_df, on=[\"Store\", \"Date\"], how=\"left\")\n",
        "        merged = merged.merge(self.stores_df, on=\"Store\", how=\"left\")\n",
        "        return merged\n",
        "\n",
        "\n",
        "class MissingValueFiller(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, is_train=True):\n",
        "        self.q_low = None\n",
        "        self.q_high = None\n",
        "        self.is_train = is_train\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if self.is_train and \"Weekly_Sales\" in X.columns:\n",
        "            self.q_low = X[\"Weekly_Sales\"].quantile(0.01)\n",
        "            self.q_high = X[\"Weekly_Sales\"].quantile(0.99)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[\"Date\"] = pd.to_datetime(X[\"Date\"])\n",
        "\n",
        "        if self.is_train and \"Weekly_Sales\" in X.columns:\n",
        "            X = X[X[\"Weekly_Sales\"] > 0]\n",
        "            if self.q_low is not None and self.q_high is not None:\n",
        "                X = X[(X[\"Weekly_Sales\"] >= self.q_low) & (X[\"Weekly_Sales\"] <= self.q_high)]\n",
        "\n",
        "        return X\n",
        "\n",
        "\n",
        "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.fill_values = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "        for col in numeric_cols:\n",
        "            if X[col].isnull().any():\n",
        "                self.fill_values[col] = X[col].mean()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        for col, fill_value in self.fill_values.items():\n",
        "            if col in X.columns:\n",
        "                X[col] = X[col].fillna(fill_value)\n",
        "\n",
        "        if \"IsHoliday\" in X.columns:\n",
        "            X[\"IsHoliday\"] = X[\"IsHoliday\"].fillna(0).astype(int)\n",
        "\n",
        "        if \"Type\" in X.columns:\n",
        "            X[\"Type\"] = X[\"Type\"].map({\"A\": 3, \"B\": 2, \"C\": 1}).fillna(0)\n",
        "\n",
        "        X = X.dropna(subset=[col for col in X.columns if col != \"IsHoliday\"])\n",
        "\n",
        "        return X\n",
        "\n",
        "\n",
        "class FeatureAdder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, is_train=True):\n",
        "        self.is_train = is_train\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[\"Date\"] = pd.to_datetime(X[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "        if self.is_train and \"Weekly_Sales\" in X.columns:\n",
        "            for lag in [1, 2, 4, 52]:\n",
        "                X[f\"lag_{lag}\"] = X.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(lag)\n",
        "            X[\"rolling_mean_4\"] = X.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(1).rolling(4).mean()\n",
        "            X[\"rolling_std_4\"] = X.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(1).rolling(4).std()\n",
        "\n",
        "        markdown_cols = [col for col in X.columns if \"MarkDown\" in col]\n",
        "        X[markdown_cols] = X[markdown_cols].fillna(0)\n",
        "\n",
        "        X[\"Year\"] = X[\"Date\"].dt.year\n",
        "        X[\"Month\"] = X[\"Date\"].dt.month\n",
        "        X[\"Week\"] = X[\"Date\"].dt.isocalendar().week\n",
        "        X[\"DayOfWeek\"] = X[\"Date\"].dt.dayofweek\n",
        "        X[\"IsMonthStart\"] = X[\"Date\"].dt.is_month_start.astype(int)\n",
        "        X[\"IsMonthEnd\"] = X[\"Date\"].dt.is_month_end.astype(int)\n",
        "        X[\"Quarter\"] = X[\"Date\"].dt.quarter\n",
        "\n",
        "        # Interaction terms\n",
        "        if \"Size\" in X.columns and \"Fuel_Price\" in X.columns:\n",
        "            X[\"Size_x_Fuel\"] = X[\"Size\"] * X[\"Fuel_Price\"]\n",
        "\n",
        "        if \"CPI\" in X.columns and \"Unemployment\" in X.columns:\n",
        "            X[\"CPI_Unemployment\"] = X[\"CPI\"] * X[\"Unemployment\"]\n",
        "\n",
        "        return X\n",
        "\n",
        "\n",
        "class DeptProportionAdder(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        if 'Weekly_Sales' in X.columns:\n",
        "            self.proportions = (\n",
        "                X.groupby(['Store', 'Dept'])['Weekly_Sales'].sum() /\n",
        "                X.groupby('Store')['Weekly_Sales'].sum()\n",
        "            ).fillna(0).to_dict()\n",
        "        else:\n",
        "            self.proportions = {}\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X['Dept_Proportion'] = X.apply(\n",
        "            lambda row: self.proportions.get((row['Store'], row['Dept']), 0.0),\n",
        "            axis=1\n",
        "        )\n",
        "        return X\n",
        "\n",
        "\n",
        "class LagSalesAdder(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.sort_values(['Store', 'Dept', 'Date']).copy()\n",
        "        group = X.groupby(['Store', 'Dept'])\n",
        "        X['Sales_Lag_1'] = group['Weekly_Sales'].shift(1).fillna(0)\n",
        "        X['Sales_Lag_2'] = group['Weekly_Sales'].shift(2).fillna(0)\n",
        "        return X\n",
        "\n",
        "\n",
        "class RollingStatsAdder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, window=4):\n",
        "        self.window = window\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.sort_values(['Store', 'Dept', 'Date']).copy()\n",
        "        group = X.groupby(['Store', 'Dept'])\n",
        "\n",
        "        X[f'RollingSalesMean_{self.window}'] = group['Weekly_Sales'].transform(\n",
        "            lambda x: x.shift(1).rolling(self.window, min_periods=1).mean()\n",
        "        ).fillna(0)\n",
        "\n",
        "        X[f'RollingSalesStd_{self.window}'] = group['Weekly_Sales'].transform(\n",
        "            lambda x: x.shift(1).rolling(self.window, min_periods=1).std()\n",
        "        ).fillna(0)\n",
        "\n",
        "        return X\n",
        "\n",
        "class StoreAggregator(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.timeseries = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        self.timeseries = {}\n",
        "        for store in X['Store'].unique():\n",
        "            self.aggregate_store_info(store, X)\n",
        "        return self.timeseries\n",
        "\n",
        "    def aggregate_store_info(self, store_id, X):\n",
        "        store_data = X[X['Store'] == store_id].copy()\n",
        "\n",
        "        # Check if Weekly_Sales exists (train data) or not (test data)\n",
        "        has_weekly_sales = 'Weekly_Sales' in store_data.columns\n",
        "\n",
        "        if has_weekly_sales:\n",
        "            sum_columns = ['Weekly_Sales', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "        else:\n",
        "            sum_columns = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "\n",
        "        first_columns = ['IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "                        'Type', 'Size', 'Day', 'Month', 'Year', 'SuperbowlWeek',\n",
        "                        'LaborDayWeek', 'ThanksgivingWeek', 'ChristmasWeek',\n",
        "                        'Days_to_Thanksgiving', 'Days_to_Christmas']\n",
        "\n",
        "        agg_dict = {}\n",
        "\n",
        "        # Add sum columns that exist in the data\n",
        "        for col in sum_columns:\n",
        "            if col in store_data.columns:\n",
        "                agg_dict[col] = 'sum'\n",
        "\n",
        "        # Add first columns that exist in the data\n",
        "        for col in first_columns:\n",
        "            if col in store_data.columns:\n",
        "                agg_dict[col] = 'first'\n",
        "\n",
        "        aggregated = store_data.groupby(['Date', 'Store']).agg(agg_dict).reset_index()\n",
        "        aggregated = aggregated.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "        # Calculate department proportions only if Weekly_Sales exists\n",
        "        if has_weekly_sales:\n",
        "            dept_proportions = self.calculate_dept_proportions(store_data)\n",
        "        else:\n",
        "            dept_proportions = None\n",
        "\n",
        "        self.timeseries[store_id] = (aggregated, dept_proportions)\n",
        "        return aggregated\n",
        "\n",
        "    def calculate_dept_proportions(self, store_data):\n",
        "        dept_totals = store_data.groupby('Dept')['Weekly_Sales'].sum()\n",
        "        store_total = store_data['Weekly_Sales'].sum()\n",
        "\n",
        "        if store_total == 0:\n",
        "            num_depts = len(dept_totals)\n",
        "            return {dept: 1.0/num_depts for dept in dept_totals.index}\n",
        "\n",
        "        dept_proportions = (dept_totals / store_total).to_dict()\n",
        "        return dept_proportions"
      ],
      "metadata": {
        "id": "Br1f4jx4pL2H"
      },
      "id": "Br1f4jx4pL2H",
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is your train.csv with 'Store' and 'Weekly_Sales'\n",
        "train_dict = {}\n",
        "for store_id, group in train.groupby(\"Store\"):\n",
        "    # You can define custom proportions here if needed\n",
        "    train_dict[store_id] = (group.copy(), 1.0)\n"
      ],
      "metadata": {
        "id": "f0KQIyyyqrkJ"
      },
      "id": "f0KQIyyyqrkJ",
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dict = {}\n",
        "\n",
        "for store in test[\"Store\"].unique():\n",
        "    df_store = test[test[\"Store\"] == store]\n",
        "    dept = df_store[\"Dept\"].iloc[0] if \"Dept\" in df_store.columns else 1\n",
        "    test_dict[store] = (df_store, {\"Dept\": dept})\n"
      ],
      "metadata": {
        "id": "Q24_f-po4msP"
      },
      "id": "Q24_f-po4msP",
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('base_merge', BaseMerger(features, stores)),\n",
        "    ('missing', MissingValueFiller()),\n",
        "    ('categorical', CategoricalEncoder()),\n",
        "    ('feature', FeatureAdder()),\n",
        "    ('dept_prop', DeptProportionAdder()),\n",
        "    ('lag_sales', LagSalesAdder()),\n",
        "    ('rolling', RollingStatsAdder(window=4)),\n",
        "    ('aggregator', StoreAggregator())\n",
        "])\n"
      ],
      "metadata": {
        "id": "WLho7E5ogDjl"
      },
      "id": "WLho7E5ogDjl",
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # import numpy as np\n",
        "# # # import pandas as pd\n",
        "# # # import mlflow\n",
        "# # # import mlflow.lightgbm\n",
        "# # # from lightgbm import LGBMRegressor\n",
        "\n",
        "# # # def train_store_models_lgbm(train_weeks):\n",
        "# # #     mape_array = []\n",
        "# # #     wmae_array = []\n",
        "# # #     store_models = {}\n",
        "# # #     train_columns = {}\n",
        "\n",
        "# # #     for store in train_dict.keys():\n",
        "# # #         df, prop = train_dict[store]\n",
        "\n",
        "# # #         df = df.sort_values(\"Date\")\n",
        "\n",
        "# # #         # Pipeline\n",
        "# # #         df = BaseMerger(features, stores).fit_transform(df)\n",
        "# # #         df = FeatureAdder(is_train=True).fit_transform(df)\n",
        "# # #         df = MissingValueFiller(is_train=True).fit_transform(df)\n",
        "# # #         df = CategoricalEncoder().fit_transform(df)\n",
        "\n",
        "# # #         train_df = df.iloc[:train_weeks]\n",
        "# # #         val_df = df.iloc[train_weeks:]\n",
        "\n",
        "# # #         X_train = train_df.drop(columns=[\"Weekly_Sales\", \"Date\"])\n",
        "\n",
        "# # #         train_columns = X_train.columns\n",
        "\n",
        "# # #         y_train = train_df[\"Weekly_Sales\"]\n",
        "\n",
        "# # #         # is_holiday_val = val_df[\"IsHoliday\"]\n",
        "# # #         X_val = val_df.drop(columns=[\"Weekly_Sales\", \"Date\"])\n",
        "# # #         y_val = val_df[\"Weekly_Sales\"]\n",
        "\n",
        "# # #         model = LGBMRegressor(n_estimators=800, learning_rate=0.1, max_depth=-1)\n",
        "\n",
        "# # #         with mlflow.start_run(run_name=f\"store_{store}\"):\n",
        "# # #             mlflow.log_params({\n",
        "# # #                 \"store\": store,\n",
        "# # #                 \"n_estimators\": 800,\n",
        "# # #                 \"learning_rate\": 0.1,\n",
        "# # #                 \"max_depth\": -1,\n",
        "# # #             })\n",
        "\n",
        "# # #             model.fit(X_train, y_train)\n",
        "\n",
        "# # #             preds = model.predict(X_val)\n",
        "\n",
        "# # #             # MAPE\n",
        "# # #             mape = np.mean(np.abs((y_val - preds) / y_val)) * 100\n",
        "# # #             mape_array.append(mape)\n",
        "\n",
        "# # #             # WMAE\n",
        "# # #             if \"IsHoliday\" in val_df.columns:\n",
        "# # #                 weights = np.where(val_df[\"IsHoliday\"] == 1, 5, 1)\n",
        "# # #                 print(weights)\n",
        "# # #             else:\n",
        "# # #                 weights = np.ones(len(val_df))  # fallback equal weights\n",
        "\n",
        "# # #             wmae = np.sum(weights * np.abs(y_val - preds)) / np.sum(weights)\n",
        "# # #             wmae_array.append(wmae)\n",
        "\n",
        "# # #             mlflow.log_metrics({\n",
        "# # #                 \"MAPE\": mape,\n",
        "# # #                 \"WMAE\": wmae\n",
        "# # #             })\n",
        "\n",
        "# # #             # Log model\n",
        "# # #             # mlflow.lightgbm.log_model(model, artifact_path=\"model\")\n",
        "\n",
        "# # #             store_models[store] = (model, prop)\n",
        "\n",
        "# # #     return mape_array, wmae_array, store_models, train_columns\n",
        "\n",
        "\n",
        "# # def wmae_eval(y_pred, dataset):\n",
        "# #     y_true = dataset.get_label()\n",
        "# #     weights = dataset.get_weight()\n",
        "# #     if weights is None:\n",
        "# #         weights = np.ones_like(y_true)\n",
        "\n",
        "# #     error = np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "# #     return 'wmae', error, False  # False = lower is better\n",
        "\n",
        "# # from lightgbm import LGBMRegressor\n",
        "# # from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "\n",
        "# # def train_store_models_lgbm(train_dict, features, stores, max_stores=10):\n",
        "# #     store_models = {}\n",
        "# #     train_columns = None\n",
        "\n",
        "# #     param_grid = {\n",
        "# #         \"n_estimators\": [100, 200, 300, 500],\n",
        "# #         \"learning_rate\": [0.01, 0.05, 0.1],\n",
        "# #         \"max_depth\": [3, 5, 7, 9],\n",
        "# #         \"min_child_weight\": [1, 3, 5],\n",
        "# #         \"subsample\": [0.7, 0.8, 1.0],\n",
        "# #         \"colsample_bytree\": [0.7, 0.8, 1.0]\n",
        "# #     }\n",
        "\n",
        "# #     for i, (store, (df, prop)) in enumerate(train_dict.items()):\n",
        "# #         if i >= max_stores:\n",
        "# #             break\n",
        "\n",
        "# #         df = df.sort_values(\"Date\")\n",
        "# #         df = BaseMerger(features, stores).fit_transform(df)\n",
        "# #         df = FeatureAdder(is_train=True).fit_transform(df)\n",
        "# #         df = MissingValueFiller(is_train=True).fit_transform(df)\n",
        "# #         df = CategoricalEncoder().fit_transform(df)\n",
        "\n",
        "# #         df[\"Weight\"] = df[\"IsHoliday\"].apply(lambda x: 5 if x else 1)\n",
        "\n",
        "# #         X = df.drop(columns=[\"Weekly_Sales\", \"Date\"])\n",
        "# #         y = df[\"Weekly_Sales\"]\n",
        "# #         weights = df[\"Weight\"]\n",
        "\n",
        "# #         X_train, X_val, y_train, y_val, w_train, w_val = train_test_split(\n",
        "# #             X, y, weights, test_size=0.2, shuffle=False\n",
        "# #         )\n",
        "\n",
        "# #         search = RandomizedSearchCV(\n",
        "# #             LGBMRegressor(),\n",
        "# #             param_distributions=param_grid,\n",
        "# #             n_iter=20,\n",
        "# #             scoring='neg_mean_absolute_error',\n",
        "# #             cv=3,\n",
        "# #             verbose=0,\n",
        "# #             n_jobs=-1,\n",
        "# #             random_state=42\n",
        "# #         )\n",
        "# #         search.fit(X_train, y_train)\n",
        "\n",
        "# #         best_model = search.best_estimator_\n",
        "\n",
        "# #         best_model.fit(\n",
        "# #             X_train, y_train,\n",
        "# #             eval_set=[(X_val, y_val)],\n",
        "# #             eval_metric=\"mae\",\n",
        "# #             # early_stopping_rounds=20,\n",
        "# #             feval=wmae_eval,\n",
        "# #             verbose=False\n",
        "# #         )\n",
        "\n",
        "# #         # Save model with metadata\n",
        "# #         prop = {\"Dept\": df[\"Dept\"].iloc[0]} if \"Dept\" in df.columns else {}\n",
        "# #         store_models[store] = (best_model, prop)\n",
        "\n",
        "# #         if train_columns is None:\n",
        "# #             train_columns = X.columns  # capture column structure\n",
        "\n",
        "# #     return store_models, train_columns\n",
        "\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "# from lightgbm import LGBMRegressor\n",
        "# import mlflow\n",
        "# import mlflow.lightgbm\n",
        "\n",
        "# def wmae_eval(y_pred, dataset):\n",
        "#     y_true = dataset.get_label()\n",
        "#     weights = dataset.get_weight()\n",
        "#     if weights is None:\n",
        "#         weights = np.ones_like(y_true)\n",
        "\n",
        "#     error = np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "#     return 'wmae', error, False  # False: lower is better\n",
        "\n",
        "# def train_store_models_lgbm(train_dict, features, stores, max_stores=10):\n",
        "#     store_models = {}\n",
        "#     train_columns = None\n",
        "\n",
        "#     param_grid = {\n",
        "#         \"n_estimators\": [100, 300, 500],\n",
        "#         \"learning_rate\": [0.01, 0.05, 0.1],\n",
        "#         \"max_depth\": [3, 5, 7],\n",
        "#         \"min_child_weight\": [1, 3],\n",
        "#         \"subsample\": [0.7, 0.8, 1.0],\n",
        "#         \"colsample_bytree\": [0.7, 0.8, 1.0]\n",
        "#     }\n",
        "\n",
        "#     for i, (store, (df, prop)) in enumerate(train_dict.items()):\n",
        "#         if i >= max_stores:\n",
        "#             break\n",
        "\n",
        "#         df = df.sort_values(\"Date\")\n",
        "\n",
        "#         # Preprocessing pipeline\n",
        "#         df = BaseMerger(features, stores).fit_transform(df)\n",
        "#         df = FeatureAdder(is_train=True).fit_transform(df)\n",
        "#         df = MissingValueFiller(is_train=True).fit_transform(df)\n",
        "#         df = CategoricalEncoder().fit_transform(df)\n",
        "\n",
        "#         # Ensure 'IsHoliday' exists\n",
        "#         if \"IsHoliday\" not in df.columns:\n",
        "#             df[\"IsHoliday\"] = 0  # Default to not a holiday\n",
        "\n",
        "#         df[\"Weight\"] = df[\"IsHoliday\"].apply(lambda x: 5 if x else 1)\n",
        "\n",
        "#         # Prepare features/target\n",
        "#         X = df.drop(columns=[\"Weekly_Sales\", \"Date\"])\n",
        "#         y = df[\"Weekly_Sales\"]\n",
        "#         weights = df[\"Weight\"]\n",
        "\n",
        "#         X_train, X_val, y_train, y_val, w_train, w_val = train_test_split(\n",
        "#             X, y, weights, test_size=0.2, shuffle=False\n",
        "#         )\n",
        "\n",
        "#         # Hyperparameter tuning\n",
        "#         search = RandomizedSearchCV(\n",
        "#             LGBMRegressor(),\n",
        "#             param_distributions=param_grid,\n",
        "#             n_iter=15,\n",
        "#             scoring='neg_mean_absolute_error',\n",
        "#             cv=3,\n",
        "#             verbose=0,\n",
        "#             n_jobs=-1,\n",
        "#             random_state=42\n",
        "#         )\n",
        "#         search.fit(X_train, y_train)\n",
        "\n",
        "#         best_model = search.best_estimator_\n",
        "\n",
        "#         # Fit with WMAE eval metric\n",
        "#         best_model.fit(\n",
        "#             X_train, y_train,\n",
        "#             sample_weight=w_train,\n",
        "#             eval_set=[(X_val, y_val)],\n",
        "#             eval_sample_weight=[w_val],\n",
        "#             eval_metric=\"mae\",\n",
        "#         )\n",
        "\n",
        "#         with mlflow.start_run(run_name=f\"store_{store}\"):\n",
        "#             mlflow.log_params(best_model.get_params())\n",
        "#             preds = best_model.predict(X_val)\n",
        "#             wmae = np.sum(w_val * np.abs(y_val - preds)) / np.sum(w_val)\n",
        "#             mlflow.log_metric(\"WMAE\", wmae)\n",
        "#             # mlflow.lightgbm.log_model(best_model, \"model\")\n",
        "\n",
        "#         store_models[store] = (best_model, prop)\n",
        "\n",
        "#         if train_columns is None:\n",
        "#             train_columns = X.columns  # store once\n",
        "\n",
        "#     return store_models, train_columns\n",
        "\n"
      ],
      "metadata": {
        "id": "P1AGjbE1quln"
      },
      "id": "P1AGjbE1quln",
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 2. Train the model\n",
        "# train_weeks = 120\n",
        "# store_models, train_columns = train_store_models_lgbm(train_dict, features, stores, max_stores=len(train_dict))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YCAcEZzusiRI",
        "outputId": "877657b2-e665-41e6-e626-a5138dd7a3b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "YCAcEZzusiRI",
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001162 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2395\n",
            "[LightGBM] [Info] Number of data points in the train set: 8030, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score 20703.954442\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001170 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2395\n",
            "[LightGBM] [Info] Number of data points in the train set: 8030, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score 20703.954442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=6, connect=7, read=6, redirect=7, status=7)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /ekvirika/WalmartRecruiting.mlflow/api/2.0/mlflow/runs/create\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "ðŸƒ View run store_1 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/464820461d1d49c0bbf2a3eae92ae858\n",
            "ðŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001723 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2393\n",
            "[LightGBM] [Info] Number of data points in the train set: 8025, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score 25814.261542\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001756 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2393\n",
            "[LightGBM] [Info] Number of data points in the train set: 8025, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score 25814.261542\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "ðŸƒ View run store_2 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/efbbcb2e1a194e5b9b3c612a9633fdbd\n",
            "ðŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001462 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2380\n",
            "[LightGBM] [Info] Number of data points in the train set: 7084, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score 4967.378992\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001600 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2380\n",
            "[LightGBM] [Info] Number of data points in the train set: 7084, number of used features: 26\n",
            "[LightGBM] [Info] Start training from score 4967.378992\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "ðŸƒ View run store_3 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/c45ed9a41b144ab98411fb9e6317cf8b\n",
            "ðŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1054007979.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 2. Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_weeks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstore_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_store_models_lgbm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_stores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-251278123.py\u001b[0m in \u001b[0;36mtrain_store_models_lgbm\u001b[0;34m(train_dict, features, stores, max_stores)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         )\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1951\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1952\u001b[0m             ParameterSampler(\n\u001b[1;32m   1953\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # submission = []\n",
        "\n",
        "# # # for store, (model, prop) in store_models.items():\n",
        "# # #     df, _ = train_dict[store]\n",
        "\n",
        "# # #     df = df.sort_values(\"Date\")\n",
        "\n",
        "# # #     # Preprocess test data\n",
        "# # #     df = BaseMerger(features, stores).fit_transform(df)\n",
        "# # #     df = FeatureAdder(is_train=True).fit_transform(df)\n",
        "# # #     df = MissingValueFiller(is_train=True).fit_transform(df)\n",
        "# # #     df = CategoricalEncoder().fit_transform(df)\n",
        "\n",
        "# # #     val_df = df.iloc[train_weeks:]\n",
        "\n",
        "# # #     X_val = val_df.drop(columns=[\"Weekly_Sales\", \"Date\"])\n",
        "# # #     dates = val_df[\"Date\"]\n",
        "\n",
        "# # #     preds = model.predict(X_val)\n",
        "\n",
        "# # #     for idx, (i, row) in enumerate(val_df.iterrows()):\n",
        "# # #         dept = prop[\"Dept\"] if isinstance(prop, dict) and \"Dept\" in prop else 1\n",
        "# # #         prediction_date = row[\"Date\"]\n",
        "# # #         submission.append({\n",
        "# # #             \"Id\": f\"{store}_{dept}_{prediction_date.strftime('%Y-%m-%d')}\",\n",
        "# # #             \"Weekly_Sales\": preds[idx]\n",
        "# # #         })\n",
        "\n",
        "# # # submission_df = pd.DataFrame(submission)\n",
        "# # # submission_df.to_csv(\"submission_lgbm.csv\", index=False)\n",
        "\n",
        "\n",
        "# # predictions = {}\n",
        "\n",
        "# # test[\"Date\"] = pd.to_datetime(test[\"Date\"])\n",
        "\n",
        "# # # Group full test_df by store\n",
        "# # for store, group in test.groupby(\"Store\"):\n",
        "# #     if store not in store_models:\n",
        "# #         continue  # Skip if model not trained\n",
        "\n",
        "# #     model, prop = store_models[store]\n",
        "\n",
        "# #     group = group.sort_values(\"Date\")\n",
        "\n",
        "# #     # Preprocessing\n",
        "# #     group = BaseMerger(features, stores).fit_transform(group)\n",
        "# #     group = FeatureAdder(is_train=False).fit_transform(group)\n",
        "# #     group = MissingValueFiller(is_train=False).fit_transform(group)\n",
        "# #     group = CategoricalEncoder().fit_transform(group)\n",
        "\n",
        "# #     X_test = group.drop(columns=[\"Date\"])\n",
        "# #     X_test = X_test.reindex(columns=train_columns, fill_value=0)\n",
        "\n",
        "# #     preds = model.predict(X_test)\n",
        "\n",
        "# #     for idx, (_, row) in enumerate(group.iterrows()):\n",
        "# #         dept = row[\"Dept\"]\n",
        "# #         date = row[\"Date\"]\n",
        "# #         predictions[(store, dept, date)] = preds[idx]\n",
        "\n",
        "# # submission_df = pd.DataFrame([\n",
        "# #     {\n",
        "# #         \"Id\": f\"{row['Store']}_{row['Dept']}_{row['Date'].strftime('%Y-%m-%d')}\",\n",
        "# #         \"Weekly_Sales\": predictions.get((row['Store'], row['Dept'], row['Date']), 0.0)\n",
        "# #     }\n",
        "# #     for _, row in test.iterrows()\n",
        "# # ])\n",
        "\n",
        "# # print(\"Submission shape:\", submission_df.shape)\n",
        "# # submission_df.to_csv(\"submission_lgbm.csv\", index=False)\n",
        "\n",
        "# submission = []\n",
        "# predictions = {}\n",
        "\n",
        "# for store, (model, prop) in store_models.items():\n",
        "#     df_test, _ = test_dict[store]\n",
        "\n",
        "#     df_test = df_test.sort_values(\"Date\")\n",
        "#     df_test[\"Date\"] = pd.to_datetime(df_test[\"Date\"])\n",
        "\n",
        "#     df_test = BaseMerger(features, stores).fit_transform(df_test)\n",
        "#     df_test = FeatureAdder(is_train=False).fit_transform(df_test)\n",
        "#     df_test = MissingValueFiller(is_train=False).fit_transform(df_test)\n",
        "#     df_test = CategoricalEncoder().fit_transform(df_test)\n",
        "\n",
        "#     X_test = df_test.drop(columns=[\"Date\"])\n",
        "#     X_test = X_test.reindex(columns=train_columns, fill_value=0)  # Ensure matching features\n",
        "\n",
        "#     preds = model.predict(X_test)\n",
        "\n",
        "#     for idx, row in df_test.iterrows():\n",
        "#         dept = prop[\"Dept\"] if isinstance(prop, dict) and \"Dept\" in prop else 1\n",
        "#         date = row[\"Date\"]\n",
        "#         predictions[(store, dept, date)] = preds[idx]\n",
        "\n",
        "# # Format as required\n",
        "# test_df = pd.concat([test_dict[store][0] for store in test_dict])\n",
        "# test_df[\"Date\"] = pd.to_datetime(test_df[\"Date\"])\n",
        "\n",
        "# submission_df = pd.DataFrame([\n",
        "#     {\n",
        "#         \"Id\": f\"{row['Store']}_{row['Dept']}_{row['Date'].strftime('%Y-%m-%d')}\",\n",
        "#         \"Weekly_Sales\": predictions.get((row[\"Store\"], row[\"Dept\"], row[\"Date\"]), 0.0)\n",
        "#     }\n",
        "#     for _, row in test_df.iterrows()\n",
        "# ])\n",
        "\n",
        "# print(\"Submission shape:\", submission_df.shape)\n",
        "# submission_df.to_csv(\"submission_lgbm.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "8jZvrjouskMW",
        "outputId": "8ccd1e94-25a8-4e48-a2ca-0114a6abac27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "8jZvrjouskMW",
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission shape: (115064, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "def train_store_models_lgbm(train_dict, features_df, stores_df, max_stores=None):\n",
        "    store_models = {}\n",
        "    train_columns_set = set()\n",
        "\n",
        "    for i, (store_id, (X_store, dept_props)) in enumerate(train_dict.items()):\n",
        "        if max_stores is not None and i >= max_stores:\n",
        "            break\n",
        "\n",
        "        print(f\"Training model for Store {store_id}...\")\n",
        "\n",
        "        X = X_store.drop(columns=[\"Weekly_Sales\", \"Date\", \"Store\"], errors=\"ignore\")\n",
        "        y = X_store[\"Weekly_Sales\"]\n",
        "\n",
        "        model = LGBMRegressor(\n",
        "            n_estimators=1000,\n",
        "            learning_rate=0.03,\n",
        "            num_leaves=31,\n",
        "            subsample=0.9,\n",
        "            colsample_bytree=0.9,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        model.fit(X, y)\n",
        "        store_models[store_id] = model\n",
        "        train_columns_set.update(X.columns)\n",
        "\n",
        "    return store_models, list(train_columns_set)\n"
      ],
      "metadata": {
        "id": "08zhQunNhJ70"
      },
      "id": "08zhQunNhJ70",
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def make_submission(test_df, test_dict, store_models, train_columns, submission_template_path, output_path):\n",
        "    submission = pd.read_csv(submission_template_path)\n",
        "    predictions = []\n",
        "\n",
        "    for store_id, (X_store, dept_props) in test_dict.items():\n",
        "        if store_id not in store_models:\n",
        "            print(f\"Skipping store {store_id}, no trained model.\")\n",
        "            continue\n",
        "\n",
        "        model = store_models[store_id]\n",
        "        X = X_store.drop(columns=[\"Date\", \"Store\"], errors=\"ignore\").copy()\n",
        "\n",
        "        # Align columns\n",
        "        for col in train_columns:\n",
        "            if col not in X.columns:\n",
        "                X[col] = 0\n",
        "        X = X[train_columns]\n",
        "\n",
        "        preds = model.predict(X)\n",
        "\n",
        "        for idx, row in X_store.iterrows():\n",
        "            date_str = row[\"Date\"].strftime(\"%Y-%m-%d\")\n",
        "            if dept_props is not None:\n",
        "                for dept, prop in dept_props.items():\n",
        "                    predictions.append({\n",
        "                        \"Id\": f\"{int(row['Store'])}_{int(dept)}_{date_str}\",\n",
        "                        \"Weekly_Sales\": max(preds[idx] * prop, 0)  # clip negative\n",
        "                    })\n",
        "            else:\n",
        "                predictions.append({\n",
        "                    \"Id\": f\"{int(row['Store'])}_{int(row['Dept'])}_{date_str}\",\n",
        "                    \"Weekly_Sales\": max(preds[idx], 0)\n",
        "                })\n",
        "\n",
        "    pred_df = pd.DataFrame(predictions)\n",
        "    final_submission = submission[[\"Id\"]].merge(pred_df, on=\"Id\", how=\"left\")\n",
        "    final_submission[\"Weekly_Sales\"] = final_submission[\"Weekly_Sales\"].fillna(0)\n",
        "    final_submission.to_csv(output_path, index=False)\n",
        "    print(f\"âœ… Submission saved to {output_path}\")\n"
      ],
      "metadata": {
        "id": "3m5ICMMxhJ4I"
      },
      "id": "3m5ICMMxhJ4I",
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply pipeline\n",
        "X_train_transformed = pipeline.fit_transform(train)\n",
        "X_test_transformed = pipeline.transform(test)\n",
        "\n",
        "# Aggregate\n",
        "aggregator = StoreAggregator()\n",
        "train_dict = aggregator.fit_transform(X_train_transformed)\n",
        "test_dict = aggregator.transform(X_test_transformed)\n",
        "\n",
        "# Train\n",
        "store_models, train_columns = train_store_models_lgbm(train_dict, features, stores)\n",
        "\n",
        "# Predict and generate submission\n",
        "make_submission(\n",
        "    test_df=test,\n",
        "    test_dict=test_dict,\n",
        "    store_models=store_models,\n",
        "    train_columns=train_columns,\n",
        "    submission_template_path='sample_submission.csv',\n",
        "    output_path='submission.csv'\n",
        ")\n"
      ],
      "metadata": {
        "id": "_eeD7XVphJ1d",
        "outputId": "52cd6d9a-e1a3-4c64-e12a-c88119b98f6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "id": "_eeD7XVphJ1d",
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Column not found: Weekly_Sales'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2086104541.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Apply pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX_train_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_test_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Aggregate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m   1090\u001b[0m             \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrouted_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3666521149.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Store'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Dept'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Store'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Dept'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sales_Lag_1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Weekly_Sales'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sales_Lag_2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Weekly_Sales'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1949\u001b[0m                 \u001b[0;34m\"Use a list instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m             )\n\u001b[0;32m-> 1951\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1953\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_gotitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Column not found: {key}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m             \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gotitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Column not found: Weekly_Sales'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('submission_lgbm.csv')"
      ],
      "metadata": {
        "id": "Bc4jXaj9sotc"
      },
      "id": "Bc4jXaj9sotc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kNooW4iBamQz"
      },
      "id": "kNooW4iBamQz",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}