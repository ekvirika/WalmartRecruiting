{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d91bb8cb",
      "metadata": {
        "id": "d91bb8cb",
        "outputId": "ebd9d377-c15f-4c1d-aed8-e62f30f23f1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "37e1ecd3",
      "metadata": {
        "id": "37e1ecd3"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q pytorch_lightning wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn mlflow wand dagshub neuralforecast\n",
        "\n",
        "# Set up Kaggle API\n",
        "!pip install -q kaggle ray[tune]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9a2fe659",
      "metadata": {
        "id": "9a2fe659"
      },
      "outputs": [],
      "source": [
        "# Upload your kaggle.json to Colab and run:\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fd383740",
      "metadata": {
        "id": "fd383740",
        "outputId": "b6e1e701-fa80-4569-fbd3-5de85ec47dec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 926MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "df11d311",
      "metadata": {
        "id": "df11d311",
        "outputId": "952b1ff3-51eb-4cd1-ed89-0a95c86541ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open stores.csv.zip, stores.csv.zip.zip or stores.csv.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!unzip -q train.csv.zip\n",
        "!unzip -q stores.csv.zip\n",
        "!unzip -q test.csv.zip\n",
        "!unzip -q features.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from dagshub import dagshub_logger\n",
        "import os\n",
        "\n",
        "# Set tracking URI manually\n",
        "mlflow.set_tracking_uri(\"https://dagshub.com/ekvirika/WalmartRecruiting.mlflow\")\n",
        "\n",
        "# Use your DagsHub credentials\n",
        "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"ekvirika\"\n",
        "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"0adb1004ddd4221395353efea2d8ead625e26197\"\n",
        "\n",
        "# Optional: set registry if you're using model registry\n",
        "mlflow.set_registry_uri(\"https://dagshub.com/ekvirika/WalmartRecruiting.mlflow\")\n",
        "mlflow.set_experiment(\"NBeats_Training\")\n",
        "\n",
        "# Detect GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# W&B setup\n",
        "wandb_project = 'WalmartRecruiting'\n",
        "wandb_entity = None  # Replace with your W&B entity if using teams\n"
      ],
      "metadata": {
        "id": "8kXR1XafxwrG"
      },
      "id": "8kXR1XafxwrG",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "OHL6uL5KyL5g",
        "outputId": "873ca82b-2b41-47ca-b218-1d0b57a9b109",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OHL6uL5KyL5g",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mellekvirikashvili\u001b[0m (\u001b[33mellekvirikashvili-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "iJXHAHdmFkWY"
      },
      "id": "iJXHAHdmFkWY"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import torch\n",
        "from neuralforecast import NeuralForecast\n",
        "from neuralforecast.models import NBEATS\n",
        "from neuralforecast.losses.pytorch import MSE\n"
      ],
      "metadata": {
        "id": "eIKZSkjlFLGR"
      },
      "id": "eIKZSkjlFLGR",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "BASE_DIR = os.getcwd()\n",
        "DATA_DIR = os.path.join(BASE_DIR, '')\n",
        "\n",
        "STORES_PATH = os.path.join(DATA_DIR, 'stores.csv')\n",
        "FEATURES_PATH = os.path.join(DATA_DIR, 'features.csv')\n",
        "TRAIN_PATH = os.path.join(DATA_DIR, 'train.csv')\n",
        "TEST_PATH = os.path.join(DATA_DIR, 'test.csv')\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def load_data():\n",
        "    return {\n",
        "        'stores': pd.read_csv(STORES_PATH),\n",
        "        'features': pd.read_csv(FEATURES_PATH),\n",
        "        'train': pd.read_csv(TRAIN_PATH),\n",
        "        'test': pd.read_csv(TEST_PATH)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "lm5yUTZovp-b"
      },
      "id": "lm5yUTZovp-b",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_wmae(y_true, y_pred, weights):\n",
        "    return np.sum(np.abs(y_true - y_pred) * weights) / np.sum(weights)\n"
      ],
      "metadata": {
        "id": "azsZxsWd2s9c"
      },
      "id": "azsZxsWd2s9c",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TweakedNBEATS(NBEATS):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.optimizer = torch.optim.AdamW(self.parameters(), lr=kwargs.get('learning_rate', 1e-3))\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.9)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return {\n",
        "            'optimizer': self.optimizer,\n",
        "            'lr_scheduler': {\n",
        "                'scheduler': self.scheduler,\n",
        "                'interval': 'epoch',\n",
        "                'frequency': 1\n",
        "            }\n",
        "        }"
      ],
      "metadata": {
        "id": "XZiLWriG2yOZ"
      },
      "id": "XZiLWriG2yOZ",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class RetailDataPreprocessor:\n",
        "    def __init__(self, scale_features=True):\n",
        "        self.scale_features = scale_features\n",
        "        self.scaler = StandardScaler() if scale_features else None\n",
        "\n",
        "    def load_and_merge(self, data_path):\n",
        "        train = pd.read_csv(f\"train.csv\")\n",
        "        features = pd.read_csv(f\"features.csv\")\n",
        "        stores = pd.read_csv(f\"stores.csv\")\n",
        "\n",
        "        data = train.merge(features, on=[\"Store\", \"Date\", \"IsHoliday\"], how=\"left\")\n",
        "        data = data.merge(stores, on=\"Store\", how=\"left\")\n",
        "        data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
        "        return data\n",
        "\n",
        "    def preprocess(self, data):\n",
        "        data = data.sort_values([\"Store\", \"Dept\", \"Date\"])\n",
        "        data[\"Year\"] = data[\"Date\"].dt.year\n",
        "        data[\"Month\"] = data[\"Date\"].dt.month\n",
        "        data[\"Week\"] = data[\"Date\"].dt.isocalendar().week\n",
        "        data[\"Day\"] = data[\"Date\"].dt.day\n",
        "\n",
        "        features = [\"Temperature\", \"Fuel_Price\", \"CPI\", \"Unemployment\", \"Year\", \"Month\", \"Week\", \"Day\"]\n",
        "        if self.scale_features:\n",
        "            data[features] = self.scaler.fit_transform(data[features])\n",
        "\n",
        "        return data\n"
      ],
      "metadata": {
        "id": "3nCCLXq7wZJ3"
      },
      "id": "3nCCLXq7wZJ3",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from neuralforecast import NeuralForecast\n",
        "from neuralforecast.models import NBEATS\n",
        "from neuralforecast.losses.pytorch import MSE\n",
        "from ray import tune\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "\n",
        "def prepare_neuralforecast_input(data):\n",
        "    df = data[[\"Store\", \"Dept\", \"Date\", \"Weekly_Sales\"]].copy()\n",
        "    df[\"unique_id\"] = df[\"Store\"].astype(str) + \"_\" + df[\"Dept\"].astype(str)\n",
        "    df.rename(columns={\"Date\": \"ds\", \"Weekly_Sales\": \"y\"}, inplace=True)\n",
        "    return df\n",
        "\n",
        "def train_model(config, forecast_data, h):\n",
        "    with mlflow.start_run():\n",
        "        model = NeuralForecast(\n",
        "            models=[\n",
        "                NBEATS(\n",
        "                    h=h,\n",
        "                    input_size=config[\"input_size\"],\n",
        "                    max_steps=config[\"steps\"],\n",
        "                    learning_rate=config[\"lr\"],\n",
        "                    loss=\"MAE\",\n",
        "                    scaler_type=\"robust\",\n",
        "                    valid_loss=\"MAE\",\n",
        "                    early_stop_patience_steps=10,\n",
        "                    random_seed=42,\n",
        "                    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "                )\n",
        "            ],\n",
        "            freq=\"D\"\n",
        "        )\n",
        "\n",
        "        model.fit(forecast_data)\n",
        "        forecast = model.predict().reset_index()\n",
        "        y_true = forecast_data.df[forecast_data.df[\"ds\"] > forecast_data.df[\"ds\"].max() - pd.Timedelta(days=h)][\"y\"].values\n",
        "        y_pred = forecast[\"NBEATS\"].values\n",
        "\n",
        "        mse = np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "        # Logging to MLflow\n",
        "        mlflow.log_params(config)\n",
        "        mlflow.log_metric(\"mse\", mse)\n",
        "\n",
        "        # Logging to W&B\n",
        "        wandb.log({\"mse\": mse, **config})\n",
        "\n",
        "        tune.report(mse=mse)\n",
        "\n",
        "def tune_hyperparameters(df, h):\n",
        "    def trainable(config):\n",
        "        model = NeuralForecast(\n",
        "            models=[\n",
        "                NBEATS(\n",
        "                    h=h,\n",
        "                    input_size=config[\"input_size\"],\n",
        "                    loss=MSE(),\n",
        "                    learning_rate=config[\"lr\"],\n",
        "                    max_steps=config[\"steps\"]\n",
        "                )\n",
        "            ],\n",
        "            freq=\"W\"\n",
        "        )\n",
        "        model.fit(df=df)\n",
        "        forecast_df = model.predict()\n",
        "        last = df.groupby(\"unique_id\").tail(h)\n",
        "        mse = ((forecast_df[\"NBEATS\"] - last[\"y\"].values) ** 2).mean()\n",
        "        tune.report(mse=mse)\n",
        "\n",
        "    analysis = tune.run(\n",
        "        trainable,\n",
        "        config={\n",
        "            \"input_size\": tune.choice([2*h, 3*h, 4*h]),\n",
        "            \"lr\": tune.loguniform(1e-4, 1e-2),\n",
        "            \"steps\": tune.choice([200, 500, 1000])\n",
        "        },\n",
        "        num_samples=5,\n",
        "        scheduler=ASHAScheduler(metric=\"mse\", mode=\"min\"),\n",
        "        resources_per_trial={\"cpu\": 2},\n",
        "        storage_path=\"/content/\"\n",
        "    )\n",
        "\n",
        "\n",
        "    print(\"Best config: \", analysis.best_config)\n",
        "    return analysis.best_config\n"
      ],
      "metadata": {
        "id": "UlWu_zSBwc08"
      },
      "id": "UlWu_zSBwc08",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def weighted_mae(true, pred, is_holiday):\n",
        "    weights = np.where(is_holiday, 5, 1)\n",
        "    return np.sum(weights * np.abs(true - pred)) / np.sum(weights)\n"
      ],
      "metadata": {
        "id": "xCrLttqGv_5l"
      },
      "id": "xCrLttqGv_5l",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from neuralforecast.models import NBEATS\n",
        "\n",
        "class TweakedNBEATS(NBEATS):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.optimizer = torch.optim.AdamW(self.parameters(), lr=kwargs.get('learning_rate', 1e-3))\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.9)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return {\n",
        "            'optimizer': self.optimizer,\n",
        "            'lr_scheduler': {\n",
        "                'scheduler': self.scheduler,\n",
        "                'interval': 'epoch',\n",
        "                'frequency': 1\n",
        "            }\n",
        "        }\n"
      ],
      "metadata": {
        "id": "VIHCo97Cv82j"
      },
      "id": "VIHCo97Cv82j",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load and preprocess\n",
        "preprocessor = RetailDataPreprocessor()\n",
        "raw_data = preprocessor.load_and_merge(\"\")\n",
        "processed_data = preprocessor.preprocess(raw_data)\n",
        "\n",
        "# 2. Prepare data for NeuralForecast\n",
        "forecast_data = prepare_neuralforecast_input(processed_data)\n",
        "horizon = 12  # number of weeks to forecast\n",
        "\n",
        "# 3. Optional hyperparameter tuning\n",
        "best_config = tune_hyperparameters(forecast_data, h=horizon)\n",
        "\n",
        "# 4. Train final model\n",
        "final_forecast = train_model(forecast_data, h=horizon)\n",
        "final_forecast.to_csv(\"/mnt/data/WalmartForecastCustom/final_forecast.csv\", index=False)\n",
        "\n",
        "print(\"Forecast complete. Output saved.\")\n"
      ],
      "metadata": {
        "id": "82w_GTt3wlao",
        "outputId": "fd07d1e2-3de8-4060-df38-24fe2be3c76e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "82w_GTt3wlao",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------+\n",
            "| Configuration for experiment     trainable_2025-08-03_02-57-00   |\n",
            "+------------------------------------------------------------------+\n",
            "| Search algorithm                 BasicVariantGenerator           |\n",
            "| Scheduler                        AsyncHyperBandScheduler         |\n",
            "| Number of trials                 5                               |\n",
            "+------------------------------------------------------------------+\n",
            "\n",
            "View detailed results here: /content/trainable_2025-08-03_02-57-00\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2025-08-03_02-33-24_172762_386/artifacts/2025-08-03_02-57-00/trainable_2025-08-03_02-57-00/driver_artifacts`\n",
            "\n",
            "Trial status: 5 PENDING\n",
            "Current time: 2025-08-03 02:57:01. Total running time: 1s\n",
            "Logical resource usage: 0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-------------------------------------------------------------------------+\n",
            "| Trial name              status       input_size            lr     steps |\n",
            "+-------------------------------------------------------------------------+\n",
            "| trainable_86a28_00000   PENDING              24   0.000234596       200 |\n",
            "| trainable_86a28_00001   PENDING              36   0.000129884       500 |\n",
            "| trainable_86a28_00002   PENDING              48   0.00995402        200 |\n",
            "| trainable_86a28_00003   PENDING              48   0.00135525        200 |\n",
            "| trainable_86a28_00004   PENDING              36   0.00237632       1000 |\n",
            "+-------------------------------------------------------------------------+\n",
            "\u001b[33m(raylet)\u001b[0m Warning: The actor ImplicitFunc is very large (31 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n",
            "\n",
            "Trial trainable_86a28_00000 started with configuration:\n",
            "+------------------------------------------------+\n",
            "| Trial trainable_86a28_00000 config             |\n",
            "+------------------------------------------------+\n",
            "| input_size                                  24 |\n",
            "| lr                                     0.00023 |\n",
            "| steps                                      200 |\n",
            "+------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(trainable pid=9696)\u001b[0m Seed set to 1\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m GPU available: False, used: False\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m 2025-08-03 02:57:13.659701: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m E0000 00:00:1754189833.676817    9786 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m E0000 00:00:1754189833.681737    9786 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m 2025-08-03 02:57:13.701764: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m \n",
            "\u001b[36m(trainable pid=9696)\u001b[0m   | Name         | Type          | Params | Mode \n",
            "\u001b[36m(trainable pid=9696)\u001b[0m -------------------------------------------------------\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m 0 | loss         | MSE           | 0      | train\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m 1 | padder_train | ConstantPad1d | 0      | train\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m 2 | scaler       | TemporalNorm  | 0      | train\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m 3 | blocks       | ModuleList    | 2.4 M  | train\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m -------------------------------------------------------\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m 2.4 M     Trainable params\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m 900       Non-trainable params\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m 2.4 M     Total params\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m 9.789     Total estimated model params size (MB)\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m 31        Modules in train mode\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m 0         Modules in eval mode\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0:   0%|          | 0/105 [00:00<?, ?it/s] \n",
            "Epoch 0:  19%|█▉        | 20/105 [00:03<00:15,  5.65it/s, v_num=0, train_loss_step=1.53e+7]\n",
            "Epoch 0:  38%|███▊      | 40/105 [00:07<00:12,  5.31it/s, v_num=0, train_loss_step=1.14e+8]\n",
            "Epoch 0:  57%|█████▋    | 60/105 [00:12<00:09,  4.97it/s, v_num=0, train_loss_step=1.97e+7]\n",
            "\n",
            "Trial status: 1 RUNNING | 4 PENDING\n",
            "Current time: 2025-08-03 02:57:31. Total running time: 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-------------------------------------------------------------------------+\n",
            "| Trial name              status       input_size            lr     steps |\n",
            "+-------------------------------------------------------------------------+\n",
            "| trainable_86a28_00000   RUNNING              24   0.000234596       200 |\n",
            "| trainable_86a28_00001   PENDING              36   0.000129884       500 |\n",
            "| trainable_86a28_00002   PENDING              48   0.00995402        200 |\n",
            "| trainable_86a28_00003   PENDING              48   0.00135525        200 |\n",
            "| trainable_86a28_00004   PENDING              36   0.00237632       1000 |\n",
            "+-------------------------------------------------------------------------+\n",
            "Epoch 0:  76%|███████▌  | 80/105 [00:15<00:04,  5.17it/s, v_num=0, train_loss_step=1.36e+7]\n",
            "Epoch 0:  95%|█████████▌| 100/105 [00:18<00:00,  5.29it/s, v_num=0, train_loss_step=3.04e+7]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/105 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/105 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m \n",
            "Validation DataLoader 0:  19%|█▉        | 20/105 [00:00<00:00, 915.77it/s]\u001b[A\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 40/105 [00:00<00:00, 952.34it/s]\u001b[A\n",
            "Validation DataLoader 0:  57%|█████▋    | 60/105 [00:00<00:00, 1013.09it/s]\u001b[A\n",
            "Validation DataLoader 0:  76%|███████▌  | 80/105 [00:00<00:00, 1030.45it/s]\u001b[A\n",
            "Validation DataLoader 0:  95%|█████████▌| 100/105 [00:00<00:00, 1036.34it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 105/105 [00:00<00:00, 1041.54it/s]\u001b[A\n",
            "Epoch 0:  95%|█████████▌| 100/105 [00:18<00:00,  5.26it/s, v_num=0, train_loss_step=3.04e+7]\n",
            "Epoch 1:   0%|          | 0/95 [00:00<?, ?it/s, v_num=0, train_loss_step=6.03e+7, train_loss_epoch=6.29e+7]\n",
            "Epoch 1:  21%|██        | 20/95 [00:04<00:16,  4.51it/s, v_num=0, train_loss_step=1.29e+8, train_loss_epoch=6.29e+7]\n",
            "Epoch 1:  42%|████▏     | 40/95 [00:10<00:14,  3.69it/s, v_num=0, train_loss_step=1.35e+8, train_loss_epoch=6.29e+7]\n",
            "Epoch 1:  63%|██████▎   | 60/95 [00:14<00:08,  4.20it/s, v_num=0, train_loss_step=3.05e+7, train_loss_epoch=6.29e+7]\n",
            "Epoch 1:  84%|████████▍ | 80/95 [00:18<00:03,  4.39it/s, v_num=0, train_loss_step=6.46e+6, train_loss_epoch=6.29e+7]\n",
            "Epoch 1: 100%|██████████| 95/95 [00:21<00:00,  4.33it/s, v_num=0, train_loss_step=9.46e+7, train_loss_epoch=6.29e+7]\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m \n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m \n",
            "Validation:   0%|          | 0/105 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/105 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  19%|█▉        | 20/105 [00:00<00:00, 584.31it/s]\u001b[A\n",
            "Validation DataLoader 0:  38%|███▊      | 40/105 [00:00<00:00, 568.68it/s]\u001b[A\n",
            "Validation DataLoader 0:  57%|█████▋    | 60/105 [00:00<00:00, 606.79it/s]\u001b[A\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(trainable pid=9696)\u001b[0m `Trainer.fit` stopped: `max_steps=200` reached.\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m GPU available: False, used: False\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(trainable pid=9696)\u001b[0m \n",
            "\u001b[36m(trainable pid=9696)\u001b[0m \rValidation DataLoader 0:  76%|███████▌  | 80/105 [00:00<00:00, 666.57it/s]\u001b[A\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m \rValidation DataLoader 0:  95%|█████████▌| 100/105 [00:00<00:00, 723.28it/s]\u001b[A\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m \rValidation DataLoader 0: 100%|██████████| 105/105 [00:00<00:00, 736.37it/s]\u001b[A\n",
            "\u001b[36m(trainable pid=9696)\u001b[0m \r                                                                           \u001b[A\rEpoch 1: 100%|██████████| 95/95 [00:22<00:00,  4.30it/s, v_num=0, train_loss_step=9.46e+7, train_loss_epoch=6.29e+7]\rEpoch 1: 100%|██████████| 95/95 [00:22<00:00,  4.30it/s, v_num=0, train_loss_step=9.46e+7, train_loss_epoch=5.82e+7]\rEpoch 1: 100%|██████████| 95/95 [00:22<00:00,  4.30it/s, v_num=0, train_loss_step=9.46e+7, train_loss_epoch=5.82e+7]\n",
            "Predicting DataLoader 0:   0%|          | 0/105 [00:00<?, ?it/s]\n",
            "Predicting DataLoader 0:  19%|█▉        | 20/105 [00:00<00:00, 192.28it/s]\n",
            "Predicting DataLoader 0:  38%|███▊      | 40/105 [00:00<00:00, 190.40it/s]\n",
            "Predicting DataLoader 0:  57%|█████▋    | 60/105 [00:00<00:00, 191.42it/s]\n",
            "Predicting DataLoader 0:  76%|███████▌  | 80/105 [00:00<00:00, 190.36it/s]\n",
            "Predicting DataLoader 0: 100%|██████████| 105/105 [00:00<00:00, 192.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-03 02:57:59,357\tERROR tune_controller.py:1331 -- Trial task failed for trial trainable_86a28_00000\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 2858, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 958, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=9696, ip=172.28.0.12, actor_id=ec7dbf16245013cf9f43b22301000000, repr=trainable)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 261, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-519843353.py\", line 67, in trainable\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/ops/common.py\", line 76, in new_method\n",
            "    return method(self, other)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py\", line 194, in __sub__\n",
            "    return self._arith_method(other, operator.sub)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\", line 6135, in _arith_method\n",
            "    return base.IndexOpsMixin._arith_method(self, other, op)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\", line 1382, in _arith_method\n",
            "    result = ops.arithmetic_op(lvalues, rvalues, op)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/ops/array_ops.py\", line 283, in arithmetic_op\n",
            "    res_values = _na_arithmetic_op(left, right, op)  # type: ignore[arg-type]\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/ops/array_ops.py\", line 218, in _na_arithmetic_op\n",
            "    result = func(left, right)\n",
            "             ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py\", line 242, in evaluate\n",
            "    return _evaluate(op, op_str, a, b)  # type: ignore[misc]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py\", line 73, in _evaluate_standard\n",
            "    return op(a, b)\n",
            "           ^^^^^^^^\n",
            "ValueError: operands could not be broadcast together with shapes (39972,) (38615,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial trainable_86a28_00000 errored after 0 iterations at 2025-08-03 02:57:59. Total running time: 58s\n",
            "Error file: /tmp/ray/session_2025-08-03_02-33-24_172762_386/artifacts/2025-08-03_02-57-00/trainable_2025-08-03_02-57-00/driver_artifacts/trainable_86a28_00000_0_input_size=24,lr=0.0002,steps=200_2025-08-03_02-57-01/error.txt\n",
            "\n",
            "Trial status: 1 ERROR | 4 PENDING\n",
            "Current time: 2025-08-03 02:58:01. Total running time: 1min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-------------------------------------------------------------------------+\n",
            "| Trial name              status       input_size            lr     steps |\n",
            "+-------------------------------------------------------------------------+\n",
            "| trainable_86a28_00001   PENDING              36   0.000129884       500 |\n",
            "| trainable_86a28_00002   PENDING              48   0.00995402        200 |\n",
            "| trainable_86a28_00003   PENDING              48   0.00135525        200 |\n",
            "| trainable_86a28_00004   PENDING              36   0.00237632       1000 |\n",
            "| trainable_86a28_00000   ERROR                24   0.000234596       200 |\n",
            "+-------------------------------------------------------------------------+\n",
            "\n",
            "Trial trainable_86a28_00001 started with configuration:\n",
            "+------------------------------------------------+\n",
            "| Trial trainable_86a28_00001 config             |\n",
            "+------------------------------------------------+\n",
            "| input_size                                  36 |\n",
            "| lr                                     0.00013 |\n",
            "| steps                                      500 |\n",
            "+------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(trainable pid=10027)\u001b[0m Seed set to 1\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m GPU available: False, used: False\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m HPU available: False, using: 0 HPUs\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m 2025-08-03 02:58:14.829802: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m E0000 00:00:1754189894.847399   10136 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m E0000 00:00:1754189894.852367   10136 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m 2025-08-03 02:58:14.869541: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m \n",
            "\u001b[36m(trainable pid=10027)\u001b[0m   | Name         | Type          | Params | Mode \n",
            "\u001b[36m(trainable pid=10027)\u001b[0m -------------------------------------------------------\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m 0 | loss         | MSE           | 0      | train\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m 1 | padder_train | ConstantPad1d | 0      | train\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m 2 | scaler       | TemporalNorm  | 0      | train\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m 3 | blocks       | ModuleList    | 2.5 M  | train\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m -------------------------------------------------------\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m 2.5 M     Trainable params\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m 1.2 K     Non-trainable params\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m 2.5 M     Total params\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m 9.889     Total estimated model params size (MB)\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m 31        Modules in train mode\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m 0         Modules in eval mode\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0:   0%|          | 0/105 [00:00<?, ?it/s] \n",
            "Epoch 0:  19%|█▉        | 20/105 [00:03<00:14,  5.71it/s, v_num=0, train_loss_step=1.43e+7]\n",
            "Epoch 0:  38%|███▊      | 40/105 [00:07<00:12,  5.28it/s, v_num=0, train_loss_step=1.12e+8]\n",
            "Epoch 0:  57%|█████▋    | 60/105 [00:12<00:09,  4.95it/s, v_num=0, train_loss_step=2.47e+7]\n",
            "\n",
            "Trial status: 1 ERROR | 1 RUNNING | 3 PENDING\n",
            "Current time: 2025-08-03 02:58:32. Total running time: 1min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-------------------------------------------------------------------------+\n",
            "| Trial name              status       input_size            lr     steps |\n",
            "+-------------------------------------------------------------------------+\n",
            "| trainable_86a28_00001   RUNNING              36   0.000129884       500 |\n",
            "| trainable_86a28_00002   PENDING              48   0.00995402        200 |\n",
            "| trainable_86a28_00003   PENDING              48   0.00135525        200 |\n",
            "| trainable_86a28_00004   PENDING              36   0.00237632       1000 |\n",
            "| trainable_86a28_00000   ERROR                24   0.000234596       200 |\n",
            "+-------------------------------------------------------------------------+\n",
            "Epoch 0:  76%|███████▌  | 80/105 [00:15<00:04,  5.13it/s, v_num=0, train_loss_step=1.18e+7]\n",
            "Epoch 0:  95%|█████████▌| 100/105 [00:19<00:00,  5.23it/s, v_num=0, train_loss_step=2.48e+7]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m \n",
            "Validation:   0%|          | 0/105 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/105 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m \n",
            "Validation DataLoader 0:  19%|█▉        | 20/105 [00:00<00:00, 872.41it/s]\u001b[A\n",
            "Validation DataLoader 0:  38%|███▊      | 40/105 [00:00<00:00, 889.44it/s]\u001b[A\n",
            "Validation DataLoader 0:  57%|█████▋    | 60/105 [00:00<00:00, 910.08it/s]\u001b[A\n",
            "Validation DataLoader 0:  76%|███████▌  | 80/105 [00:00<00:00, 926.54it/s]\u001b[A\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m \n",
            "Validation DataLoader 0:  95%|█████████▌| 100/105 [00:00<00:00, 907.05it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 105/105 [00:00<00:00, 908.46it/s]\u001b[A\n",
            "Epoch 0:  95%|█████████▌| 100/105 [00:19<00:00,  5.20it/s, v_num=0, train_loss_step=2.48e+7]\n",
            "Epoch 1:   0%|          | 0/105 [00:00<?, ?it/s, v_num=0, train_loss_step=5.58e+7, train_loss_epoch=6.23e+7]\n",
            "Epoch 1:  19%|█▉        | 20/105 [00:04<00:19,  4.32it/s, v_num=0, train_loss_step=9.68e+7, train_loss_epoch=6.23e+7]\n",
            "Epoch 1:  38%|███▊      | 40/105 [00:08<00:14,  4.58it/s, v_num=0, train_loss_step=9.79e+7, train_loss_epoch=6.23e+7]\n",
            "Epoch 1:  57%|█████▋    | 60/105 [00:12<00:09,  4.90it/s, v_num=0, train_loss_step=2.7e+7, train_loss_epoch=6.23e+7] \n",
            "Epoch 1:  76%|███████▌  | 80/105 [00:15<00:04,  5.11it/s, v_num=0, train_loss_step=6.27e+6, train_loss_epoch=6.23e+7]\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m \n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/105 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/105 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  19%|█▉        | 20/105 [00:00<00:00, 538.12it/s]\u001b[A\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 40/105 [00:00<00:00, 503.97it/s]\u001b[A\n",
            "Validation DataLoader 0:  57%|█████▋    | 60/105 [00:00<00:00, 495.29it/s]\u001b[A\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m \n",
            "Validation DataLoader 0:  76%|███████▌  | 80/105 [00:00<00:00, 494.65it/s]\u001b[A\n",
            "Validation DataLoader 0:  95%|█████████▌| 100/105 [00:00<00:00, 502.82it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 105/105 [00:00<00:00, 503.55it/s]\u001b[A\n",
            "Epoch 1:  76%|███████▌  | 80/105 [00:19<00:06,  4.15it/s, v_num=0, train_loss_step=7.65e+7, train_loss_epoch=6.23e+7]\n",
            "Epoch 1:  95%|█████████▌| 100/105 [00:20<00:01,  4.87it/s, v_num=0, train_loss_step=3.45e+7, train_loss_epoch=6.23e+7]\n",
            "Epoch 2:   0%|          | 0/105 [00:00<?, ?it/s, v_num=0, train_loss_step=9.39e+7, train_loss_epoch=5.21e+7]\n",
            "Trial status: 1 ERROR | 1 RUNNING | 3 PENDING\n",
            "Current time: 2025-08-03 02:59:02. Total running time: 2min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-------------------------------------------------------------------------+\n",
            "| Trial name              status       input_size            lr     steps |\n",
            "+-------------------------------------------------------------------------+\n",
            "| trainable_86a28_00001   RUNNING              36   0.000129884       500 |\n",
            "| trainable_86a28_00002   PENDING              48   0.00995402        200 |\n",
            "| trainable_86a28_00003   PENDING              48   0.00135525        200 |\n",
            "| trainable_86a28_00004   PENDING              36   0.00237632       1000 |\n",
            "| trainable_86a28_00000   ERROR                24   0.000234596       200 |\n",
            "+-------------------------------------------------------------------------+\n",
            "Epoch 2:  19%|█▉        | 20/105 [00:03<00:15,  5.66it/s, v_num=0, train_loss_step=3.94e+7, train_loss_epoch=5.21e+7]\n",
            "Epoch 2:  38%|███▊      | 40/105 [00:07<00:11,  5.69it/s, v_num=0, train_loss_step=4.24e+7, train_loss_epoch=5.21e+7]\n",
            "Epoch 2:  57%|█████▋    | 60/105 [00:10<00:08,  5.59it/s, v_num=0, train_loss_step=2.38e+7, train_loss_epoch=5.21e+7]\n",
            "Epoch 2:  76%|███████▌  | 80/105 [00:15<00:04,  5.10it/s, v_num=0, train_loss_step=1.33e+7, train_loss_epoch=5.21e+7]\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m \n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/105 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/105 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  19%|█▉        | 20/105 [00:00<00:00, 1033.05it/s]\u001b[A\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m \n",
            "Validation DataLoader 0:  38%|███▊      | 40/105 [00:00<00:00, 990.85it/s] \u001b[A\n",
            "Validation DataLoader 0:  57%|█████▋    | 60/105 [00:00<00:00, 1009.43it/s]\u001b[A\n",
            "Validation DataLoader 0:  76%|███████▌  | 80/105 [00:00<00:00, 1020.75it/s]\u001b[A\n",
            "Validation DataLoader 0:  95%|█████████▌| 100/105 [00:00<00:00, 1024.99it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 105/105 [00:00<00:00, 1025.53it/s]\u001b[A\n",
            "Epoch 2:  76%|███████▌  | 80/105 [00:17<00:05,  4.57it/s, v_num=0, train_loss_step=8.05e+7, train_loss_epoch=5.21e+7]\n",
            "Epoch 2:  95%|█████████▌| 100/105 [00:19<00:00,  5.04it/s, v_num=0, train_loss_step=8.49e+7, train_loss_epoch=5.21e+7]\n",
            "Epoch 3:   0%|          | 0/105 [00:00<?, ?it/s, v_num=0, train_loss_step=3.95e+7, train_loss_epoch=4.48e+7]\n",
            "Epoch 3:  19%|█▉        | 20/105 [00:03<00:14,  5.80it/s, v_num=0, train_loss_step=1.49e+7, train_loss_epoch=4.48e+7]\n",
            "Epoch 3:  38%|███▊      | 40/105 [00:07<00:12,  5.03it/s, v_num=0, train_loss_step=1.52e+7, train_loss_epoch=4.48e+7]\n",
            "Trial status: 1 ERROR | 1 RUNNING | 3 PENDING\n",
            "Current time: 2025-08-03 02:59:32. Total running time: 2min 31s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-------------------------------------------------------------------------+\n",
            "| Trial name              status       input_size            lr     steps |\n",
            "+-------------------------------------------------------------------------+\n",
            "| trainable_86a28_00001   RUNNING              36   0.000129884       500 |\n",
            "| trainable_86a28_00002   PENDING              48   0.00995402        200 |\n",
            "| trainable_86a28_00003   PENDING              48   0.00135525        200 |\n",
            "| trainable_86a28_00004   PENDING              36   0.00237632       1000 |\n",
            "| trainable_86a28_00000   ERROR                24   0.000234596       200 |\n",
            "+-------------------------------------------------------------------------+\n",
            "Epoch 3:  57%|█████▋    | 60/105 [00:12<00:09,  4.96it/s, v_num=0, train_loss_step=3.23e+7, train_loss_epoch=4.48e+7]\n",
            "Epoch 3:  76%|███████▌  | 80/105 [00:15<00:04,  5.13it/s, v_num=0, train_loss_step=2.3e+7, train_loss_epoch=4.48e+7] \n",
            "\u001b[36m(trainable pid=10027)\u001b[0m \n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/105 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/105 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  19%|█▉        | 20/105 [00:00<00:00, 1042.76it/s]\u001b[A\n",
            "Validation DataLoader 0:  38%|███▊      | 40/105 [00:00<00:00, 1042.25it/s]\u001b[A\n",
            "Validation DataLoader 0:  57%|█████▋    | 60/105 [00:00<00:00, 1032.45it/s]\u001b[A\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m \n",
            "Validation DataLoader 0:  76%|███████▌  | 80/105 [00:00<00:00, 1009.68it/s]\u001b[A\n",
            "Validation DataLoader 0:  95%|█████████▌| 100/105 [00:00<00:00, 1025.80it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 105/105 [00:00<00:00, 1032.26it/s]\u001b[A\n",
            "Epoch 3:  76%|███████▌  | 80/105 [00:16<00:05,  4.83it/s, v_num=0, train_loss_step=7.35e+7, train_loss_epoch=4.48e+7]\n",
            "Epoch 3:  95%|█████████▌| 100/105 [00:19<00:00,  5.22it/s, v_num=0, train_loss_step=1.42e+8, train_loss_epoch=4.48e+7]\n",
            "Epoch 4:   0%|          | 0/80 [00:00<?, ?it/s, v_num=0, train_loss_step=3.48e+6, train_loss_epoch=4.66e+7]\n",
            "Epoch 4:  25%|██▌       | 20/80 [00:04<00:14,  4.08it/s, v_num=0, train_loss_step=3.47e+7, train_loss_epoch=4.66e+7]\n",
            "Epoch 4:  50%|█████     | 40/80 [00:08<00:08,  4.73it/s, v_num=0, train_loss_step=2.04e+7, train_loss_epoch=4.66e+7]\n",
            "Epoch 4:  75%|███████▌  | 60/80 [00:11<00:03,  5.08it/s, v_num=0, train_loss_step=1.11e+7, train_loss_epoch=4.66e+7]\n",
            "Epoch 4: 100%|██████████| 80/80 [00:15<00:00,  5.29it/s, v_num=0, train_loss_step=4.04e+7, train_loss_epoch=4.66e+7]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/105 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/105 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:  19%|█▉        | 20/105 [00:00<00:00, 1210.25it/s]\u001b[A\n",
            "Validation DataLoader 0:  38%|███▊      | 40/105 [00:00<00:00, 1192.89it/s]\u001b[A\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m \n",
            "Validation DataLoader 0:  57%|█████▋    | 60/105 [00:00<00:00, 1130.80it/s]\u001b[A\n",
            "Validation DataLoader 0:  76%|███████▌  | 80/105 [00:00<00:00, 1123.68it/s]\u001b[A\n",
            "Validation DataLoader 0:  95%|█████████▌| 100/105 [00:00<00:00, 1124.95it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 105/105 [00:00<00:00, 1127.26it/s]\u001b[A\n",
            "Epoch 4: 100%|██████████| 80/80 [00:15<00:00,  5.25it/s, v_num=0, train_loss_step=4.04e+7, train_loss_epoch=4.89e+7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(trainable pid=10027)\u001b[0m `Trainer.fit` stopped: `max_steps=500` reached.\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m GPU available: False, used: False\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m TPU available: False, using: 0 TPU cores\n",
            "\u001b[36m(trainable pid=10027)\u001b[0m HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(trainable pid=10027)\u001b[0m \rPredicting: |          | 0/? [00:00<?, ?it/s]\rPredicting:   0%|          | 0/105 [00:00<?, ?it/s]\rPredicting DataLoader 0:   0%|          | 0/105 [00:00<?, ?it/s]\n",
            "Predicting DataLoader 0:  19%|█▉        | 20/105 [00:00<00:00, 203.58it/s]\n",
            "Predicting DataLoader 0:  38%|███▊      | 40/105 [00:00<00:00, 203.73it/s]\n",
            "Predicting DataLoader 0:  57%|█████▋    | 60/105 [00:00<00:00, 172.06it/s]\n",
            "Predicting DataLoader 0:  76%|███████▌  | 80/105 [00:00<00:00, 156.54it/s]\n",
            "Predicting DataLoader 0:  95%|█████████▌| 100/105 [00:00<00:00, 150.94it/s]\n",
            "Predicting DataLoader 0: 100%|██████████| 105/105 [00:00<00:00, 151.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-03 02:59:56,657\tERROR tune_controller.py:1331 -- Trial task failed for trial trainable_86a28_00001\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 2858, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/_private/worker.py\", line 958, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=10027, ip=172.28.0.12, actor_id=a4410f2cfe57dca2a08fb3a101000000, repr=trainable)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/air/_internal/util.py\", line 107, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ray/tune/trainable/function_trainable.py\", line 261, in _trainable_func\n",
            "    output = fn()\n",
            "             ^^^^\n",
            "  File \"/tmp/ipython-input-519843353.py\", line 67, in trainable\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/ops/common.py\", line 76, in new_method\n",
            "    return method(self, other)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py\", line 194, in __sub__\n",
            "    return self._arith_method(other, operator.sub)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\", line 6135, in _arith_method\n",
            "    return base.IndexOpsMixin._arith_method(self, other, op)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\", line 1382, in _arith_method\n",
            "    result = ops.arithmetic_op(lvalues, rvalues, op)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/ops/array_ops.py\", line 283, in arithmetic_op\n",
            "    res_values = _na_arithmetic_op(left, right, op)  # type: ignore[arg-type]\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/ops/array_ops.py\", line 218, in _na_arithmetic_op\n",
            "    result = func(left, right)\n",
            "             ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py\", line 242, in evaluate\n",
            "    return _evaluate(op, op_str, a, b)  # type: ignore[misc]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py\", line 73, in _evaluate_standard\n",
            "    return op(a, b)\n",
            "           ^^^^^^^^\n",
            "ValueError: operands could not be broadcast together with shapes (39972,) (38615,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial trainable_86a28_00001 errored after 0 iterations at 2025-08-03 02:59:56. Total running time: 2min 56s\n",
            "Error file: /tmp/ray/session_2025-08-03_02-33-24_172762_386/artifacts/2025-08-03_02-57-00/trainable_2025-08-03_02-57-00/driver_artifacts/trainable_86a28_00001_1_input_size=36,lr=0.0001,steps=500_2025-08-03_02-57-01/error.txt\n",
            "\n",
            "Trial status: 2 ERROR | 3 PENDING\n",
            "Current time: 2025-08-03 03:00:02. Total running time: 3min 1s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-------------------------------------------------------------------------+\n",
            "| Trial name              status       input_size            lr     steps |\n",
            "+-------------------------------------------------------------------------+\n",
            "| trainable_86a28_00002   PENDING              48   0.00995402        200 |\n",
            "| trainable_86a28_00003   PENDING              48   0.00135525        200 |\n",
            "| trainable_86a28_00004   PENDING              36   0.00237632       1000 |\n",
            "| trainable_86a28_00000   ERROR                24   0.000234596       200 |\n",
            "| trainable_86a28_00001   ERROR                36   0.000129884       500 |\n",
            "+-------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-03 03:00:07,776\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
            "2025-08-03 03:00:07,785\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/content/trainable_2025-08-03_02-57-00' in 0.0048s.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial status: 2 ERROR | 3 PENDING\n",
            "Current time: 2025-08-03 03:00:07. Total running time: 3min 7s\n",
            "Logical resource usage: 2.0/2 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
            "+-------------------------------------------------------------------------+\n",
            "| Trial name              status       input_size            lr     steps |\n",
            "+-------------------------------------------------------------------------+\n",
            "| trainable_86a28_00002   PENDING              48   0.00995402        200 |\n",
            "| trainable_86a28_00003   PENDING              48   0.00135525        200 |\n",
            "| trainable_86a28_00004   PENDING              36   0.00237632       1000 |\n",
            "| trainable_86a28_00000   ERROR                24   0.000234596       200 |\n",
            "| trainable_86a28_00001   ERROR                36   0.000129884       500 |\n",
            "+-------------------------------------------------------------------------+\n",
            "\n",
            "Number of errored trials: 2\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| Trial name                # failures   error file                                                                                                                                                                                                           |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "| trainable_86a28_00000              1   /tmp/ray/session_2025-08-03_02-33-24_172762_386/artifacts/2025-08-03_02-57-00/trainable_2025-08-03_02-57-00/driver_artifacts/trainable_86a28_00000_0_input_size=24,lr=0.0002,steps=200_2025-08-03_02-57-01/error.txt |\n",
            "| trainable_86a28_00001              1   /tmp/ray/session_2025-08-03_02-33-24_172762_386/artifacts/2025-08-03_02-57-00/trainable_2025-08-03_02-57-00/driver_artifacts/trainable_86a28_00001_1_input_size=36,lr=0.0001,steps=500_2025-08-03_02-57-01/error.txt |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-03 03:00:09,106\tERROR tune.py:1037 -- Trials did not complete: [trainable_86a28_00000, trainable_86a28_00001]\n",
            "2025-08-03 03:00:09,108\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
            "Resume experiment with: tune.run(..., resume=True)\n",
            "2025-08-03 03:00:09,118\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 3 trial(s):\n",
            "- trainable_86a28_00002: FileNotFoundError('Could not fetch metrics for trainable_86a28_00002: both result.json and progress.csv were not found at /content/trainable_2025-08-03_02-57-00/trainable_86a28_00002_2_input_size=48,lr=0.0100,steps=200_2025-08-03_02-57-01')\n",
            "- trainable_86a28_00003: FileNotFoundError('Could not fetch metrics for trainable_86a28_00003: both result.json and progress.csv were not found at /content/trainable_2025-08-03_02-57-00/trainable_86a28_00003_3_input_size=48,lr=0.0014,steps=200_2025-08-03_02-57-01')\n",
            "- trainable_86a28_00004: FileNotFoundError('Could not fetch metrics for trainable_86a28_00004: both result.json and progress.csv were not found at /content/trainable_2025-08-03_02-57-00/trainable_86a28_00004_4_input_size=36,lr=0.0024,steps=1000_2025-08-03_02-57-01')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "To fetch the `best_config`, pass a `metric` and `mode` parameter to `tune.run()`. Alternatively, use the `get_best_config(metric, mode)` method to set the metric and mode explicitly.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-253945223.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 3. Optional hyperparameter tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mbest_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecast_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# 4. Train final model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-519843353.py\u001b[0m in \u001b[0;36mtune_hyperparameters\u001b[0;34m(df, h)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best config: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ray/tune/analysis/experiment_analysis.py\u001b[0m in \u001b[0;36mbest_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \"\"\"\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_metric\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    241\u001b[0m                 \u001b[0;34m\"To fetch the `best_config`, pass a `metric` and `mode` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                 \u001b[0;34m\"parameter to `tune.run()`. Alternatively, use the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: To fetch the `best_config`, pass a `metric` and `mode` parameter to `tune.run()`. Alternatively, use the `get_best_config(metric, mode)` method to set the metric and mode explicitly."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "Etp7Dy11FnCn"
      },
      "id": "Etp7Dy11FnCn"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from neuralforecast import NeuralForecast\n",
        "from neuralforecast.models import NBEATS, PatchTST\n",
        "from itertools import product\n",
        "import joblib\n",
        "import logging\n",
        "import os\n",
        "# Configure logging\n",
        "logging.getLogger().setLevel(logging.WARNING)\n",
        "logging.getLogger(\"neuralforecast\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"lightning_fabric\").setLevel(logging.WARNING)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "def prepare_data_for_nbeats():\n",
        "    train = pd.read_csv('train.csv')\n",
        "    test = pd.read_csv('test.csv')\n",
        "\n",
        "    train['Date'] = pd.to_datetime(train['Date'])\n",
        "    test['Date'] = pd.to_datetime(test['Date'])\n",
        "\n",
        "    train['unique_id'] = train['Store'].astype(str) + '_' + train['Dept'].astype(str)\n",
        "    test['unique_id'] = test['Store'].astype(str) + '_' + test['Dept'].astype(str)\n",
        "\n",
        "    train_nbeats = train[['unique_id', 'Date', 'Weekly_Sales']].copy()\n",
        "    train_nbeats.columns = ['unique_id', 'ds', 'y']\n",
        "    train_nbeats = train_nbeats.sort_values(['unique_id', 'ds'])\n",
        "\n",
        "    # Updated fillna warning fix\n",
        "    train_nbeats['y'] = train_nbeats.groupby('unique_id')['y'].ffill()\n",
        "    train_nbeats['y'] = train_nbeats.groupby('unique_id')['y'].transform(lambda x: x.fillna(x.median()))\n",
        "\n",
        "    min_length = 104\n",
        "    series_lengths = train_nbeats.groupby('unique_id').size()\n",
        "    valid_series = series_lengths[series_lengths >= min_length].index\n",
        "    train_nbeats = train_nbeats[train_nbeats['unique_id'].isin(valid_series)]\n",
        "\n",
        "    # Split into training and validation (last 12 weeks for validation)\n",
        "    horizon = 12\n",
        "    X_train = train_nbeats.groupby('unique_id').apply(lambda g: g.iloc[:-horizon]).reset_index(drop=True)\n",
        "    X_valid = train_nbeats.groupby('unique_id').apply(lambda g: g.iloc[-horizon:]).reset_index(drop=True)\n",
        "\n",
        "    y_train = X_train['y'].values\n",
        "    y_valid = X_valid['y'].values\n",
        "\n",
        "    test_nbeats = test[['unique_id', 'Date']].copy()\n",
        "    test_nbeats.columns = ['unique_id', 'ds']\n",
        "    test_nbeats = test_nbeats[test_nbeats['unique_id'].isin(valid_series)]\n",
        "\n",
        "    return X_train, y_train, X_valid, y_valid, test_nbeats\n",
        "\n",
        "\n",
        "# Load data\n",
        "X_train, y_train, X_valid, y_valid, test_data = prepare_data_for_nbeats()\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_valid shape: {X_valid.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvq3bEpZFmkd",
        "outputId": "d9bb4665-df47-45c9-9df3-cfa71941d3d4"
      },
      "id": "tvq3bEpZFmkd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1462562671.py:52: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  X_train = train_nbeats.groupby('unique_id').apply(lambda g: g.iloc[:-horizon]).reset_index(drop=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (372569, 3)\n",
            "X_valid shape: (34416, 3)\n",
            "Test data shape: (110613, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1462562671.py:53: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  X_valid = train_nbeats.groupby('unique_id').apply(lambda g: g.iloc[-horizon:]).reset_index(drop=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8yLCN632HYsn"
      },
      "id": "8yLCN632HYsn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_wmae(y_true, y_pred, is_holiday):\n",
        "    \"\"\"\n",
        "    Compute the Weighted Mean Absolute Error (WMAE).\n",
        "\n",
        "    Parameters:\n",
        "    - y_true: array-like or pandas Series of true values\n",
        "    - y_pred: array-like or pandas Series of predicted values\n",
        "    - is_holiday: array-like or Series of booleans indicating if the observation is a holiday\n",
        "\n",
        "    Returns:\n",
        "    - WMAE (float)\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    is_holiday = np.array(is_holiday)\n",
        "\n",
        "    weights = np.where(is_holiday, 5, 1)\n",
        "    absolute_errors = np.abs(y_true - y_pred)\n",
        "    wmae = np.sum(weights * absolute_errors) / np.sum(weights)\n",
        "\n",
        "    return wmae\n"
      ],
      "metadata": {
        "id": "SbzZnbnmMUnz"
      },
      "id": "SbzZnbnmMUnz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "from neuralforecast.models import PatchTST\n",
        "import logging\n",
        "\n",
        "logging.getLogger().setLevel(logging.WARNING)\n",
        "logging.getLogger(\"neuralforecast\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"lightning_fabric\").setLevel(logging.WARNING)\n",
        "\n",
        "def run_nbeats_cv(X_train, y_train, X_valid, y_valid,\n",
        "                            param_grid,\n",
        "                            fixed_params,\n",
        "                            return_all=False):\n",
        "    results = []\n",
        "\n",
        "    keys, values = zip(*param_grid.items())\n",
        "    for vals in product(*values):\n",
        "        params = dict(zip(keys, vals))\n",
        "        params.update(fixed_params)\n",
        "\n",
        "        params['enable_progress_bar'] = False\n",
        "        params['enable_model_summary'] = False\n",
        "\n",
        "        model = NBEATS(**params)\n",
        "\n",
        "        nf_model = NeuralForecastModels(models=[model], model_names=['NBEATS'], freq='W-FRI', one_model=True)\n",
        "        nf_model.fit(X_train, y_train)\n",
        "        y_pred = nf_model.predict(X_valid)\n",
        "        score = compute_wmae(y_valid, y_pred, X_valid['IsHoliday'])\n",
        "\n",
        "        result = {'wmae': score, 'preds': y_pred}\n",
        "        result.update(params)\n",
        "\n",
        "        results.append(result)\n",
        "        print(\" → \".join(f\"{k}={v}\" for k,v in params.items() if k not in ['enable_progress_bar','enable_model_summary']) + f\" → WMAE={score:.4f}\")\n",
        "\n",
        "    if return_all:\n",
        "        return results\n",
        "    else:\n",
        "        return min(results, key=lambda r: r['wmae'])\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'input_size' : [40,52,60,72]\n",
        "}\n",
        "\n",
        "fixed_params = {\n",
        "    'max_steps': 25 * 104,\n",
        "    'h': 53,\n",
        "    'random_seed': 42,\n",
        "    'batch_size' : 64,\n",
        "}\n",
        "\n",
        "best_result = run_nbeats_cv(\n",
        "    X_train, y_train, X_valid, y_valid,\n",
        "    param_grid=param_grid,\n",
        "    fixed_params=fixed_params,\n",
        "    return_all=False\n",
        ")\n",
        "\n",
        "print(\"\\nBest hyperparameters found:\")\n",
        "for param in param_grid.keys():\n",
        "    print(f\"  {param}: {best_result[param]}\")\n",
        "print(f\"Best WMAE: {best_result['wmae']:.4f}\")\n"
      ],
      "metadata": {
        "id": "XO3h1CLVMBtW",
        "outputId": "f98c1cf5-545c-45f6-d0d5-c0811bca27a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "id": "XO3h1CLVMBtW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Date'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Date'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2949142829.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m }\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m best_result = run_nbeats_cv(\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2949142829.py\u001b[0m in \u001b[0;36mrun_nbeats_cv\u001b[0;34m(X_train, y_train, X_valid, y_valid, param_grid, fixed_params, return_all)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mnf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralForecastModels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NBEATS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'W-FRI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mnf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_wmae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'IsHoliday'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2684743597.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralForecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2684743597.py\u001b[0m in \u001b[0;36m_prepare_df\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prepare_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ds'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unique_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Date'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import mlflow.sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import tempfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from neuralforecast import NeuralForecast\n",
        "from neuralforecast.models import NBEATS\n",
        "from itertools import product\n",
        "import logging\n",
        "\n",
        "# Add the compute_wmae function if not already defined\n",
        "def compute_wmae(y_true, y_pred, is_holiday):\n",
        "    \"\"\"\n",
        "    Compute the Weighted Mean Absolute Error (WMAE).\n",
        "\n",
        "    Parameters:\n",
        "    - y_true: array-like or pandas Series of true values\n",
        "    - y_pred: array-like or pandas Series of predicted values\n",
        "    - is_holiday: array-like or Series of booleans indicating if the observation is a holiday\n",
        "\n",
        "    Returns:\n",
        "    - WMAE (float)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        y_true = np.array(y_true)\n",
        "        y_pred = np.array(y_pred)\n",
        "        is_holiday = np.array(is_holiday)\n",
        "\n",
        "        # Handle case where arrays have different lengths\n",
        "        min_len = min(len(y_true), len(y_pred), len(is_holiday))\n",
        "        y_true = y_true[:min_len]\n",
        "        y_pred = y_pred[:min_len]\n",
        "        is_holiday = is_holiday[:min_len]\n",
        "\n",
        "        weights = np.where(is_holiday.astype(bool), 5, 1)\n",
        "        absolute_errors = np.abs(y_true - y_pred)\n",
        "\n",
        "        # Handle edge case where all weights are zero\n",
        "        total_weights = np.sum(weights)\n",
        "        if total_weights == 0:\n",
        "            return np.mean(absolute_errors)\n",
        "\n",
        "        wmae = np.sum(weights * absolute_errors) / total_weights\n",
        "        return wmae\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing WMAE: {e}\")\n",
        "        return np.mean(np.abs(y_true - y_pred)) if len(y_true) > 0 else float('inf')\n",
        "\n",
        "# Configure MLflow\n",
        "def setup_mlflow(experiment_name=\"walmart-nbeats-forecasting\", tracking_uri=None):\n",
        "    \"\"\"Setup MLflow experiment and tracking\"\"\"\n",
        "    if tracking_uri:\n",
        "        mlflow.set_tracking_uri(tracking_uri)\n",
        "\n",
        "    # Set or create experiment\n",
        "    try:\n",
        "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
        "        if experiment is None:\n",
        "            experiment_id = mlflow.create_experiment(experiment_name)\n",
        "        else:\n",
        "            experiment_id = experiment.experiment_id\n",
        "        mlflow.set_experiment(experiment_name)\n",
        "        print(f\"Using MLflow experiment: {experiment_name}\")\n",
        "        return experiment_id\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting up MLflow: {e}\")\n",
        "        return None\n",
        "\n",
        "class MLflowNeuralForecastLogger:\n",
        "    \"\"\"Enhanced MLflow logger for Neural Forecasting experiments\"\"\"\n",
        "\n",
        "    def __init__(self, experiment_name=\"walmart-nbeats-forecasting\", auto_log=True):\n",
        "        self.experiment_name = experiment_name\n",
        "        self.auto_log = auto_log\n",
        "        self.experiment_id = setup_mlflow(experiment_name)\n",
        "\n",
        "    def log_hyperparameters(self, params_dict):\n",
        "        \"\"\"Log hyperparameters to MLflow\"\"\"\n",
        "        for key, value in params_dict.items():\n",
        "            # Handle special types that MLflow can't serialize directly\n",
        "            if isinstance(value, torch.optim.Optimizer):\n",
        "                mlflow.log_param(key, value.__class__.__name__)\n",
        "            elif callable(value):\n",
        "                mlflow.log_param(key, value.__name__ if hasattr(value, '__name__') else str(value))\n",
        "            elif isinstance(value, (list, dict)):\n",
        "                mlflow.log_param(key, json.dumps(value))\n",
        "            else:\n",
        "                mlflow.log_param(key, value)\n",
        "\n",
        "    def log_metrics(self, metrics_dict, step=None):\n",
        "        \"\"\"Log metrics to MLflow\"\"\"\n",
        "        for key, value in metrics_dict.items():\n",
        "            try:\n",
        "                # Ensure value is a valid number\n",
        "                if pd.isna(value) or not np.isfinite(value):\n",
        "                    print(f\"Warning: Invalid metric value for {key}: {value}\")\n",
        "                    continue\n",
        "                mlflow.log_metric(key, float(value), step=step)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not log metric {key}: {e}\")\n",
        "\n",
        "    def log_data_info(self, X_train=None, y_train=None, X_valid=None, y_valid=None, test_data=None):\n",
        "        \"\"\"Log dataset information with robust date handling\"\"\"\n",
        "        data_info = {}\n",
        "\n",
        "        if X_train is not None:\n",
        "            try:\n",
        "                data_info[\"train_samples\"] = len(X_train)\n",
        "                data_info[\"train_features\"] = X_train.shape[1] if hasattr(X_train, 'shape') else len(X_train.columns) if hasattr(X_train, 'columns') else 0\n",
        "\n",
        "                # Handle Date column more robustly\n",
        "                date_columns = []\n",
        "                if hasattr(X_train, 'columns'):\n",
        "                    # Look for common date column names\n",
        "                    possible_date_cols = ['Date', 'date', 'DATE', 'ds', 'timestamp', 'time']\n",
        "                    date_columns = [col for col in possible_date_cols if col in X_train.columns]\n",
        "\n",
        "                if date_columns:\n",
        "                    date_col = date_columns[0]  # Use the first found date column\n",
        "                    data_info[\"train_date_range\"] = safe_date_range(X_train[date_col])\n",
        "                    data_info[\"date_column_used\"] = date_col\n",
        "                else:\n",
        "                    data_info[\"train_date_range\"] = \"No date column found\"\n",
        "                    data_info[\"available_columns\"] = list(X_train.columns) if hasattr(X_train, 'columns') else \"Non-DataFrame input\"\n",
        "\n",
        "                # Handle Store column\n",
        "                if hasattr(X_train, 'columns') and 'Store' in X_train.columns:\n",
        "                    data_info[\"unique_stores\"] = X_train['Store'].nunique()\n",
        "                elif hasattr(X_train, 'columns') and 'store' in X_train.columns:\n",
        "                    data_info[\"unique_stores\"] = X_train['store'].nunique()\n",
        "\n",
        "                # Handle Dept column\n",
        "                if hasattr(X_train, 'columns') and 'Dept' in X_train.columns:\n",
        "                    data_info[\"unique_depts\"] = X_train['Dept'].nunique()\n",
        "                elif hasattr(X_train, 'columns') and 'dept' in X_train.columns:\n",
        "                    data_info[\"unique_depts\"] = X_train['dept'].nunique()\n",
        "\n",
        "                # Handle unique series calculation\n",
        "                if hasattr(X_train, 'columns'):\n",
        "                    store_cols = [col for col in X_train.columns if col.lower() in ['store', 'Store']]\n",
        "                    dept_cols = [col for col in X_train.columns if col.lower() in ['dept', 'Dept', 'department']]\n",
        "\n",
        "                    if store_cols and dept_cols:\n",
        "                        data_info[\"unique_series\"] = len(X_train.groupby([store_cols[0], dept_cols[0]]))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error processing X_train info: {e}\")\n",
        "                data_info[\"train_processing_error\"] = str(e)\n",
        "\n",
        "        if y_train is not None:\n",
        "            try:\n",
        "                # Convert to numpy array for safe processing\n",
        "                y_train_array = np.array(y_train)\n",
        "                # Remove any infinite values for statistics\n",
        "                y_train_finite = y_train_array[np.isfinite(y_train_array)]\n",
        "\n",
        "                if len(y_train_finite) > 0:\n",
        "                    data_info[\"target_mean\"] = float(np.mean(y_train_finite))\n",
        "                    data_info[\"target_std\"] = float(np.std(y_train_finite))\n",
        "                    data_info[\"target_min\"] = float(np.min(y_train_finite))\n",
        "                    data_info[\"target_max\"] = float(np.max(y_train_finite))\n",
        "                    data_info[\"target_finite_count\"] = len(y_train_finite)\n",
        "                    data_info[\"target_total_count\"] = len(y_train_array)\n",
        "                else:\n",
        "                    data_info[\"target_processing_error\"] = \"No finite values in target\"\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error processing y_train info: {e}\")\n",
        "                data_info[\"target_processing_error\"] = str(e)\n",
        "\n",
        "        if X_valid is not None:\n",
        "            try:\n",
        "                data_info[\"valid_samples\"] = len(X_valid)\n",
        "\n",
        "                # Handle Date column for validation set\n",
        "                if hasattr(X_valid, 'columns'):\n",
        "                    possible_date_cols = ['Date', 'date', 'DATE', 'ds', 'timestamp', 'time']\n",
        "                    date_columns = [col for col in possible_date_cols if col in X_valid.columns]\n",
        "\n",
        "                    if date_columns:\n",
        "                        date_col = date_columns[0]\n",
        "                        data_info[\"valid_date_range\"] = safe_date_range(X_valid[date_col])\n",
        "                    else:\n",
        "                        data_info[\"valid_date_range\"] = \"No date column found\"\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error processing X_valid info: {e}\")\n",
        "                data_info[\"valid_processing_error\"] = str(e)\n",
        "\n",
        "        if test_data is not None:\n",
        "            try:\n",
        "                data_info[\"test_samples\"] = len(test_data)\n",
        "\n",
        "                # Handle Date column for test set\n",
        "                if hasattr(test_data, 'columns'):\n",
        "                    possible_date_cols = ['Date', 'date', 'DATE', 'ds', 'timestamp', 'time']\n",
        "                    date_columns = [col for col in possible_date_cols if col in test_data.columns]\n",
        "\n",
        "                    if date_columns:\n",
        "                        date_col = date_columns[0]\n",
        "                        data_info[\"test_date_range\"] = safe_date_range(test_data[date_col])\n",
        "                    else:\n",
        "                        data_info[\"test_date_range\"] = \"No date column found\"\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error processing test_data info: {e}\")\n",
        "                data_info[\"test_processing_error\"] = str(e)\n",
        "\n",
        "        # Log all collected data info\n",
        "        self.log_hyperparameters(data_info)\n",
        "\n",
        "        # Create and log data summary plots (only if targets are present)\n",
        "        if y_train is not None and y_valid is not None:\n",
        "            self._log_data_plots(y_train, y_valid)\n",
        "\n",
        "\n",
        "    def _log_data_plots(self, y_train, y_valid):\n",
        "        \"\"\"Create and log data visualization plots\"\"\"\n",
        "        try:\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "            # Target distribution - handle edge cases\n",
        "            y_train_clean = y_train[np.isfinite(y_train)]\n",
        "            y_valid_clean = y_valid[np.isfinite(y_valid)]\n",
        "\n",
        "            if len(y_train_clean) > 0 and len(y_valid_clean) > 0:\n",
        "                axes[0, 0].hist(y_train_clean, bins=50, alpha=0.7, label='Train')\n",
        "                axes[0, 0].hist(y_valid_clean, bins=50, alpha=0.7, label='Valid')\n",
        "                axes[0, 0].set_title('Target Distribution')\n",
        "                axes[0, 0].legend()\n",
        "                axes[0, 0].set_xlabel('Weekly Sales')\n",
        "                axes[0, 0].set_ylabel('Frequency')\n",
        "\n",
        "            # Log scale distribution - handle negative values and zeros\n",
        "            # Use log1p only for positive values, clip negatives to small positive value\n",
        "            y_train_for_log = np.maximum(y_train_clean, 0.01)  # Clip to avoid log(0)\n",
        "            y_valid_for_log = np.maximum(y_valid_clean, 0.01)\n",
        "\n",
        "            log_train = np.log1p(y_train_for_log)\n",
        "            log_valid = np.log1p(y_valid_for_log)\n",
        "\n",
        "            # Remove any remaining infinite values\n",
        "            log_train = log_train[np.isfinite(log_train)]\n",
        "            log_valid = log_valid[np.isfinite(log_valid)]\n",
        "\n",
        "            if len(log_train) > 0 and len(log_valid) > 0:\n",
        "                axes[0, 1].hist(log_train, bins=50, alpha=0.7, label='Train (log)')\n",
        "                axes[0, 1].hist(log_valid, bins=50, alpha=0.7, label='Valid (log)')\n",
        "                axes[0, 1].set_title('Log Target Distribution')\n",
        "                axes[0, 1].legend()\n",
        "                axes[0, 1].set_xlabel('Log(Weekly Sales + 1)')\n",
        "\n",
        "            # Box plots - use cleaned data\n",
        "            if len(y_train_clean) > 0 and len(y_valid_clean) > 0:\n",
        "                # Sample data if too large for visualization\n",
        "                max_samples = 10000\n",
        "                if len(y_train_clean) > max_samples:\n",
        "                    y_train_sample = np.random.choice(y_train_clean, max_samples, replace=False)\n",
        "                else:\n",
        "                    y_train_sample = y_train_clean\n",
        "\n",
        "                if len(y_valid_clean) > max_samples:\n",
        "                    y_valid_sample = np.random.choice(y_valid_clean, max_samples, replace=False)\n",
        "                else:\n",
        "                    y_valid_sample = y_valid_clean\n",
        "\n",
        "                data_for_box = pd.DataFrame({\n",
        "                    'Sales': np.concatenate([y_train_sample, y_valid_sample]),\n",
        "                    'Split': ['Train'] * len(y_train_sample) + ['Valid'] * len(y_valid_sample)\n",
        "                })\n",
        "                sns.boxplot(data=data_for_box, x='Split', y='Sales', ax=axes[1, 0])\n",
        "                axes[1, 0].set_title('Sales Distribution by Split')\n",
        "\n",
        "            # Summary stats - use original data but handle infinites\n",
        "            train_stats = {\n",
        "                'mean': np.mean(y_train_clean) if len(y_train_clean) > 0 else 0,\n",
        "                'std': np.std(y_train_clean) if len(y_train_clean) > 0 else 0,\n",
        "                'min': np.min(y_train_clean) if len(y_train_clean) > 0 else 0,\n",
        "                'max': np.max(y_train_clean) if len(y_train_clean) > 0 else 0,\n",
        "                'negative_count': np.sum(y_train < 0),\n",
        "                'zero_count': np.sum(y_train == 0),\n",
        "                'total_count': len(y_train)\n",
        "            }\n",
        "\n",
        "            valid_stats = {\n",
        "                'mean': np.mean(y_valid_clean) if len(y_valid_clean) > 0 else 0,\n",
        "                'std': np.std(y_valid_clean) if len(y_valid_clean) > 0 else 0,\n",
        "                'min': np.min(y_valid_clean) if len(y_valid_clean) > 0 else 0,\n",
        "                'max': np.max(y_valid_clean) if len(y_valid_clean) > 0 else 0,\n",
        "                'negative_count': np.sum(y_valid < 0),\n",
        "                'zero_count': np.sum(y_valid == 0),\n",
        "                'total_count': len(y_valid)\n",
        "            }\n",
        "\n",
        "            stats_text = f\"\"\"\n",
        "            Train Stats:\n",
        "            Mean: {train_stats['mean']:.2f}\n",
        "            Std: {train_stats['std']:.2f}\n",
        "            Min: {train_stats['min']:.2f}\n",
        "            Max: {train_stats['max']:.2f}\n",
        "            Negatives: {train_stats['negative_count']}\n",
        "            Zeros: {train_stats['zero_count']}\n",
        "            Total: {train_stats['total_count']}\n",
        "\n",
        "            Valid Stats:\n",
        "            Mean: {valid_stats['mean']:.2f}\n",
        "            Std: {valid_stats['std']:.2f}\n",
        "            Min: {valid_stats['min']:.2f}\n",
        "            Max: {valid_stats['max']:.2f}\n",
        "            Negatives: {valid_stats['negative_count']}\n",
        "            Zeros: {valid_stats['zero_count']}\n",
        "            Total: {valid_stats['total_count']}\n",
        "            \"\"\"\n",
        "            axes[1, 1].text(0.1, 0.1, stats_text, transform=axes[1, 1].transAxes,\n",
        "                            fontsize=9, verticalalignment='bottom', fontfamily='monospace')\n",
        "            axes[1, 1].set_title('Summary Statistics')\n",
        "            axes[1, 1].axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            mlflow.log_figure(fig, \"data_summary.png\")\n",
        "            plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not create data plots: {e}\")\n",
        "            plt.close('all')  # Clean up any open figures\n",
        "\n",
        "    def log_predictions(self, y_true, y_pred, X_valid, prefix=\"validation\"):\n",
        "        \"\"\"Log prediction results and visualizations\"\"\"\n",
        "        # Clean data - remove any infinite values\n",
        "        valid_mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
        "        y_true_clean = y_true[valid_mask]\n",
        "        y_pred_clean = y_pred[valid_mask]\n",
        "\n",
        "        if len(y_true_clean) == 0:\n",
        "            print(\"Warning: No valid predictions to evaluate\")\n",
        "            return {}\n",
        "\n",
        "        # Calculate metrics\n",
        "        mae = np.mean(np.abs(y_true_clean - y_pred_clean))\n",
        "        mse = np.mean((y_true_clean - y_pred_clean) ** 2)\n",
        "        rmse = np.sqrt(mse)\n",
        "\n",
        "        # Handle MAPE calculation carefully\n",
        "        mape_denominator = np.abs(y_true_clean) + 1e-8\n",
        "        mape = np.mean(np.abs((y_true_clean - y_pred_clean) / mape_denominator)) * 100\n",
        "\n",
        "        # Calculate WMAE\n",
        "        is_holiday = X_valid['IsHoliday'].values[valid_mask] if len(X_valid) == len(y_true) else X_valid['IsHoliday'].values\n",
        "        wmae = compute_wmae(y_true_clean, y_pred_clean, is_holiday[:len(y_true_clean)])\n",
        "\n",
        "        # Calculate R2 safely\n",
        "        r2 = 0\n",
        "        if len(set(y_pred_clean)) > 1 and len(set(y_true_clean)) > 1:\n",
        "            try:\n",
        "                correlation_matrix = np.corrcoef(y_true_clean, y_pred_clean)\n",
        "                if correlation_matrix.shape == (2, 2) and np.isfinite(correlation_matrix[0, 1]):\n",
        "                    r2 = correlation_matrix[0, 1] ** 2\n",
        "            except:\n",
        "                r2 = 0\n",
        "\n",
        "        metrics = {\n",
        "            f\"{prefix}_mae\": mae,\n",
        "            f\"{prefix}_mse\": mse,\n",
        "            f\"{prefix}_rmse\": rmse,\n",
        "            f\"{prefix}_mape\": mape,\n",
        "            f\"{prefix}_wmae\": wmae,\n",
        "            f\"{prefix}_r2\": r2,\n",
        "            f\"{prefix}_valid_predictions\": len(y_true_clean),\n",
        "            f\"{prefix}_total_predictions\": len(y_true)\n",
        "        }\n",
        "\n",
        "        self.log_metrics(metrics)\n",
        "\n",
        "        # Create prediction plots\n",
        "        self._log_prediction_plots(y_true_clean, y_pred_clean, is_holiday[:len(y_true_clean)], prefix)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _log_prediction_plots(self, y_true, y_pred, is_holiday, prefix):\n",
        "        \"\"\"Create and log prediction visualization plots\"\"\"\n",
        "        try:\n",
        "            fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "            # Scatter plot: Actual vs Predicted\n",
        "            # Sample data if too large for visualization\n",
        "            max_points = 5000\n",
        "            if len(y_true) > max_points:\n",
        "                indices = np.random.choice(len(y_true), max_points, replace=False)\n",
        "                y_true_sample = y_true[indices]\n",
        "                y_pred_sample = y_pred[indices]\n",
        "            else:\n",
        "                y_true_sample = y_true\n",
        "                y_pred_sample = y_pred\n",
        "\n",
        "            axes[0, 0].scatter(y_true_sample, y_pred_sample, alpha=0.5, s=1)\n",
        "\n",
        "            # Add perfect prediction line\n",
        "            min_val = min(y_true_sample.min(), y_pred_sample.min())\n",
        "            max_val = max(y_true_sample.max(), y_pred_sample.max())\n",
        "            axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
        "            axes[0, 0].set_xlabel('Actual')\n",
        "            axes[0, 0].set_ylabel('Predicted')\n",
        "            axes[0, 0].set_title('Actual vs Predicted')\n",
        "\n",
        "            # Residuals plot\n",
        "            residuals = y_true - y_pred\n",
        "            residuals_sample = residuals[indices] if len(y_true) > max_points else residuals\n",
        "            y_pred_sample_res = y_pred[indices] if len(y_true) > max_points else y_pred\n",
        "\n",
        "            axes[0, 1].scatter(y_pred_sample_res, residuals_sample, alpha=0.5, s=1)\n",
        "            axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
        "            axes[0, 1].set_xlabel('Predicted')\n",
        "            axes[0, 1].set_ylabel('Residuals')\n",
        "            axes[0, 1].set_title('Residuals vs Predicted')\n",
        "\n",
        "            # Residuals histogram - clean data\n",
        "            residuals_clean = residuals[np.isfinite(residuals)]\n",
        "            if len(residuals_clean) > 0:\n",
        "                axes[0, 2].hist(residuals_clean, bins=50, alpha=0.7)\n",
        "                axes[0, 2].set_xlabel('Residuals')\n",
        "                axes[0, 2].set_ylabel('Frequency')\n",
        "                axes[0, 2].set_title('Residuals Distribution')\n",
        "\n",
        "            # Holiday vs Non-holiday performance\n",
        "            holiday_mask = is_holiday.astype(bool)\n",
        "            if holiday_mask.sum() > 0 and (~holiday_mask).sum() > 0:\n",
        "                holiday_mae = np.mean(np.abs(residuals[holiday_mask]))\n",
        "                non_holiday_mae = np.mean(np.abs(residuals[~holiday_mask]))\n",
        "\n",
        "                axes[1, 0].bar(['Non-Holiday', 'Holiday'], [non_holiday_mae, holiday_mae])\n",
        "                axes[1, 0].set_ylabel('MAE')\n",
        "                axes[1, 0].set_title('MAE: Holiday vs Non-Holiday')\n",
        "\n",
        "                # Log holiday metrics\n",
        "                mlflow.log_metric(f\"{prefix}_holiday_mae\", holiday_mae)\n",
        "                mlflow.log_metric(f\"{prefix}_non_holiday_mae\", non_holiday_mae)\n",
        "            else:\n",
        "                axes[1, 0].text(0.5, 0.5, 'No holiday data\\navailable',\n",
        "                               ha='center', va='center', transform=axes[1, 0].transAxes)\n",
        "                axes[1, 0].set_title('Holiday Analysis')\n",
        "\n",
        "            # Error distribution by prediction magnitude\n",
        "            if len(y_pred) > 0:\n",
        "                pred_quantiles = np.quantile(y_pred, [0.25, 0.5, 0.75])\n",
        "                low_mask = y_pred <= pred_quantiles[0]\n",
        "                mid_mask = (y_pred > pred_quantiles[0]) & (y_pred <= pred_quantiles[2])\n",
        "                high_mask = y_pred > pred_quantiles[2]\n",
        "\n",
        "                error_by_magnitude = []\n",
        "                labels = []\n",
        "\n",
        "                if low_mask.sum() > 0:\n",
        "                    error_by_magnitude.append(np.mean(np.abs(residuals[low_mask])))\n",
        "                    labels.append('Low')\n",
        "                if mid_mask.sum() > 0:\n",
        "                    error_by_magnitude.append(np.mean(np.abs(residuals[mid_mask])))\n",
        "                    labels.append('Medium')\n",
        "                if high_mask.sum() > 0:\n",
        "                    error_by_magnitude.append(np.mean(np.abs(residuals[high_mask])))\n",
        "                    labels.append('High')\n",
        "\n",
        "                if error_by_magnitude:\n",
        "                    axes[1, 1].bar(labels, error_by_magnitude)\n",
        "                    axes[1, 1].set_ylabel('MAE')\n",
        "                    axes[1, 1].set_title('MAE by Prediction Magnitude')\n",
        "\n",
        "            # Time series sample (first 1000 points)\n",
        "            sample_size = min(1000, len(y_true))\n",
        "            indices = np.arange(sample_size)\n",
        "            axes[1, 2].plot(indices, y_true[:sample_size], label='Actual', alpha=0.7, linewidth=1)\n",
        "            axes[1, 2].plot(indices, y_pred[:sample_size], label='Predicted', alpha=0.7, linewidth=1)\n",
        "            axes[1, 2].legend()\n",
        "            axes[1, 2].set_xlabel('Sample Index')\n",
        "            axes[1, 2].set_ylabel('Sales')\n",
        "            axes[1, 2].set_title(f'Sample Predictions (First {sample_size} points)')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            mlflow.log_figure(fig, f\"{prefix}_predictions.png\")\n",
        "            plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not create prediction plots: {e}\")\n",
        "            plt.close('all')  # Clean up any open figures\n",
        "\n",
        "    def log_model(self, model_wrapper, model_name=\"nbeats_model\"):\n",
        "        \"\"\"Log the trained model\"\"\"\n",
        "        try:\n",
        "            # Save model to temporary file\n",
        "            with tempfile.TemporaryDirectory() as tmp_dir:\n",
        "                model_path = os.path.join(tmp_dir, f\"{model_name}.pkl\")\n",
        "\n",
        "                # Save using joblib (more reliable for sklearn-like objects)\n",
        "                import joblib\n",
        "                joblib.dump(model_wrapper, model_path)\n",
        "\n",
        "                # Log as artifact\n",
        "                mlflow.log_artifact(model_path, \"models\")\n",
        "\n",
        "                # Also try to log as MLflow model if possible\n",
        "                try:\n",
        "                    mlflow.sklearn.log_model(\n",
        "                        model_wrapper,\n",
        "                        f\"models/{model_name}\",\n",
        "                        registered_model_name=model_name\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    print(f\"Could not log as MLflow model: {e}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error logging model: {e}\")\n",
        "\n",
        "def run_nbeats_cv_with_mlflow(X_train, y_train, X_valid, y_valid,\n",
        "                              param_grid, fixed_params,\n",
        "                              experiment_name=\"walmart-nbeats-cv\",\n",
        "                              return_all=False):\n",
        "    \"\"\"Enhanced cross-validation with comprehensive MLflow logging\"\"\"\n",
        "\n",
        "    # Setup MLflow\n",
        "    logger = MLflowNeuralForecastLogger(experiment_name)\n",
        "\n",
        "    results = []\n",
        "    keys, values = zip(*param_grid.items())\n",
        "\n",
        "    for i, vals in enumerate(product(*values)):\n",
        "        params = dict(zip(keys, vals))\n",
        "        params.update(fixed_params)\n",
        "\n",
        "        # Start MLflow run\n",
        "        with mlflow.start_run(run_name=f\"nbeats_run_{i+1}\"):\n",
        "            print(f\"\\n=== Run {i+1}/{len(list(product(*values)))} ===\")\n",
        "\n",
        "            # Log data info (only once)\n",
        "            if i == 0:\n",
        "                logger.log_data_info(X_train, y_train, X_valid, y_valid)\n",
        "\n",
        "            # Log hyperparameters\n",
        "            logger.log_hyperparameters(params)\n",
        "\n",
        "            # Prepare model parameters\n",
        "            model_params = params.copy()\n",
        "            model_params['enable_progress_bar'] = False\n",
        "            model_params['enable_model_summary'] = False\n",
        "\n",
        "            try:\n",
        "                # Train model\n",
        "                model = NBEATS(**model_params)\n",
        "                nf_model = NeuralForecastModels(\n",
        "                    models=[model],\n",
        "                    model_names=['NBEATS'],\n",
        "                    freq='W-FRI',\n",
        "                    one_model=True\n",
        "                )\n",
        "\n",
        "                # Fit and predict\n",
        "                nf_model.fit(X_train, y_train)\n",
        "                y_pred = nf_model.predict(X_valid)\n",
        "\n",
        "                # Log predictions and metrics\n",
        "                metrics = logger.log_predictions(y_valid, y_pred, X_valid, \"validation\")\n",
        "\n",
        "                # Log model\n",
        "                logger.log_model(nf_model, f\"nbeats_model_run_{i+1}\")\n",
        "\n",
        "                # Store results\n",
        "                result = {'run_id': mlflow.active_run().info.run_id}\n",
        "                result.update(metrics)\n",
        "                result.update(params)\n",
        "                result['preds'] = y_pred\n",
        "\n",
        "                results.append(result)\n",
        "\n",
        "                print(f\"WMAE: {metrics['validation_wmae']:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in run {i+1}: {e}\")\n",
        "                mlflow.log_param(\"error\", str(e))\n",
        "                continue\n",
        "\n",
        "    # Log summary of all runs\n",
        "    if results:\n",
        "        with mlflow.start_run(run_name=\"cv_summary\"):\n",
        "            best_run = min(results, key=lambda r: r['validation_wmae'])\n",
        "            worst_run = max(results, key=lambda r: r['validation_wmae'])\n",
        "\n",
        "            summary_metrics = {\n",
        "                \"cv_runs_total\": len(results),\n",
        "                \"cv_best_wmae\": best_run['validation_wmae'],\n",
        "                \"cv_worst_wmae\": worst_run['validation_wmae'],\n",
        "                \"cv_mean_wmae\": np.mean([r['validation_wmae'] for r in results]),\n",
        "                \"cv_std_wmae\": np.std([r['validation_wmae'] for r in results])\n",
        "            }\n",
        "\n",
        "            logger.log_metrics(summary_metrics)\n",
        "            logger.log_hyperparameters({\"best_params\": json.dumps({k: v for k, v in best_run.items()\n",
        "                                                                  if k in param_grid.keys()})})\n",
        "\n",
        "    if return_all:\n",
        "        return results\n",
        "    else:\n",
        "        return min(results, key=lambda r: r['validation_wmae']) if results else None\n",
        "\n",
        "# Example usage with your existing code:\n",
        "def main_experiment():\n",
        "    \"\"\"Main experiment function with MLflow logging\"\"\"\n",
        "\n",
        "    # Setup MLflow (replace with your tracking server if needed)\n",
        "    experiment_id = setup_mlflow(\"NBeats_Training\")\n",
        "\n",
        "    # Your existing parameter grids\n",
        "    param_grids = [\n",
        "        {\n",
        "            'input_size': [40, 52, 60, 72],\n",
        "            'fixed': {\n",
        "                'max_steps': 25 * 104,\n",
        "                'h': 53,\n",
        "                'random_seed': 42,\n",
        "                'batch_size': 64,\n",
        "            },\n",
        "            'name': 'tuning'\n",
        "        },\n",
        "        {\n",
        "            'batch_size': [32, 64, 128, 256, 512],\n",
        "            'fixed': {\n",
        "                'max_steps': 25 * 104,\n",
        "                'h': 53,\n",
        "                'random_seed': 42,\n",
        "                'input_size': 60,  # Use best from previous search\n",
        "            },\n",
        "            'name': 'batch_size_search'\n",
        "        },\n",
        "        {\n",
        "            'learning_rate': [1e-3, 2e-3, 4e-3],\n",
        "            'fixed': {\n",
        "                'max_steps': 25 * 104,\n",
        "                'h': 53,\n",
        "                'random_seed': 42,\n",
        "                'input_size': 60,\n",
        "                'batch_size': 64,  # Use best from previous search\n",
        "            },\n",
        "            'name': 'learning_rate_search'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for param_config in param_grids:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Running {param_config['name']}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        results = run_nbeats_cv_with_mlflow(\n",
        "            X_train, y_train, X_valid, y_valid,\n",
        "            param_grid={k: v for k, v in param_config.items() if k not in ['fixed', 'name']},\n",
        "            fixed_params=param_config['fixed'],\n",
        "            experiment_name=f\"walmart-nbeats-{param_config['name']}\",\n",
        "            return_all=True\n",
        "        )\n",
        "\n",
        "        all_results.extend(results)\n",
        "\n",
        "        # Print best result for this grid\n",
        "        if results:\n",
        "            best = min(results, key=lambda r: r['validation_wmae'])\n",
        "            print(f\"Best {param_config['name']}: WMAE = {best['validation_wmae']:.4f}\")\n",
        "\n",
        "    # Final summary\n",
        "    if all_results:\n",
        "        overall_best = min(all_results, key=lambda r: r['validation_wmae'])\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"OVERALL BEST RESULT:\")\n",
        "        print(f\"WMAE: {overall_best['validation_wmae']:.4f}\")\n",
        "        print(f\"Run ID: {overall_best['run_id']}\")\n",
        "        # print(f\"Parameters: {json.dumps({k: v for k, v in overall_best.items()\n",
        "        #                                if k in ['input_size', 'batch_size', 'learning_rate', 'max_steps']},\n",
        "        #                               indent=2)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the comprehensive experiment\n",
        "    main_experiment()"
      ],
      "metadata": {
        "id": "wZQhS3FhTzsP",
        "outputId": "936f33f2-88ae-4545-f14e-bb8f11b89351",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        }
      },
      "id": "wZQhS3FhTzsP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using MLflow experiment: NBeats_Training\n",
            "\n",
            "==================================================\n",
            "Running tuning\n",
            "==================================================\n",
            "Using MLflow experiment: walmart-nbeats-tuning\n",
            "\n",
            "=== Run 1/4 ===\n",
            "Warning: Error processing X_train info: name 'safe_date_range' is not defined\n",
            "Warning: Error processing X_valid info: name 'safe_date_range' is not defined\n",
            "Error in run 1: 'Date'\n",
            "🏃 View run nbeats_run_1 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/5/runs/2caf69930cfa46e5906c52459ba870ce\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/5\n",
            "\n",
            "=== Run 2/4 ===\n",
            "🏃 View run nbeats_run_2 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/5/runs/d9f8ffc19a6c4b3f9633c75bfa846fe7\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2077459963.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;31m# Run the comprehensive experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m     \u001b[0mmain_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2077459963.py\u001b[0m in \u001b[0;36mmain_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{'='*50}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         results = run_nbeats_cv_with_mlflow(\n\u001b[0m\u001b[1;32m    656\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'fixed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2077459963.py\u001b[0m in \u001b[0;36mrun_nbeats_cv_with_mlflow\u001b[0;34m(X_train, y_train, X_valid, y_valid, param_grid, fixed_params, experiment_name, return_all)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0;31m# Log hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0;31m# Prepare model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2077459963.py\u001b[0m in \u001b[0;36mlog_hyperparameters\u001b[0;34m(self, params_dict)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracking/fluent.py\u001b[0m in \u001b[0;36mlog_param\u001b[0;34m(key, value, synchronous)\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0mrun_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_or_start_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0msynchronous\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynchronous\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msynchronous\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mMLFLOW_ENABLE_ASYNC_LOGGING\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mMlflowClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracking/client.py\u001b[0m in \u001b[0;36mlog_param\u001b[0;34m(self, run_id, key, value, synchronous)\u001b[0m\n\u001b[1;32m   2107\u001b[0m         )\n\u001b[1;32m   2108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msynchronous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2109\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tracking_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2110\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracking/_tracking_service/client.py\u001b[0m in \u001b[0;36mlog_param\u001b[0;34m(self, run_id, key, value, synchronous)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msynchronous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/store/tracking/rest_store.py\u001b[0m in \u001b[0;36mlog_param\u001b[0;34m(self, run_id, param)\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0mLogParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_uuid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         )\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogParam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_experiment_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/store/tracking/rest_store.py\u001b[0m in \u001b[0;36m_call_endpoint\u001b[0;34m(self, api, json_body, endpoint, retry_timeout_seconds)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_METHOD_TO_INFO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mresponse_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         return call_endpoint(\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_host_creds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/utils/rest_utils.py\u001b[0m in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers, retry_timeout_seconds)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mcall_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"json\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_body\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverify_rest_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/utils/rest_utils.py\u001b[0m in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, retry_timeout_seconds, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         return _get_http_response_with_retries(\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/utils/request_utils.py\u001b[0m in \u001b[0;36m_get_http_response_with_retries\u001b[0;34m(method, url, max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status, allow_redirects, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mallow_redirects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_redirects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from neuralforecast import NeuralForecast\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "\n",
        "\n",
        "class NeuralForecastModels(BaseEstimator, RegressorMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        models,\n",
        "        model_names,\n",
        "        freq='W-FRI',\n",
        "        group_cols=['Store', 'Dept'],\n",
        "        one_model=False,\n",
        "        date_col='Date'\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Scikit-learn-style wrapper for NeuralForecast models.\n",
        "\n",
        "        Args:\n",
        "            models (list): List of NeuralForecast model instances (e.g., [NBEATS(...), RNN(...)])\n",
        "            model_names (list): List of names for the models (should match column names in forecast output)\n",
        "            freq (str): Frequency of the time series data (e.g., 'D', 'W-FRI')\n",
        "            group_cols (list): List of columns to create unique_id for each series\n",
        "            one_model (bool): Whether only one model is used (simplifies prediction output)\n",
        "            date_col (str): Name of the datetime column\n",
        "        \"\"\"\n",
        "        assert len(models) == len(model_names), \"Each model must have a corresponding name.\"\n",
        "        self.models = models\n",
        "        self.model_names = model_names\n",
        "        self.freq = freq\n",
        "        self.group_cols = group_cols\n",
        "        self.date_col = date_col\n",
        "        self.one_model = one_model\n",
        "        self.nf = None\n",
        "        self.fitted = False\n",
        "\n",
        "    def _prepare_df(self, X, y=None):\n",
        "        df = X.copy()\n",
        "        df['ds'] = df[self.date_col]\n",
        "        df['unique_id'] = df[self.group_cols].astype(str).agg('-'.join, axis=1)\n",
        "        if y is not None:\n",
        "            df['y'] = y.values if isinstance(y, pd.Series) else y\n",
        "            return df[['unique_id', 'ds', 'y']]\n",
        "        else:\n",
        "            return df[['unique_id', 'ds']]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        df = self._prepare_df(X, y)\n",
        "        self.nf = NeuralForecast(models=self.models, freq=self.freq)\n",
        "        self.nf.fit(df)\n",
        "        self.fitted = True\n",
        "        return self\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        if not self.fitted:\n",
        "            raise ValueError(\"Model is not fitted. Call fit() first.\")\n",
        "\n",
        "        test_df = self._prepare_df(X_test)\n",
        "        forecast_df = self.nf.predict()\n",
        "\n",
        "        # Merge predictions\n",
        "        predictions = {}\n",
        "        for name in self.model_names:\n",
        "            merged = test_df.merge(\n",
        "                forecast_df[['unique_id', 'ds', name]],\n",
        "                on=['unique_id', 'ds'],\n",
        "                how='left'\n",
        "            )\n",
        "            merged[name].fillna(0, inplace=True)\n",
        "            predictions[name] = merged[name]\n",
        "\n",
        "        if self.one_model:\n",
        "            return predictions[self.model_names[0]]\n",
        "        return predictions\n",
        "\n",
        "    def forecast(self):\n",
        "        if not self.fitted:\n",
        "            raise ValueError(\"Model is not fitted. Call fit() first.\")\n",
        "        return self.nf.predict()\n"
      ],
      "metadata": {
        "id": "PmyKw2BFTsWf"
      },
      "id": "PmyKw2BFTsWf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'batch_size' : [32,64,128,256,512]\n",
        "}\n",
        "\n",
        "fixed_params = {\n",
        "    'max_steps': 25 * 104,\n",
        "    'h': 53,\n",
        "    'random_seed': 42,\n",
        "    'input_size':52\n",
        "    # 'batch_size' : 64,\n",
        "}\n",
        "\n",
        "best_result = run_nbeats_cv(\n",
        "    X_train, y_train, X_valid, y_valid,\n",
        "    param_grid=param_grid,\n",
        "    fixed_params=fixed_params,\n",
        "    return_all=False\n",
        ")\n",
        "\n",
        "print(\"\\nBest hyperparameters found:\")\n",
        "for param in param_grid.keys():\n",
        "    print(f\"  {param}: {best_result[param]}\")\n",
        "print(f\"Best WMAE: {best_result['wmae']:.4f}\")"
      ],
      "metadata": {
        "id": "wYedtyaWN7k4",
        "outputId": "7d53512c-d53c-492a-ba0d-2d9b2c427f45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wYedtyaWN7k4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size=32 → max_steps=2600 → h=53 → random_seed=42 → input_size=52 → WMAE=1342.0787\n",
            "batch_size=64 → max_steps=2600 → h=53 → random_seed=42 → input_size=52 → WMAE=1339.4898\n",
            "batch_size=128 → max_steps=2600 → h=53 → random_seed=42 → input_size=52 → WMAE=1347.3169\n",
            "batch_size=256 → max_steps=2600 → h=53 → random_seed=42 → input_size=52 → WMAE=1357.4192\n",
            "batch_size=512 → max_steps=2600 → h=53 → random_seed=42 → input_size=52 → WMAE=1364.6991\n",
            "\n",
            "Best hyperparameters found:\n",
            "  batch_size: 64\n",
            "Best WMAE: 1339.4898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'learning_rate' : [1e-3,2e-3,4e-3]\n",
        "}\n",
        "\n",
        "fixed_params = {\n",
        "    'max_steps': 25 * 104,\n",
        "    'h': 53,\n",
        "    'random_seed': 42,\n",
        "    'input_size':52,\n",
        "    'batch_size' : 256,\n",
        "}\n",
        "\n",
        "best_result = run_nbeats_cv(\n",
        "    X_train, y_train, X_valid, y_valid,\n",
        "    param_grid=param_grid,\n",
        "    fixed_params=fixed_params,\n",
        "    return_all=False\n",
        ")\n",
        "\n",
        "print(\"\\nBest hyperparameters found:\")\n",
        "for param in param_grid.keys():\n",
        "    print(f\"  {param}: {best_result[param]}\")\n",
        "print(f\"Best WMAE: {best_result['wmae']:.4f}\")"
      ],
      "metadata": {
        "id": "-MQOJ2bPODLj",
        "outputId": "adf880bf-8e97-4e46-b5d5-01f7487c1942",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "id": "-MQOJ2bPODLj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning_rate=0.001 → max_steps=2600 → h=53 → random_seed=42 → input_size=52 → batch_size=256 → WMAE=1357.4192\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'exit' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    598\u001b[0m         )\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mdataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;31m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fetchers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# this will run only when no pre-fetching was done.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fetchers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/combined_loader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Sequential\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/combined_loader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/neuralforecast/tsdataset.py\u001b[0m in \u001b[0;36m_collate_fn\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 return dict(\n\u001b[0;32m---> 64\u001b[0;31m                     \u001b[0mtemporal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"temporal\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                     \u001b[0mtemporal_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"temporal_cols\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/neuralforecast/tsdataset.py\u001b[0m in \u001b[0;36m_collate_fn\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3416614903.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m best_result = run_nbeats_cv(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2949142829.py\u001b[0m in \u001b[0;36mrun_nbeats_cv\u001b[0;34m(X_train, y_train, X_valid, y_valid, param_grid, fixed_params, return_all)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mnf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralForecastModels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NBEATS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'W-FRI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mnf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_wmae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'IsHoliday'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2684743597.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralForecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/neuralforecast/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, df, static_df, val_size, use_init_models, verbose, id_col, time_col, target_col, distributed_config, prediction_intervals)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             self.models[i] = model.fit(\n\u001b[0m\u001b[1;32m    563\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistributed_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/neuralforecast/common/_base_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, val_size, test_size, random_seed, distributed_config)\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtemporal\u001b[0m \u001b[0mcross\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mbr\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m         \"\"\"\n\u001b[0;32m-> 1486\u001b[0;31m         return self._fit(\n\u001b[0m\u001b[1;32m   1487\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/neuralforecast/common/_base_model.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset, batch_size, valid_batch_size, val_size, test_size, random_seed, shuffle_train, distributed_config)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_trainer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    562\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SubprocessScriptLauncher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_sigkill_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'learning_rate' : [1e-3,2e-3,4e-3]\n",
        "}\n",
        "\n",
        "fixed_params = {\n",
        "    'max_steps': 25 * 104,\n",
        "    'h': 53,\n",
        "    'random_seed': 42,\n",
        "    'input_size':52,\n",
        "    'batch_size' : 256,\n",
        "}\n",
        "\n",
        "best_result = run_nbeats_cv(\n",
        "    X_train, y_train, X_valid, y_valid,\n",
        "    param_grid=param_grid,\n",
        "    fixed_params=fixed_params,\n",
        "    return_all=False\n",
        ")\n",
        "\n",
        "print(\"\\nBest hyperparameters found:\")\n",
        "for param in param_grid.keys():\n",
        "    print(f\"  {param}: {best_result[param]}\")\n",
        "print(f\"Best WMAE: {best_result['wmae']:.4f}\")"
      ],
      "metadata": {
        "id": "LPcpomfcOHlM"
      },
      "id": "LPcpomfcOHlM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'activation': ['LeakyReLU','ReLU', 'Tanh','PReLU'],\n",
        "    # 'stride': [1, 2, 4],\n",
        "    # 'input_size\n",
        "}\n",
        "\n",
        "fixed_params = {\n",
        "    'max_steps': 25 * 104,\n",
        "    'h': 53,\n",
        "    'random_seed': 42,\n",
        "    'input_size':52,\n",
        "    'batch_size' : 256,\n",
        "    'learning_rate': 1e-3,\n",
        "    'optimizer' : torch.optim.AdamW\n",
        "}\n",
        "\n",
        "best_result = run_nbeats_cv(\n",
        "    X_train, y_train, X_valid, y_valid,\n",
        "    param_grid=param_grid,\n",
        "    fixed_params=fixed_params,\n",
        "    return_all=False\n",
        ")\n",
        "\n",
        "print(\"\\nBest hyperparameters found:\")\n",
        "for param in param_grid.keys():\n",
        "    print(f\"  {param}: {best_result[param]}\")\n",
        "print(f\"Best WMAE: {best_result['wmae']:.4f}\")"
      ],
      "metadata": {
        "id": "op5XdHsrOK2p"
      },
      "id": "op5XdHsrOK2p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NBEATS(\n",
        "    max_steps= 25 * 104,\n",
        "    h= 53,\n",
        "    random_seed= 42,\n",
        "    input_size=52,\n",
        "    batch_size= 256,\n",
        "    learning_rate= 1e-3,\n",
        "    shared_weights=True,\n",
        "    optimizer= torch.optim.AdamW,\n",
        "    activation = 'ReLU'\n",
        ")\n",
        "nf_model = NeuralForecastModels(models=[model], model_names=['NBEATS'], freq='W-FRI', one_model=True)\n",
        "\n",
        "nf_model.fit(X_train, y_train)\n",
        "y_pred = nf_model.predict(X_valid)\n",
        "wmae = compute_wmae(y_valid, y_pred, X_valid['IsHoliday'])\n",
        "\n",
        "print(wmae)"
      ],
      "metadata": {
        "id": "NP0N8gzQOYKx"
      },
      "id": "NP0N8gzQOYKx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NBEATS(\n",
        "    max_steps= 25 * 104,\n",
        "    h= 53,\n",
        "    random_seed= 42,\n",
        "    input_size=52,\n",
        "    batch_size= 256,\n",
        "    learning_rate= 1e-3,\n",
        "    shared_weights=True,\n",
        "    optimizer= torch.optim.AdamW,\n",
        "    activation = 'ReLU'\n",
        ")\n",
        "nf_model = NeuralForecastModels(models=[model], model_names=['NBEATS'], freq='W-FRI', one_model=True)\n",
        "\n",
        "nf_model.fit(df.drop(columns='Weekly_Sales'), df['Weekly_Sales'])\n"
      ],
      "metadata": {
        "id": "hdwXkwhiOhiI"
      },
      "id": "hdwXkwhiOhiI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import joblib\n",
        "\n",
        "def log_nbeats_to_wandb(model, config_dict, val_wmae_score, run_name=\"nbeats_run\", model_filename=\"nbeats_model.pkl\"):\n",
        "    \"\"\"\n",
        "    Logs the N-BEATS model run to Weights & Biases (wandb).\n",
        "\n",
        "    Parameters:\n",
        "    - model: trained model to be saved\n",
        "    - config_dict: dictionary of hyperparameters and configurations\n",
        "    - val_wmae_score: validation WMAE score (float)\n",
        "    - run_name: name of the wandb run\n",
        "    - model_filename: filename to save the model as a joblib file\n",
        "    \"\"\"\n",
        "    # Save model\n",
        "    joblib.dump(model, model_filename)\n",
        "\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=\"Walmart Recruiting - Store Sales Forecasting\", name=run_name)\n",
        "\n",
        "    # Add scoring policy if not already included\n",
        "    config_dict.setdefault('score_metric', 'WMAE')\n",
        "    config_dict.setdefault('score_policy', {'weight on holidays': 5, 'weight on non_holidays': 1})\n",
        "\n",
        "    # Log config\n",
        "    wandb.config.update(config_dict)\n",
        "\n",
        "    # Log metric\n",
        "    wandb.log({'val_wmae': val_wmae_score})\n",
        "\n",
        "    # Log model artifact\n",
        "    artifact = wandb.Artifact(name=run_name, type=\"model\")\n",
        "    artifact.add_file(model_filename)\n",
        "    wandb.log_artifact(artifact)\n",
        "\n",
        "    # Finish run\n",
        "    wandb.finish()\n"
      ],
      "metadata": {
        "id": "F-IXh_38OzqF"
      },
      "id": "F-IXh_38OzqF",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}