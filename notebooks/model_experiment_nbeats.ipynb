{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d91bb8cb",
      "metadata": {
        "id": "d91bb8cb",
        "outputId": "c913c396-0ebf-4203-c8f7-c452bc81a049",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "37e1ecd3",
      "metadata": {
        "id": "37e1ecd3",
        "outputId": "b5be20de-f101-4f86-ae0d-7f8c0c7b7b66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (24.2)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn mlflow\n",
        "\n",
        "# Set up Kaggle API\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9a2fe659",
      "metadata": {
        "id": "9a2fe659"
      },
      "outputs": [],
      "source": [
        "# Upload your kaggle.json to Colab and run:\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fd383740",
      "metadata": {
        "id": "fd383740",
        "outputId": "f73949c6-ac4a-47d7-8a8c-72161bf10c9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 711MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "df11d311",
      "metadata": {
        "id": "df11d311",
        "outputId": "a8c6831e-7bfb-41db-9c87-571e11d1e9e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open stores.csv.zip, stores.csv.zip.zip or stores.csv.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!unzip -q train.csv.zip\n",
        "!unzip -q stores.csv.zip\n",
        "!unzip -q test.csv.zip\n",
        "!unzip -q features.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_experiment_nbeats_enhanced.ipynb\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import wandb\n",
        "import os\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib.pyplot as plt\n",
        "import dagshub\n",
        "\n",
        "# DagsHub and MLflow Setup\n",
        "def setup_tracking(project_name=\"walmart-nbeats-forecasting\", dagshub_repo_owner=\"ekvirika\", dagshub_repo_name=\"WalmartRecruiting\"):\n",
        "    \"\"\"\n",
        "    Setup MLflow, DagsHub, and Wandb tracking\n",
        "\n",
        "    Args:\n",
        "        project_name: Name for the experiment\n",
        "        dagshub_repo_owner: Your DagsHub username\n",
        "        dagshub_repo_name: Your DagsHub repository name\n",
        "    \"\"\"\n",
        "\n",
        "    # DagsHub Setup\n",
        "    dagshub.init(repo_owner=dagshub_repo_owner, repo_name=dagshub_repo_name, mlflow=True)\n",
        "\n",
        "    # Set MLflow tracking URI to DagsHub\n",
        "    os.environ['MLFLOW_TRACKING_URI'] = f'https://dagshub.com/{dagshub_repo_owner}/{dagshub_repo_name}.mlflow'\n",
        "    os.environ['MLFLOW_TRACKING_USERNAME'] = dagshub.auth.get_username()\n",
        "    os.environ['MLFLOW_TRACKING_PASSWORD'] = dagshub.auth.get_password()\n",
        "\n",
        "    # Initialize Wandb\n",
        "    wandb.init(\n",
        "        project=project_name,\n",
        "        config={\n",
        "            \"framework\": \"pytorch\",\n",
        "            \"model\": \"nbeats\",\n",
        "            \"dataset\": \"walmart_sales\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(\"✅ Tracking systems initialized:\")\n",
        "    print(f\"   - DagsHub: https://dagshub.com/{dagshub_repo_owner}/{dagshub_repo_name}\")\n",
        "    print(f\"   - MLflow: {os.environ.get('MLFLOW_TRACKING_URI')}\")\n",
        "    print(f\"   - Wandb: {wandb.run.get_url()}\")\n",
        "\n",
        "class NBEATSBlock(nn.Module):\n",
        "    def __init__(self, input_size, theta_size, basis_function, layers, layer_size):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([nn.Linear(input_size, layer_size)] +\n",
        "                                    [nn.Linear(layer_size, layer_size) for _ in range(layers-1)])\n",
        "        self.basis_function = basis_function\n",
        "        self.backcast_fc = nn.Linear(layer_size, theta_size)\n",
        "        self.forecast_fc = nn.Linear(layer_size, theta_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward through fully connected layers\n",
        "        for layer in self.layers:\n",
        "            x = torch.relu(layer(x))\n",
        "\n",
        "        # Generate theta parameters\n",
        "        backcast_theta = self.backcast_fc(x)\n",
        "        forecast_theta = self.forecast_fc(x)\n",
        "\n",
        "        # Apply basis functions\n",
        "        backcast, forecast = self.basis_function(backcast_theta, forecast_theta)\n",
        "\n",
        "        return backcast, forecast\n",
        "\n",
        "class GenericBasis(nn.Module):\n",
        "    def __init__(self, backcast_size, forecast_size):\n",
        "        super().__init__()\n",
        "        self.backcast_size = backcast_size\n",
        "        self.forecast_size = forecast_size\n",
        "\n",
        "    def forward(self, backcast_theta, forecast_theta):\n",
        "        # Generic basis function - fixed size handling\n",
        "        batch_size = backcast_theta.shape[0]\n",
        "        backcast = backcast_theta[:, :self.backcast_size]\n",
        "        forecast = forecast_theta[:, :self.forecast_size]\n",
        "\n",
        "        # Ensure correct dimensions\n",
        "        if backcast.shape[1] < self.backcast_size:\n",
        "            padding = torch.zeros(batch_size, self.backcast_size - backcast.shape[1], device=backcast.device)\n",
        "            backcast = torch.cat([backcast, padding], dim=1)\n",
        "\n",
        "        if forecast.shape[1] < self.forecast_size:\n",
        "            padding = torch.zeros(batch_size, self.forecast_size - forecast.shape[1], device=forecast.device)\n",
        "            forecast = torch.cat([forecast, padding], dim=1)\n",
        "\n",
        "        return backcast, forecast\n",
        "\n",
        "class SeasonalityBasis(nn.Module):\n",
        "    def __init__(self, harmonics, backcast_size, forecast_size):\n",
        "        super().__init__()\n",
        "        self.harmonics = harmonics\n",
        "        self.backcast_size = backcast_size\n",
        "        self.forecast_size = forecast_size\n",
        "\n",
        "    def forward(self, backcast_theta, forecast_theta):\n",
        "        # Seasonality basis using Fourier series\n",
        "        batch_size = backcast_theta.shape[0]\n",
        "        device = backcast_theta.device\n",
        "\n",
        "        # Create time indices on the correct device\n",
        "        backcast_time = torch.linspace(0, 1, self.backcast_size, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
        "        forecast_time = torch.linspace(0, 1, self.forecast_size, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
        "\n",
        "        backcast = torch.zeros_like(backcast_time)\n",
        "        forecast = torch.zeros_like(forecast_time)\n",
        "\n",
        "        for i in range(self.harmonics):\n",
        "            cos_coef = backcast_theta[:, i].unsqueeze(-1)\n",
        "            sin_coef = backcast_theta[:, i + self.harmonics].unsqueeze(-1)\n",
        "\n",
        "            # Apply Fourier basis\n",
        "            backcast += (cos_coef * torch.cos(2 * np.pi * (i + 1) * backcast_time) +\n",
        "                        sin_coef * torch.sin(2 * np.pi * (i + 1) * backcast_time))\n",
        "\n",
        "            cos_coef_f = forecast_theta[:, i].unsqueeze(-1)\n",
        "            sin_coef_f = forecast_theta[:, i + self.harmonics].unsqueeze(-1)\n",
        "\n",
        "            forecast += (cos_coef_f * torch.cos(2 * np.pi * (i + 1) * forecast_time) +\n",
        "                        sin_coef_f * torch.sin(2 * np.pi * (i + 1) * forecast_time))\n",
        "\n",
        "        return backcast, forecast\n",
        "\n",
        "class TrendBasis(nn.Module):\n",
        "    def __init__(self, degree_of_polynomial, backcast_size, forecast_size):\n",
        "        super().__init__()\n",
        "        self.degree = degree_of_polynomial\n",
        "        self.backcast_size = backcast_size\n",
        "        self.forecast_size = forecast_size\n",
        "\n",
        "    def forward(self, backcast_theta, forecast_theta):\n",
        "        # Polynomial trend basis\n",
        "        batch_size = backcast_theta.shape[0]\n",
        "        device = backcast_theta.device\n",
        "\n",
        "        backcast_time = torch.linspace(-1, 1, self.backcast_size, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
        "        forecast_time = torch.linspace(-1, 1, self.forecast_size, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
        "\n",
        "        backcast = torch.zeros_like(backcast_time)\n",
        "        forecast = torch.zeros_like(forecast_time)\n",
        "\n",
        "        for i in range(self.degree + 1):\n",
        "            backcast += backcast_theta[:, i].unsqueeze(-1) * (backcast_time ** i)\n",
        "            forecast += forecast_theta[:, i].unsqueeze(-1) * (forecast_time ** i)\n",
        "\n",
        "        return backcast, forecast\n",
        "\n",
        "class NBEATS(nn.Module):\n",
        "    def __init__(self, backcast_length, forecast_length,\n",
        "                 stack_types=['generic', 'seasonality', 'trend'],\n",
        "                 nb_blocks_per_stack=3, hidden_layer_units=128,\n",
        "                 nb_harmonics=10, polynomial_degree=3):\n",
        "        super().__init__()\n",
        "        self.backcast_length = backcast_length\n",
        "        self.forecast_length = forecast_length\n",
        "        self.hidden_layer_units = hidden_layer_units\n",
        "        self.nb_blocks_per_stack = nb_blocks_per_stack\n",
        "        self.stack_types = stack_types\n",
        "\n",
        "        self.stacks = nn.ModuleList()\n",
        "\n",
        "        for stack_type in stack_types:\n",
        "            stack_blocks = nn.ModuleList()\n",
        "\n",
        "            for _ in range(nb_blocks_per_stack):\n",
        "                if stack_type == 'generic':\n",
        "                    theta_size = backcast_length + forecast_length\n",
        "                    basis_function = GenericBasis(backcast_length, forecast_length)\n",
        "                elif stack_type == 'seasonality':\n",
        "                    theta_size = 2 * nb_harmonics\n",
        "                    basis_function = SeasonalityBasis(nb_harmonics, backcast_length, forecast_length)\n",
        "                elif stack_type == 'trend':\n",
        "                    theta_size = polynomial_degree + 1\n",
        "                    basis_function = TrendBasis(polynomial_degree, backcast_length, forecast_length)\n",
        "\n",
        "                block = NBEATSBlock(\n",
        "                    input_size=backcast_length,\n",
        "                    theta_size=theta_size,\n",
        "                    basis_function=basis_function,\n",
        "                    layers=4,\n",
        "                    layer_size=hidden_layer_units\n",
        "                )\n",
        "                stack_blocks.append(block)\n",
        "\n",
        "            self.stacks.append(stack_blocks)\n",
        "\n",
        "    def forward(self, backcast):\n",
        "        # Initialize forecast with correct device\n",
        "        forecast = torch.zeros(backcast.size(0), self.forecast_length, device=backcast.device)\n",
        "\n",
        "        for stack in self.stacks:\n",
        "            for block in stack:\n",
        "                b, f = block(backcast)\n",
        "                backcast = backcast - b\n",
        "                forecast = forecast + f\n",
        "\n",
        "        return forecast\n",
        "\n",
        "class WalmartDataset(Dataset):\n",
        "    def __init__(self, data, backcast_length, forecast_length):\n",
        "        self.data = data\n",
        "        self.backcast_length = backcast_length\n",
        "        self.forecast_length = forecast_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.backcast_length - self.forecast_length + 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        backcast = self.data[idx:idx + self.backcast_length]\n",
        "        forecast = self.data[idx + self.backcast_length:idx + self.backcast_length + self.forecast_length]\n",
        "        return torch.FloatTensor(backcast), torch.FloatTensor(forecast)\n",
        "\n",
        "def calculate_metrics(actuals, predictions):\n",
        "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
        "    actuals = np.array(actuals)\n",
        "    predictions = np.array(predictions)\n",
        "\n",
        "    # Basic metrics\n",
        "    mae = mean_absolute_error(actuals, predictions)\n",
        "    mse = mean_squared_error(actuals, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # Percentage-based metrics\n",
        "    mape = mean_absolute_percentage_error(actuals, predictions)\n",
        "\n",
        "    # Handle R² calculation safely\n",
        "    try:\n",
        "        r2 = r2_score(actuals, predictions)\n",
        "    except:\n",
        "        r2 = float('nan')\n",
        "\n",
        "    # Symmetric Mean Absolute Percentage Error (SMAPE)\n",
        "    smape = 100 * np.mean(2 * np.abs(predictions - actuals) / (np.abs(actuals) + np.abs(predictions)))\n",
        "\n",
        "    # Mean Directional Accuracy (MDA)\n",
        "    if len(actuals) > 1:\n",
        "        actual_direction = np.diff(actuals) > 0\n",
        "        pred_direction = np.diff(predictions) > 0\n",
        "        mda = np.mean(actual_direction == pred_direction) * 100\n",
        "    else:\n",
        "        mda = float('nan')\n",
        "\n",
        "    # Tracking Signal (TS)\n",
        "    cumulative_error = np.sum(predictions - actuals)\n",
        "    mad = np.mean(np.abs(predictions - actuals))\n",
        "    ts = cumulative_error / mad if mad != 0 else float('nan')\n",
        "\n",
        "    # Normalized metrics\n",
        "    actual_std = np.std(actuals)\n",
        "    nrmse = rmse / actual_std if actual_std != 0 else float('nan')\n",
        "    nmae = mae / np.mean(np.abs(actuals)) if np.mean(np.abs(actuals)) != 0 else float('nan')\n",
        "\n",
        "    # Forecast bias\n",
        "    bias = np.mean(predictions - actuals)\n",
        "\n",
        "    return {\n",
        "        'MAE': mae,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAPE': mape,\n",
        "        'SMAPE': smape,\n",
        "        'R2': r2,\n",
        "        'MDA': mda,\n",
        "        'Tracking_Signal': ts,\n",
        "        'NRMSE': nrmse,\n",
        "        'NMAE': nmae,\n",
        "        'Bias': bias,\n",
        "        'Mean_Actual': np.mean(actuals),\n",
        "        'Mean_Predicted': np.mean(predictions),\n",
        "        'Std_Actual': np.std(actuals),\n",
        "        'Std_Predicted': np.std(predictions)\n",
        "    }\n",
        "\n",
        "def train_nbeats(model, train_loader, val_loader, epochs, lr, device):\n",
        "    \"\"\"\n",
        "    Enhanced training loop with comprehensive logging to MLflow, DagsHub, and Wandb.\n",
        "    \"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    # Watch model with wandb\n",
        "    wandb.watch(model, log_freq=100)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        batch_count = 0\n",
        "\n",
        "        # Training loop\n",
        "        for batch_idx, (backcast, forecast) in enumerate(train_loader):\n",
        "            # Move tensors to the correct device\n",
        "            backcast, forecast = backcast.to(device), forecast.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(backcast)\n",
        "            loss = criterion(outputs, forecast)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_train_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "            # Log batch-level metrics to wandb\n",
        "            if batch_idx % 10 == 0:  # Log every 10 batches\n",
        "                wandb.log({\n",
        "                    \"batch_train_loss\": loss.item(),\n",
        "                    \"epoch\": epoch,\n",
        "                    \"batch\": batch_idx\n",
        "                })\n",
        "\n",
        "        avg_train_loss = running_train_loss / max(1, len(train_loader))\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for backcast, forecast in val_loader:\n",
        "                # Move tensors to the correct device\n",
        "                backcast, forecast = backcast.to(device), forecast.to(device)\n",
        "\n",
        "                outputs = model(backcast)\n",
        "                loss = criterion(outputs, forecast)\n",
        "                running_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = running_val_loss / max(1, len(val_loader))\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Log epoch-level metrics to all platforms\n",
        "        metrics = {\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": avg_train_loss,\n",
        "            \"val_loss\": avg_val_loss,\n",
        "            \"learning_rate\": lr\n",
        "        }\n",
        "\n",
        "        # Log to MLflow\n",
        "        mlflow.log_metrics(metrics, step=epoch)\n",
        "\n",
        "        # Log to Wandb\n",
        "        wandb.log(metrics)\n",
        "\n",
        "        # Enhanced periodic logging\n",
        "        if epoch % 10 == 0 or epoch == 1 or epoch == epochs:\n",
        "            print(f\"[Epoch {epoch}/{epochs}] Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "            # Calculate and log additional training metrics\n",
        "            overfitting_gap = avg_val_loss - avg_train_loss\n",
        "            improvement_rate = (train_losses[0] - avg_train_loss) / train_losses[0] * 100 if epoch > 1 else 0\n",
        "\n",
        "            additional_metrics = {\n",
        "                \"overfitting_gap\": overfitting_gap,\n",
        "                \"training_improvement_rate\": improvement_rate,\n",
        "                \"epoch_batch_count\": batch_count\n",
        "            }\n",
        "\n",
        "            mlflow.log_metrics(additional_metrics, step=epoch)\n",
        "            wandb.log(additional_metrics)\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "def create_forecast_plots(actuals, predictions, sales_mean, sales_std):\n",
        "    \"\"\"Create and save forecast visualization plots with enhanced logging\"\"\"\n",
        "\n",
        "    # Create forecast comparison plot\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Plot 1: Actual vs Predicted\n",
        "    axes[0, 0].plot(actuals, label='Actual', color='blue', linewidth=2)\n",
        "    axes[0, 0].plot(predictions, label='Predicted', color='red', linewidth=2, linestyle='--')\n",
        "    axes[0, 0].set_title('Actual vs Predicted Sales')\n",
        "    axes[0, 0].set_xlabel('Time Steps')\n",
        "    axes[0, 0].set_ylabel('Sales')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Residuals\n",
        "    residuals = np.array(predictions) - np.array(actuals)\n",
        "    axes[0, 1].plot(residuals, color='green', linewidth=1)\n",
        "    axes[0, 1].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
        "    axes[0, 1].set_title('Prediction Residuals')\n",
        "    axes[0, 1].set_xlabel('Time Steps')\n",
        "    axes[0, 1].set_ylabel('Residuals (Predicted - Actual)')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: Scatter plot\n",
        "    axes[1, 0].scatter(actuals, predictions, alpha=0.6, color='purple')\n",
        "    min_val = min(min(actuals), min(predictions))\n",
        "    max_val = max(max(actuals), max(predictions))\n",
        "    axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
        "    axes[1, 0].set_xlabel('Actual Sales')\n",
        "    axes[1, 0].set_ylabel('Predicted Sales')\n",
        "    axes[1, 0].set_title('Actual vs Predicted Scatter Plot')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Distribution comparison\n",
        "    axes[1, 1].hist(actuals, bins=20, alpha=0.6, label='Actual', color='blue', density=True)\n",
        "    axes[1, 1].hist(predictions, bins=20, alpha=0.6, label='Predicted', color='red', density=True)\n",
        "    axes[1, 1].set_xlabel('Sales Values')\n",
        "    axes[1, 1].set_ylabel('Density')\n",
        "    axes[1, 1].set_title('Distribution Comparison')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('forecast_evaluation.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # Log to all platforms\n",
        "    mlflow.log_artifact('forecast_evaluation.png')\n",
        "    wandb.log({\"forecast_evaluation\": wandb.Image('forecast_evaluation.png')})\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def log_training_history(train_losses, val_losses):\n",
        "    \"\"\"Enhanced training history logging\"\"\"\n",
        "\n",
        "    # Create training history plot\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot 1: Loss curves\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    axes[0].plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
        "    axes[0].plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
        "    axes[0].set_title('Training and Validation Loss')\n",
        "    axes[0].set_xlabel('Epochs')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Loss curves (log scale)\n",
        "    axes[1].semilogy(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
        "    axes[1].semilogy(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
        "    axes[1].set_title('Training and Validation Loss (Log Scale)')\n",
        "    axes[1].set_xlabel('Epochs')\n",
        "    axes[1].set_ylabel('Log Loss')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # Log to all platforms\n",
        "    mlflow.log_artifact('training_history.png')\n",
        "    wandb.log({\"training_history\": wandb.Image('training_history.png')})\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Enhanced metrics logging\n",
        "    final_metrics = {\n",
        "        \"final_train_loss\": train_losses[-1],\n",
        "        \"final_val_loss\": val_losses[-1],\n",
        "        \"min_val_loss\": min(val_losses),\n",
        "        \"min_val_loss_epoch\": val_losses.index(min(val_losses)) + 1,\n",
        "        \"total_epochs\": len(train_losses)\n",
        "    }\n",
        "\n",
        "    # Calculate learning curve metrics\n",
        "    early_train_loss = np.mean(train_losses[:10]) if len(train_losses) >= 10 else train_losses[0]\n",
        "    late_train_loss = np.mean(train_losses[-10:]) if len(train_losses) >= 10 else train_losses[-1]\n",
        "    train_improvement = (early_train_loss - late_train_loss) / early_train_loss * 100\n",
        "\n",
        "    early_val_loss = np.mean(val_losses[:10]) if len(val_losses) >= 10 else val_losses[0]\n",
        "    late_val_loss = np.mean(val_losses[-10:]) if len(val_losses) >= 10 else val_losses[-1]\n",
        "    val_improvement = (early_val_loss - late_val_loss) / early_val_loss * 100\n",
        "\n",
        "    final_metrics.update({\n",
        "        \"train_improvement_percent\": train_improvement,\n",
        "        \"val_improvement_percent\": val_improvement,\n",
        "        \"final_train_val_gap\": val_losses[-1] - train_losses[-1],\n",
        "        \"overfitting_ratio\": val_losses[-1] / train_losses[-1],\n",
        "        \"convergence_stability\": np.std(val_losses[-10:]) if len(val_losses) >= 10 else 0\n",
        "    })\n",
        "\n",
        "    # Log to all platforms\n",
        "    mlflow.log_metrics(final_metrics)\n",
        "    wandb.log(final_metrics)\n",
        "\n",
        "def run_nbeats_experiment(dagshub_repo_owner=\"your-username\", dagshub_repo_name=\"your-repo-name\"):\n",
        "    \"\"\"Enhanced N-BEATS experiment runner with comprehensive tracking\"\"\"\n",
        "\n",
        "    # Setup all tracking systems\n",
        "    setup_tracking(\n",
        "        project_name=\"walmart-nbeats-forecasting\",\n",
        "        dagshub_repo_owner=dagshub_repo_owner,\n",
        "        dagshub_repo_name=dagshub_repo_name\n",
        "    )\n",
        "\n",
        "    # Check if CUDA is available\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load data with error handling\n",
        "    try:\n",
        "        df = pd.read_csv('train.csv')\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: train.csv file not found. Please ensure the file exists in the current directory.\")\n",
        "        return None, None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "    # MLflow experiment setup\n",
        "    mlflow.set_experiment(\"Walmart_NBEATS_Forecasting\")\n",
        "\n",
        "    with mlflow.start_run():\n",
        "        # Parameters\n",
        "        BACKCAST_LENGTH = 20\n",
        "        FORECAST_LENGTH = 4\n",
        "        BATCH_SIZE = 16\n",
        "        EPOCHS = 100\n",
        "        LEARNING_RATE = 0.001\n",
        "\n",
        "        # Enhanced parameter logging\n",
        "        params = {\n",
        "            \"backcast_length\": BACKCAST_LENGTH,\n",
        "            \"forecast_length\": FORECAST_LENGTH,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"epochs\": EPOCHS,\n",
        "            \"learning_rate\": LEARNING_RATE,\n",
        "            \"device\": str(device),\n",
        "            \"optimizer\": \"Adam\",\n",
        "            \"criterion\": \"MSELoss\",\n",
        "            \"data_source\": \"walmart_sales\",\n",
        "            \"model_architecture\": \"NBEATS\"\n",
        "        }\n",
        "\n",
        "        # Log to all platforms\n",
        "        mlflow.log_params(params)\n",
        "        wandb.config.update(params)\n",
        "\n",
        "        # Prepare dataset\n",
        "        weekly_sales = df.groupby('Date')['Weekly_Sales'].sum().values\n",
        "        print(f\"Weekly sales data points: {len(weekly_sales)}\")\n",
        "\n",
        "        # Data validation\n",
        "        min_required = BACKCAST_LENGTH + FORECAST_LENGTH\n",
        "        if len(weekly_sales) < min_required:\n",
        "            print(f\"Error: Not enough data points. Need at least {min_required}, got {len(weekly_sales)}\")\n",
        "            return None, None, None\n",
        "\n",
        "        # Normalize data\n",
        "        sales_mean = weekly_sales.mean()\n",
        "        sales_std = weekly_sales.std()\n",
        "        weekly_sales_normalized = (weekly_sales - sales_mean) / sales_std\n",
        "\n",
        "        # Log data statistics\n",
        "        data_stats = {\n",
        "            \"sales_mean\": sales_mean,\n",
        "            \"sales_std\": sales_std,\n",
        "            \"data_points\": len(weekly_sales),\n",
        "            \"sales_min\": weekly_sales.min(),\n",
        "            \"sales_max\": weekly_sales.max(),\n",
        "            \"sales_median\": np.median(weekly_sales)\n",
        "        }\n",
        "\n",
        "        mlflow.log_params(data_stats)\n",
        "        wandb.config.update(data_stats)\n",
        "\n",
        "        # Enhanced data splitting\n",
        "        train_size = int(0.7 * len(weekly_sales_normalized))\n",
        "        val_size = int(0.15 * len(weekly_sales_normalized))\n",
        "\n",
        "        train_data = weekly_sales_normalized[:train_size]\n",
        "        val_data = weekly_sales_normalized[train_size:train_size + val_size]\n",
        "        test_data = weekly_sales_normalized[train_size + val_size:]\n",
        "\n",
        "        print(f\"Train size: {len(train_data)}, Val size: {len(val_data)}, Test size: {len(test_data)}\")\n",
        "\n",
        "        # Data split validation and adjustment\n",
        "        if len(train_data) < min_required or len(val_data) < min_required:\n",
        "            print(\"Adjusting split for limited data...\")\n",
        "            train_end = int(0.6 * len(weekly_sales_normalized))\n",
        "            val_end = int(0.8 * len(weekly_sales_normalized))\n",
        "\n",
        "            train_data = weekly_sales_normalized[:train_end]\n",
        "            val_data = weekly_sales_normalized[train_end:val_end]\n",
        "            test_data = weekly_sales_normalized[val_end:]\n",
        "\n",
        "            print(f\"Adjusted split - Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
        "\n",
        "        # Final validation\n",
        "        if len(train_data) < min_required or len(val_data) < min_required:\n",
        "            print(f\"Error: Insufficient data after adjustment\")\n",
        "            return None, None, None\n",
        "\n",
        "        # Create datasets and loaders\n",
        "        train_dataset = WalmartDataset(train_data, BACKCAST_LENGTH, FORECAST_LENGTH)\n",
        "        val_dataset = WalmartDataset(val_data, BACKCAST_LENGTH, FORECAST_LENGTH)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        # Model initialization\n",
        "        model = NBEATS(\n",
        "            backcast_length=BACKCAST_LENGTH,\n",
        "            forecast_length=FORECAST_LENGTH,\n",
        "            stack_types=['trend', 'seasonality', 'generic'],\n",
        "            nb_blocks_per_stack=3,\n",
        "            hidden_layer_units=128\n",
        "        ).to(device)\n",
        "\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "        model_info = {\n",
        "            \"total_parameters\": total_params,\n",
        "            \"trainable_parameters\": trainable_params,\n",
        "            \"model_size_mb\": total_params * 4 / 1024 / 1024  # Approximate size in MB\n",
        "        }\n",
        "\n",
        "        mlflow.log_params(model_info)\n",
        "        wandb.config.update(model_info)\n",
        "\n",
        "        print(f\"Model initialized with {total_params} parameters\")\n",
        "\n",
        "        # Enhanced training with comprehensive logging\n",
        "        train_losses, val_losses = train_nbeats(model, train_loader, val_loader, EPOCHS, LEARNING_RATE, device)\n",
        "\n",
        "        # Log training history and create visualizations\n",
        "        log_training_history(train_losses, val_losses)\n",
        "\n",
        "        # Save model to all platforms\n",
        "        mlflow.pytorch.log_model(model, \"nbeats_model\")\n",
        "\n",
        "        # Save model for wandb\n",
        "        torch.save(model.state_dict(), \"nbeats_model.pth\")\n",
        "        wandb.save(\"nbeats_model.pth\")\n",
        "\n",
        "        # Enhanced testing and evaluation\n",
        "        if len(test_data) >= BACKCAST_LENGTH + FORECAST_LENGTH:\n",
        "            test_dataset = WalmartDataset(test_data, BACKCAST_LENGTH, FORECAST_LENGTH)\n",
        "            test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "            model.eval()\n",
        "            predictions = []\n",
        "            actuals = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for backcast, forecast in test_loader:\n",
        "                    backcast, forecast = backcast.to(device), forecast.to(device)\n",
        "                    pred = model(backcast)\n",
        "\n",
        "                    # Denormalize predictions and actuals\n",
        "                    pred_denorm = pred.cpu().numpy() * sales_std + sales_mean\n",
        "                    actual_denorm = forecast.cpu().numpy() * sales_std + sales_mean\n",
        "\n",
        "                    predictions.extend(pred_denorm.flatten())\n",
        "                    actuals.extend(actual_denorm.flatten())\n",
        "\n",
        "            # Calculate comprehensive metrics\n",
        "            metrics = calculate_metrics(actuals, predictions)\n",
        "\n",
        "            # Log all metrics to all platforms\n",
        "            test_metrics = {}\n",
        "            for metric_name, metric_value in metrics.items():\n",
        "                if not np.isnan(metric_value):\n",
        "                    test_metric_name = f\"test_{metric_name.lower()}\"\n",
        "                    test_metrics[test_metric_name] = metric_value\n",
        "\n",
        "            mlflow.log_metrics(test_metrics)\n",
        "            wandb.log(test_metrics)\n",
        "\n",
        "            # Create summary table for wandb\n",
        "            results_table = wandb.Table(\n",
        "                columns=[\"Metric\", \"Value\", \"Description\"],\n",
        "                data=[\n",
        "                    [\"MAE\", f\"{metrics['MAE']:.2f}\", \"Mean Absolute Error\"],\n",
        "                    [\"RMSE\", f\"{metrics['RMSE']:.2f}\", \"Root Mean Square Error\"],\n",
        "                    [\"MAPE\", f\"{metrics['MAPE']:.2f}%\", \"Mean Absolute Percentage Error\"],\n",
        "                    [\"SMAPE\", f\"{metrics['SMAPE']:.2f}%\", \"Symmetric MAPE\"],\n",
        "                    [\"R²\", f\"{metrics['R2']:.4f}\", \"Coefficient of Determination\"],\n",
        "                    [\"MDA\", f\"{metrics['MDA']:.2f}%\", \"Mean Directional Accuracy\"],\n",
        "                    [\"Bias\", f\"{metrics['Bias']:.2f}\", \"Forecast Bias\"],\n",
        "                    [\"NRMSE\", f\"{metrics['NRMSE']:.4f}\", \"Normalized RMSE\"],\n",
        "                    [\"Tracking Signal\", f\"{metrics['Tracking_Signal']:.4f}\", \"Cumulative Error / MAD\"]\n",
        "                ]\n",
        "            )\n",
        "            wandb.log({\"test_metrics_summary\": results_table})\n",
        "\n",
        "            # Print comprehensive results\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"🎯 TEST SET EVALUATION RESULTS\")\n",
        "            print(f\"{'='*50}\")\n",
        "            print(f\"📊 Accuracy Metrics:\")\n",
        "            print(f\"   • MAE (Mean Absolute Error): {metrics['MAE']:.2f}\")\n",
        "            print(f\"   • RMSE (Root Mean Square Error): {metrics['RMSE']:.2f}\")\n",
        "            print(f\"   • R² (Coefficient of Determination): {metrics['R2']:.4f}\")\n",
        "            print(f\"\\n📈 Percentage-based Metrics:\")\n",
        "            print(f\"   • MAPE (Mean Absolute Percentage Error): {metrics['MAPE']:.2f}%\")\n",
        "            print(f\"   • SMAPE (Symmetric MAPE): {metrics['SMAPE']:.2f}%\")\n",
        "            print(f\"   • MDA (Mean Directional Accuracy): {metrics['MDA']:.2f}%\")\n",
        "            print(f\"\\n⚖️ Bias and Reliability:\")\n",
        "            print(f\"   • Forecast Bias: {metrics['Bias']:.2f}\")\n",
        "            print(f\"   • Tracking Signal: {metrics['Tracking_Signal']:.4f}\")\n",
        "            print(f\"   • NRMSE (Normalized RMSE): {metrics['NRMSE']:.4f}\")\n",
        "            print(f\"   • NMAE (Normalized MAE): {metrics['NMAE']:.4f}\")\n",
        "            print(f\"\\n📋 Data Statistics:\")\n",
        "            print(f\"   • Mean Actual Sales: {metrics['Mean_Actual']:.2f}\")\n",
        "            print(f\"   • Mean Predicted Sales: {metrics['Mean_Predicted']:.2f}\")\n",
        "            print(f\"   • Std Actual Sales: {metrics['Std_Actual']:.2f}\")\n",
        "            print(f\"   • Std Predicted Sales: {metrics['Std_Predicted']:.2f}\")\n",
        "            print(f\"{'='*50}\")\n",
        "\n",
        "            # Create and log comprehensive visualizations\n",
        "            create_forecast_plots(actuals, predictions, sales_mean, sales_std)\n",
        "\n",
        "            # Create additional analysis plots\n",
        "            create_advanced_analysis_plots(actuals, predictions, train_losses, val_losses)\n",
        "\n",
        "            # Log prediction vs actual data as wandb table\n",
        "            prediction_table = wandb.Table(\n",
        "                columns=[\"Time_Step\", \"Actual\", \"Predicted\", \"Error\", \"Abs_Error\", \"Pct_Error\"],\n",
        "                data=[\n",
        "                    [i, actual, pred, pred-actual, abs(pred-actual), abs(pred-actual)/actual*100 if actual != 0 else 0]\n",
        "                    for i, (actual, pred) in enumerate(zip(actuals, predictions))\n",
        "                ]\n",
        "            )\n",
        "            wandb.log({\"predictions_detailed\": prediction_table})\n",
        "\n",
        "            return model, predictions, actuals\n",
        "        else:\n",
        "            print(\"❌ Not enough test data for evaluation\")\n",
        "            return model, [], []\n",
        "\n",
        "def create_advanced_analysis_plots(actuals, predictions, train_losses, val_losses):\n",
        "    \"\"\"Create advanced analysis and diagnostic plots\"\"\"\n",
        "\n",
        "    # Create a comprehensive analysis figure\n",
        "    fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "    # Plot 1: Training convergence analysis\n",
        "    ax1 = plt.subplot(3, 3, 1)\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
        "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
        "    plt.fill_between(epochs, train_losses, val_losses, alpha=0.2, color='gray')\n",
        "    plt.title('Training Convergence Analysis')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Loss ratio analysis\n",
        "    ax2 = plt.subplot(3, 3, 2)\n",
        "    loss_ratios = [v/t if t != 0 else 1 for v, t in zip(val_losses, train_losses)]\n",
        "    plt.plot(epochs, loss_ratios, 'g-', linewidth=2)\n",
        "    plt.axhline(y=1, color='black', linestyle='--', alpha=0.5, label='Perfect Fit')\n",
        "    plt.title('Validation/Training Loss Ratio')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Val Loss / Train Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: Error distribution\n",
        "    ax3 = plt.subplot(3, 3, 3)\n",
        "    errors = np.array(predictions) - np.array(actuals)\n",
        "    plt.hist(errors, bins=20, alpha=0.7, color='purple', density=True, edgecolor='black')\n",
        "    plt.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
        "    plt.axvline(x=np.mean(errors), color='orange', linestyle='-', linewidth=2, label=f'Mean Error: {np.mean(errors):.2f}')\n",
        "    plt.title('Error Distribution')\n",
        "    plt.xlabel('Prediction Error')\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Q-Q plot for error normality\n",
        "    from scipy import stats\n",
        "    ax4 = plt.subplot(3, 3, 4)\n",
        "    stats.probplot(errors, dist=\"norm\", plot=plt)\n",
        "    plt.title('Q-Q Plot: Error Normality Check')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 5: Cumulative error\n",
        "    ax5 = plt.subplot(3, 3, 5)\n",
        "    cumsum_errors = np.cumsum(errors)\n",
        "    plt.plot(range(len(cumsum_errors)), cumsum_errors, 'r-', linewidth=2)\n",
        "    plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "    plt.title('Cumulative Prediction Error')\n",
        "    plt.xlabel('Time Steps')\n",
        "    plt.ylabel('Cumulative Error')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 6: Actual vs Predicted with confidence intervals\n",
        "    ax6 = plt.subplot(3, 3, 6)\n",
        "    time_steps = range(len(actuals))\n",
        "    plt.plot(time_steps, actuals, 'b-', label='Actual', linewidth=2, marker='o', markersize=4)\n",
        "    plt.plot(time_steps, predictions, 'r--', label='Predicted', linewidth=2, marker='s', markersize=4)\n",
        "\n",
        "    # Add confidence intervals (using standard error)\n",
        "    std_error = np.std(errors)\n",
        "    upper_bound = np.array(predictions) + 1.96 * std_error\n",
        "    lower_bound = np.array(predictions) - 1.96 * std_error\n",
        "    plt.fill_between(time_steps, lower_bound, upper_bound, alpha=0.2, color='red', label='95% CI')\n",
        "\n",
        "    plt.title('Predictions with Confidence Intervals')\n",
        "    plt.xlabel('Time Steps')\n",
        "    plt.ylabel('Sales')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 7: Percentage error over time\n",
        "    ax7 = plt.subplot(3, 3, 7)\n",
        "    pct_errors = [abs(p-a)/a*100 if a != 0 else 0 for p, a in zip(predictions, actuals)]\n",
        "    plt.plot(range(len(pct_errors)), pct_errors, 'g-', linewidth=2, marker='o', markersize=3)\n",
        "    plt.axhline(y=np.mean(pct_errors), color='red', linestyle='--', label=f'Mean: {np.mean(pct_errors):.1f}%')\n",
        "    plt.title('Absolute Percentage Error Over Time')\n",
        "    plt.xlabel('Time Steps')\n",
        "    plt.ylabel('Absolute Percentage Error (%)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 8: Autocorrelation of errors\n",
        "    ax8 = plt.subplot(3, 3, 8)\n",
        "    from statsmodels.tsa.stattools import acf\n",
        "    if len(errors) > 10:  # Need sufficient data for autocorrelation\n",
        "        lags = min(20, len(errors)//2)\n",
        "        autocorr = acf(errors, nlags=lags, fft=True)\n",
        "        plt.bar(range(len(autocorr)), autocorr, alpha=0.7, color='orange')\n",
        "        plt.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
        "        plt.axhline(y=0.2, color='red', linestyle='--', alpha=0.5, label='Significance')\n",
        "        plt.axhline(y=-0.2, color='red', linestyle='--', alpha=0.5)\n",
        "    plt.title('Error Autocorrelation')\n",
        "    plt.xlabel('Lag')\n",
        "    plt.ylabel('Autocorrelation')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 9: Model performance summary\n",
        "    ax9 = plt.subplot(3, 3, 9)\n",
        "    metrics_names = ['MAE', 'RMSE', 'MAPE', 'SMAPE', 'R²']\n",
        "    metrics_values = [\n",
        "        mean_absolute_error(actuals, predictions),\n",
        "        np.sqrt(mean_squared_error(actuals, predictions)),\n",
        "        mean_absolute_percentage_error(actuals, predictions),\n",
        "        100 * np.mean(2 * np.abs(np.array(predictions) - np.array(actuals)) /\n",
        "                     (np.abs(actuals) + np.abs(predictions))),\n",
        "        r2_score(actuals, predictions) if len(set(actuals)) > 1 else 0\n",
        "    ]\n",
        "\n",
        "    colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold', 'plum']\n",
        "    bars = plt.bar(metrics_names, metrics_values, color=colors, alpha=0.7, edgecolor='black')\n",
        "    plt.title('Model Performance Summary')\n",
        "    plt.ylabel('Metric Value')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars, metrics_values):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(metrics_values)*0.01,\n",
        "                f'{value:.2f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('advanced_analysis.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # Log to all platforms\n",
        "    mlflow.log_artifact('advanced_analysis.png')\n",
        "    wandb.log({\"advanced_analysis\": wandb.Image('advanced_analysis.png')})\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def create_model_interpretation_plots(model, sample_input, device):\n",
        "    \"\"\"Create model interpretation and feature importance plots\"\"\"\n",
        "\n",
        "    try:\n",
        "        model.eval()\n",
        "        sample_input = sample_input.to(device)\n",
        "\n",
        "        # Get model predictions and intermediate outputs\n",
        "        with torch.no_grad():\n",
        "            prediction = model(sample_input)\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "        # Plot 1: Input vs Prediction\n",
        "        axes[0, 0].plot(sample_input[0].cpu().numpy(), 'b-', label='Input (Backcast)', linewidth=2)\n",
        "        axes[0, 0].set_title('Model Input (Historical Data)')\n",
        "        axes[0, 0].set_xlabel('Time Steps')\n",
        "        axes[0, 0].set_ylabel('Normalized Sales')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 2: Prediction output\n",
        "        axes[0, 1].plot(prediction[0].cpu().numpy(), 'r-', label='Prediction (Forecast)', linewidth=2, marker='o')\n",
        "        axes[0, 1].set_title('Model Prediction (Forecast)')\n",
        "        axes[0, 1].set_xlabel('Future Time Steps')\n",
        "        axes[0, 1].set_ylabel('Normalized Sales')\n",
        "        axes[0, 1].legend()\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 3: Combined view\n",
        "        input_extended = list(sample_input[0].cpu().numpy()) + [None] * len(prediction[0])\n",
        "        pred_extended = [None] * len(sample_input[0]) + list(prediction[0].cpu().numpy())\n",
        "\n",
        "        axes[1, 0].plot(range(len(input_extended)), input_extended, 'b-', label='Historical', linewidth=2)\n",
        "        axes[1, 0].plot(range(len(pred_extended)), pred_extended, 'r-', label='Forecast', linewidth=2, marker='o')\n",
        "        axes[1, 0].axvline(x=len(sample_input[0])-1, color='gray', linestyle='--', alpha=0.7, label='Forecast Start')\n",
        "        axes[1, 0].set_title('Historical Data → Forecast')\n",
        "        axes[1, 0].set_xlabel('Time Steps')\n",
        "        axes[1, 0].set_ylabel('Normalized Sales')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 4: Model architecture summary (text-based)\n",
        "        axes[1, 1].text(0.1, 0.9, \"N-BEATS Model Architecture:\", fontsize=14, fontweight='bold', transform=axes[1, 1].transAxes)\n",
        "        axes[1, 1].text(0.1, 0.8, f\"• Backcast Length: {model.backcast_length}\", fontsize=12, transform=axes[1, 1].transAxes)\n",
        "        axes[1, 1].text(0.1, 0.7, f\"• Forecast Length: {model.forecast_length}\", fontsize=12, transform=axes[1, 1].transAxes)\n",
        "        axes[1, 1].text(0.1, 0.6, f\"• Stack Types: {model.stack_types}\", fontsize=12, transform=axes[1, 1].transAxes)\n",
        "        axes[1, 1].text(0.1, 0.5, f\"• Blocks per Stack: {model.nb_blocks_per_stack}\", fontsize=12, transform=axes[1, 1].transAxes)\n",
        "        axes[1, 1].text(0.1, 0.4, f\"• Hidden Units: {model.hidden_layer_units}\", fontsize=12, transform=axes[1, 1].transAxes)\n",
        "\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        axes[1, 1].text(0.1, 0.3, f\"• Total Parameters: {total_params:,}\", fontsize=12, transform=axes[1, 1].transAxes)\n",
        "        axes[1, 1].text(0.1, 0.2, f\"• Model Size: ~{total_params * 4 / 1024 / 1024:.1f} MB\", fontsize=12, transform=axes[1, 1].transAxes)\n",
        "\n",
        "        axes[1, 1].set_xlim(0, 1)\n",
        "        axes[1, 1].set_ylim(0, 1)\n",
        "        axes[1, 1].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('model_interpretation.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "        # Log to all platforms\n",
        "        mlflow.log_artifact('model_interpretation.png')\n",
        "        wandb.log({\"model_interpretation\": wandb.Image('model_interpretation.png')})\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Could not create model interpretation plots: {e}\")\n",
        "\n",
        "# Usage example and main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🚀 Starting Enhanced N-BEATS Experiment with MLflow, DagsHub & Wandb Integration\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Configure your credentials here\n",
        "    DAGSHUB_REPO_OWNER = \"ekvirika\"  # Replace with your DagsHub username\n",
        "    DAGSHUB_REPO_NAME = \"WalmartRecruiting\"   # Replace with your DagsHub repository name\n",
        "\n",
        "    try:\n",
        "        # Run the experiment\n",
        "        model, predictions, actuals = run_nbeats_experiment(\n",
        "            dagshub_repo_owner=DAGSHUB_REPO_OWNER,\n",
        "            dagshub_repo_name=DAGSHUB_REPO_NAME\n",
        "        )\n",
        "\n",
        "        if model is not None:\n",
        "            print(\"\\n✅ Experiment completed successfully!\")\n",
        "            print(f\"📊 Results logged to:\")\n",
        "            print(f\"   • MLflow: Check your MLflow UI\")\n",
        "            print(f\"   • DagsHub: https://dagshub.com/{DAGSHUB_REPO_OWNER}/{DAGSHUB_REPO_NAME}\")\n",
        "            print(f\"   • Wandb: Check your Wandb dashboard\")\n",
        "\n",
        "            # Create model interpretation plots if we have predictions\n",
        "            if len(predictions) > 0:\n",
        "                # Create a sample input for interpretation\n",
        "                try:\n",
        "                    df = pd.read_csv('train.csv')\n",
        "                    weekly_sales = df.groupby('Date')['Weekly_Sales'].sum().values\n",
        "                    sales_mean = weekly_sales.mean()\n",
        "                    sales_std = weekly_sales.std()\n",
        "                    weekly_sales_normalized = (weekly_sales - sales_mean) / sales_std\n",
        "\n",
        "                    # Get a sample input\n",
        "                    sample_input = torch.FloatTensor(weekly_sales_normalized[:20]).unsqueeze(0)\n",
        "                    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "                    create_model_interpretation_plots(model, sample_input, device)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Could not create interpretation plots: {e}\")\n",
        "\n",
        "        else:\n",
        "            print(\"❌ Experiment failed. Please check the error messages above.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Experiment failed with error: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Clean up wandb\n",
        "        try:\n",
        "            wandb.finish()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# Additional utility functions for experiment management\n",
        "\n",
        "def compare_experiments(experiment_names, metric='test_mae'):\n",
        "    \"\"\"\n",
        "    Compare multiple experiments from MLflow\n",
        "\n",
        "    Args:\n",
        "        experiment_names: List of experiment names to compare\n",
        "        metric: Metric to compare (default: 'test_mae')\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        import mlflow\n",
        "        from mlflow.tracking import MlflowClient\n",
        "\n",
        "        client = MlflowClient()\n",
        "\n",
        "        comparison_data = []\n",
        "\n",
        "        for exp_name in experiment_names:\n",
        "            experiment = client.get_experiment_by_name(exp_name)\n",
        "            if experiment:\n",
        "                runs = client.search_runs(experiment_ids=[experiment.experiment_id])\n",
        "\n",
        "                for run in runs:\n",
        "                    if metric in run.data.metrics:\n",
        "                        comparison_data.append({\n",
        "                            'experiment': exp_name,\n",
        "                            'run_id': run.info.run_id,\n",
        "                            'metric_value': run.data.metrics[metric],\n",
        "                            'start_time': run.info.start_time\n",
        "                        })\n",
        "\n",
        "        # Create comparison DataFrame\n",
        "        import pandas as pd\n",
        "        df_comparison = pd.DataFrame(comparison_data)\n",
        "\n",
        "        if not df_comparison.empty:\n",
        "            print(\"🔍 Experiment Comparison Results:\")\n",
        "            print(\"=\"*50)\n",
        "            summary = df_comparison.groupby('experiment')[['metric_value']].agg(['mean', 'std', 'min', 'max'])\n",
        "            print(summary)\n",
        "\n",
        "            # Create comparison plot\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            df_comparison.boxplot(column='metric_value', by='experiment')\n",
        "            plt.title(f'Experiment Comparison: {metric}')\n",
        "            plt.ylabel(metric)\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('experiment_comparison.png', dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "\n",
        "            return df_comparison\n",
        "        else:\n",
        "            print(\"No comparison data found.\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error comparing experiments: {e}\")\n",
        "        return None\n",
        "\n",
        "def export_experiment_report(run_id, output_file='experiment_report.html'):\n",
        "    \"\"\"\n",
        "    Export a comprehensive HTML report for a specific experiment run\n",
        "\n",
        "    Args:\n",
        "        run_id: MLflow run ID\n",
        "        output_file: Output HTML file name\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        from mlflow.tracking import MlflowClient\n",
        "        import json\n",
        "\n",
        "        client = MlflowClient()\n",
        "        run = client.get_run(run_id)\n",
        "\n",
        "        # Generate HTML report\n",
        "        html_content = f\"\"\"\n",
        "        <!DOCTYPE html>\n",
        "        <html>\n",
        "        <head>\n",
        "            <title>N-BEATS Experiment Report</title>\n",
        "            <style>\n",
        "                body {{ font-family: Arial, sans-serif; margin: 40px; }}\n",
        "                .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 10px; }}\n",
        "                .section {{ margin: 20px 0; }}\n",
        "                .metric {{ background-color: #e8f4f8; padding: 10px; margin: 5px 0; border-radius: 5px; }}\n",
        "                table {{ border-collapse: collapse; width: 100%; }}\n",
        "                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
        "                th {{ background-color: #f2f2f2; }}\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "            <div class=\"header\">\n",
        "                <h1>🎯 N-BEATS Experiment Report</h1>\n",
        "                <p><strong>Run ID:</strong> {run_id}</p>\n",
        "                <p><strong>Start Time:</strong> {run.info.start_time}</p>\n",
        "                <p><strong>Status:</strong> {run.info.status}</p>\n",
        "            </div>\n",
        "\n",
        "            <div class=\"section\">\n",
        "                <h2>📋 Parameters</h2>\n",
        "                <table>\n",
        "                    <tr><th>Parameter</th><th>Value</th></tr>\n",
        "        \"\"\"\n",
        "\n",
        "        for key, value in run.data.params.items():\n",
        "            html_content += f\"<tr><td>{key}</td><td>{value}</td></tr>\"\n",
        "\n",
        "        html_content += \"\"\"\n",
        "                </table>\n",
        "            </div>\n",
        "\n",
        "            <div class=\"section\">\n",
        "                <h2>📊 Metrics</h2>\n",
        "        \"\"\"\n",
        "\n",
        "        for key, value in run.data.metrics.items():\n",
        "            html_content += f'<div class=\"metric\"><strong>{key}:</strong> {value:.6f}</div>'\n",
        "\n",
        "        html_content += \"\"\"\n",
        "            </div>\n",
        "\n",
        "            <div class=\"section\">\n",
        "                <h2>📁 Artifacts</h2>\n",
        "                <ul>\n",
        "        \"\"\"\n",
        "\n",
        "        artifacts = client.list_artifacts(run_id)\n",
        "        for artifact in artifacts:\n",
        "            html_content += f\"<li>{artifact.path}</li>\"\n",
        "\n",
        "        html_content += \"\"\"\n",
        "                </ul>\n",
        "            </div>\n",
        "        </body>\n",
        "        </html>\n",
        "        \"\"\"\n",
        "\n",
        "        with open(output_file, 'w') as f:\n",
        "            f.write(html_content)\n",
        "\n",
        "        print(f\"✅ Experiment report exported to: {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error exporting report: {e}\")\n",
        "\n",
        "print(\"📚 Enhanced N-BEATS experiment script loaded successfully!\")\n",
        "print(\"🔧 Available functions:\")\n",
        "print(\"   • run_nbeats_experiment() - Main experiment runner\")\n",
        "print(\"   • compare_experiments() - Compare multiple experiments\")\n",
        "print(\"   • export_experiment_report() - Export detailed HTML report\")\n",
        "print(\"   • setup_tracking() - Initialize all tracking systems\")\n",
        "print(\"\\n💡 Don't forget to:\")\n",
        "print(\"   1. Replace DAGSHUB_REPO_OWNER and DAGSHUB_REPO_NAME with your actual values\")\n",
        "print(\"   2. Ensure you have train.csv in your working directory\")\n",
        "print(\"   3. Install required packages: mlflow, wandb, dagshub, torch, sklearn, matplotlib, scipy, statsmodels\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "O0mChVZ2qK-N",
        "outputId": "0805bd35-0d51-456f-b4f4-5ac6d53ff9ff"
      },
      "id": "O0mChVZ2qK-N",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Enhanced N-BEATS Experiment with MLflow, DagsHub & Wandb Integration\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"ekvirika/WalmartRecruiting\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"ekvirika/WalmartRecruiting\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository ekvirika/WalmartRecruiting initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository ekvirika/WalmartRecruiting initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Experiment failed with error: module 'dagshub.auth' has no attribute 'get_username'\n",
            "📚 Enhanced N-BEATS experiment script loaded successfully!\n",
            "🔧 Available functions:\n",
            "   • run_nbeats_experiment() - Main experiment runner\n",
            "   • compare_experiments() - Compare multiple experiments\n",
            "   • export_experiment_report() - Export detailed HTML report\n",
            "   • setup_tracking() - Initialize all tracking systems\n",
            "\n",
            "💡 Don't forget to:\n",
            "   1. Replace DAGSHUB_REPO_OWNER and DAGSHUB_REPO_NAME with your actual values\n",
            "   2. Ensure you have train.csv in your working directory\n",
            "   3. Install required packages: mlflow, wandb, dagshub, torch, sklearn, matplotlib, scipy, statsmodels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wand dagshub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B53a7-sore9p",
        "outputId": "45c77c0a-64e0-4adc-a24b-2db2d09b0459"
      },
      "id": "B53a7-sore9p",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wand\n",
            "  Downloading Wand-0.6.13-py2.py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting dagshub\n",
            "  Downloading dagshub-0.5.10-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: PyYAML>=5 in /usr/local/lib/python3.11/dist-packages (from dagshub) (6.0.2)\n",
            "Collecting appdirs>=1.4.4 (from dagshub)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: click>=8.0.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (8.2.1)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.28.1)\n",
            "Requirement already satisfied: GitPython>=3.1.29 in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.1.44)\n",
            "Requirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (13.9.4)\n",
            "Collecting dacite~=1.6.0 (from dagshub)\n",
            "  Downloading dacite-1.6.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from dagshub) (8.5.0)\n",
            "Collecting gql[requests] (from dagshub)\n",
            "  Downloading gql-3.5.3-py2.py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting dataclasses-json (from dagshub)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.2.2)\n",
            "Collecting treelib>=1.6.4 (from dagshub)\n",
            "  Downloading treelib-1.8.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting pathvalidate>=3.0.0 (from dagshub)\n",
            "  Downloading pathvalidate-3.3.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.9.0.post0)\n",
            "Collecting boto3 (from dagshub)\n",
            "  Downloading boto3-1.38.46-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting semver (from dagshub)\n",
            "  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting dagshub-annotation-converter>=0.1.5 (from dagshub)\n",
            "  Downloading dagshub_annotation_converter-0.1.10-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (5.4.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (11.2.1)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (4.14.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython>=3.1.29->dagshub) (4.0.12)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->dagshub) (0.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (2.19.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from treelib>=1.6.4->dagshub) (1.17.0)\n",
            "Collecting botocore<1.39.0,>=1.38.46 (from boto3->dagshub)\n",
            "  Downloading botocore-1.38.46-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->dagshub)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3->dagshub)\n",
            "  Downloading s3transfer-0.13.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->dagshub)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->dagshub)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: graphql-core<3.2.7,>=3.2 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (3.2.6)\n",
            "Requirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.20.1)\n",
            "Collecting backoff<3.0,>=1.11.1 (from gql[requests]->dagshub)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: requests<3,>=2.26 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2025.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->dagshub) (1.3.1)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.39.0,>=1.38.46->boto3->dagshub) (2.4.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython>=3.1.29->dagshub) (5.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->dagshub) (0.1.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->dagshub) (24.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.5->dagshub) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.5->dagshub) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.5->dagshub) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.26->gql[requests]->dagshub) (3.4.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (0.3.2)\n",
            "Downloading Wand-0.6.13-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dagshub-0.5.10-py3-none-any.whl (260 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading dacite-1.6.0-py3-none-any.whl (12 kB)\n",
            "Downloading dagshub_annotation_converter-0.1.10-py3-none-any.whl (33 kB)\n",
            "Downloading pathvalidate-3.3.1-py3-none-any.whl (24 kB)\n",
            "Downloading treelib-1.8.0-py3-none-any.whl (30 kB)\n",
            "Downloading boto3-1.38.46-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading semver-3.0.4-py3-none-any.whl (17 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading botocore-1.38.46-py3-none-any.whl (13.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.13.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading gql-3.5.3-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: wand, appdirs, treelib, semver, pathvalidate, mypy-extensions, marshmallow, jmespath, dacite, backoff, typing-inspect, gql, botocore, s3transfer, dataclasses-json, dagshub-annotation-converter, boto3, dagshub\n",
            "Successfully installed appdirs-1.4.4 backoff-2.2.1 boto3-1.38.46 botocore-1.38.46 dacite-1.6.0 dagshub-0.5.10 dagshub-annotation-converter-0.1.10 dataclasses-json-0.6.7 gql-3.5.3 jmespath-1.0.1 marshmallow-3.26.1 mypy-extensions-1.1.0 pathvalidate-3.3.1 s3transfer-0.13.0 semver-3.0.4 treelib-1.8.0 typing-inspect-0.9.0 wand-0.6.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "agZpOpVt0qc1"
      },
      "id": "agZpOpVt0qc1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}