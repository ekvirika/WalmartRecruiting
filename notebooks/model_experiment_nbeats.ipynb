{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d91bb8cb",
      "metadata": {
        "id": "d91bb8cb",
        "outputId": "83ab034a-c894-483b-804a-dedd13a7d697",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "37e1ecd3",
      "metadata": {
        "id": "37e1ecd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb47d306-ed90-4307-dbff-f3719573f217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-lightning==2.5.1.post0 in /usr/local/lib/python3.11/dist-packages (2.5.1.post0)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.5.1.post0) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.5.1.post0) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.5.1.post0) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning==2.5.1.post0) (2025.3.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.5.1.post0) (1.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.5.1.post0) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.5.1.post0) (4.14.1)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==2.5.1.post0) (0.15.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning==2.5.1.post0) (3.12.14)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning==2.5.1.post0) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning==2.5.1.post0) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning==2.5.1.post0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning==2.5.1.post0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning==2.5.1.post0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning==2.5.1.post0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning==2.5.1.post0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning==2.5.1.post0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning==2.5.1.post0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning==2.5.1.post0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning==2.5.1.post0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning==2.5.1.post0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning==2.5.1.post0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning==2.5.1.post0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning==2.5.1.post0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning==2.5.1.post0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning==2.5.1.post0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning==2.5.1.post0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning==2.5.1.post0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning==2.5.1.post0) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning==2.5.1.post0) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.5.1.post0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.5.1.post0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.5.1.post0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.5.1.post0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.5.1.post0) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.5.1.post0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.5.1.post0) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning==2.5.1.post0) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning==2.5.1.post0) (3.10)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q  wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn mlflow wand dagshub neuralforecast\n",
        "!pip install pytorch-lightning==2.5.1.post0\n",
        "# Set up Kaggle API\n",
        "!pip install -q kaggle ray[tune]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9a2fe659",
      "metadata": {
        "id": "9a2fe659"
      },
      "outputs": [],
      "source": [
        "# Upload your kaggle.json to Colab and run:\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "fd383740",
      "metadata": {
        "id": "fd383740",
        "outputId": "3e156bb5-332a-439f-b01f-b8d030f2c226",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "walmart-recruiting-store-sales-forecasting.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "replace features.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "!unzip -q walmart-recruiting-store-sales-forecasting.zip\n",
        "!unzip -q sampleSubmission.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "df11d311",
      "metadata": {
        "id": "df11d311",
        "outputId": "cca75919-a5e0-4a1a-ef83-f5ec027bb82e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "unzip:  cannot find or open stores.csv.zip, stores.csv.zip.zip or stores.csv.zip.ZIP.\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "replace features.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ],
      "source": [
        "!unzip -q train.csv.zip\n",
        "!unzip -q stores.csv.zip\n",
        "!unzip -q test.csv.zip\n",
        "!unzip -q features.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from dagshub import dagshub_logger\n",
        "import os\n",
        "import mlflow.pytorch\n",
        "mlflow.pytorch.autolog()\n",
        "import torch\n",
        "\n",
        "\n",
        "# Set tracking URI manually\n",
        "mlflow.set_tracking_uri(\"https://dagshub.com/ekvirika/WalmartRecruiting.mlflow\")\n",
        "\n",
        "# Use your DagsHub credentials\n",
        "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"ekvirika\"\n",
        "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"0adb1004ddd4221395353efea2d8ead625e26197\"\n",
        "\n",
        "# Optional: set registry if you're using model registry\n",
        "mlflow.set_registry_uri(\"https://dagshub.com/ekvirika/WalmartRecruiting.mlflow\")\n",
        "mlflow.set_experiment(\"NBeats_Training\")\n",
        "\n",
        "# Detect GPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# W&B setup\n",
        "wandb_project = 'WalmartRecruiting'\n",
        "wandb_entity = None  # Replace with your W&B entity if using teams\n"
      ],
      "metadata": {
        "id": "8kXR1XafxwrG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d736701-8b46-490f-b617-523881aa5b77"
      },
      "id": "8kXR1XafxwrG",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHL6uL5KyL5g",
        "outputId": "85237c0c-616d-46b3-823a-c052e69c179a"
      },
      "id": "OHL6uL5KyL5g",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mellekvirikashvili\u001b[0m (\u001b[33mellekvirikashvili-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "iJXHAHdmFkWY"
      },
      "id": "iJXHAHdmFkWY"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import wandb\n",
        "import joblib\n",
        "import mlflow\n",
        "import os\n",
        "from itertools import product\n",
        "from neuralforecast.models import NBEATS\n",
        "from neuralforecast import NeuralForecast\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.WARNING)\n",
        "for lib in [\"neuralforecast\", \"pytorch_lightning\", \"lightning_fabric\"]:\n",
        "    logging.getLogger(lib).setLevel(logging.WARNING)\n",
        "\n",
        "# --- Data loading ---\n",
        "# Replace these with your actual CSV file paths\n",
        "STORES_PATH = \"stores.csv\"\n",
        "FEATURES_PATH = \"features.csv\"\n",
        "TRAIN_PATH = \"train.csv\"\n",
        "TEST_PATH = \"test.csv\"\n",
        "\n",
        "stores = pd.read_csv(STORES_PATH)\n",
        "features = pd.read_csv(FEATURES_PATH)\n",
        "train = pd.read_csv(TRAIN_PATH)\n",
        "test = pd.read_csv(TEST_PATH)\n",
        "\n",
        "class NeuralForecastModels:\n",
        "    def __init__(self, models, model_names=None, freq='W-FRI', one_model=False):\n",
        "        self.freq = freq\n",
        "        self.one_model = one_model\n",
        "        self.models = models\n",
        "        self.model_names = model_names if model_names else [f\"model_{i}\" for i in range(len(models))]\n",
        "        self.nf = NeuralForecast(models=self.models, freq=self.freq)\n",
        "        self.fitted_df = None\n",
        "\n",
        "    def fit(self, df):\n",
        "        \"\"\"\n",
        "        Fit the model with a DataFrame containing unique_id, ds, y columns\n",
        "        \"\"\"\n",
        "        self.fitted_df = df.copy()\n",
        "        self.nf.fit(df=df)\n",
        "\n",
        "    def predict(self, h=None):\n",
        "        \"\"\"\n",
        "        Generate predictions\n",
        "        \"\"\"\n",
        "        if h is None:\n",
        "            # Use the horizon from the model\n",
        "            h = self.models[0].h\n",
        "        return self.nf.predict(h=h)\n",
        "\n",
        "    def cross_validation(self, df, n_windows=1):\n",
        "        \"\"\"\n",
        "        Perform cross-validation\n",
        "        \"\"\"\n",
        "        return self.nf.cross_validation(df=df, n_windows=n_windows)\n",
        "\n",
        "def preprocess(df):\n",
        "    \"\"\"Preprocess the data for NeuralForecast\"\"\"\n",
        "    df = df.copy()\n",
        "    df['unique_id'] = df['Store'].astype(str) + \"_\" + df['Dept'].astype(str)\n",
        "    df.rename(columns={'Date': 'ds', 'Weekly_Sales': 'y'}, inplace=True)\n",
        "    df['ds'] = pd.to_datetime(df['ds'])\n",
        "\n",
        "    # Sort by unique_id and ds for proper time series format\n",
        "    df = df.sort_values(['unique_id', 'ds']).reset_index(drop=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "def prepare_data_for_cv(df, n_windows=1, h=53):\n",
        "    \"\"\"\n",
        "    Prepare data for cross-validation by ensuring each time series has enough data\n",
        "    \"\"\"\n",
        "    min_length = h * (n_windows + 1) + 10  # minimum length needed for CV\n",
        "\n",
        "    # Filter time series that are long enough\n",
        "    series_lengths = df.groupby('unique_id').size()\n",
        "    valid_series = series_lengths[series_lengths >= min_length].index\n",
        "\n",
        "    filtered_df = df[df['unique_id'].isin(valid_series)].copy()\n",
        "\n",
        "    print(f\"Original series: {len(series_lengths)}, Valid series for CV: {len(valid_series)}\")\n",
        "    print(f\"Original data points: {len(df)}, Filtered data points: {len(filtered_df)}\")\n",
        "\n",
        "    return filtered_df\n",
        "\n",
        "# Preprocess the data\n",
        "df = preprocess(train)\n",
        "\n",
        "# Prepare data for cross-validation\n",
        "df_cv = prepare_data_for_cv(df, n_windows=1, h=53)\n",
        "\n",
        "# --- WMAE metric function ---\n",
        "def compute_wmae(y_true, y_pred, is_holiday=None, holiday_weight=5, non_holiday_weight=1):\n",
        "    \"\"\"\n",
        "    Compute Weighted Mean Absolute Error\n",
        "    \"\"\"\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # Ensure arrays have the same shape\n",
        "    if y_true.shape != y_pred.shape:\n",
        "        min_len = min(len(y_true), len(y_pred))\n",
        "        y_true = y_true[:min_len]\n",
        "        y_pred = y_pred[:min_len]\n",
        "        if is_holiday is not None:\n",
        "            is_holiday = np.array(is_holiday)[:min_len]\n",
        "\n",
        "    if is_holiday is not None:\n",
        "        weights = np.where(np.array(is_holiday), holiday_weight, non_holiday_weight)\n",
        "    else:\n",
        "        weights = np.ones_like(y_true)\n",
        "\n",
        "    abs_errors = np.abs(y_true - y_pred)\n",
        "    weighted_errors = weights * abs_errors\n",
        "\n",
        "    return weighted_errors.sum() / weights.sum()\n",
        "\n",
        "# --- Cross-validation function ---\n",
        "def run_nbeats_cv(df, param_grid, fixed_params, experiment_name=\"NBEATS_Hyperparam_Tuning\"):\n",
        "    \"\"\"\n",
        "    Run cross-validation for NBEATS hyperparameter tuning\n",
        "    \"\"\"\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "    results = []\n",
        "    keys, values = zip(*param_grid.items())\n",
        "\n",
        "    for vals in product(*values):\n",
        "        params = dict(zip(keys, vals))\n",
        "        params.update(fixed_params)\n",
        "        params.update({'enable_progress_bar': False, 'enable_model_summary': False})\n",
        "\n",
        "        try:\n",
        "            with mlflow.start_run(nested=True):\n",
        "                # Log hyperparams\n",
        "                for k, v in params.items():\n",
        "                    if not isinstance(v, (list, dict)):\n",
        "                        mlflow.log_param(k, v)\n",
        "                    else:\n",
        "                        mlflow.log_param(k, str(v))\n",
        "\n",
        "                model = NBEATS(**params)\n",
        "                nf_model = NeuralForecastModels(\n",
        "                    models=[model],\n",
        "                    model_names=['NBEATS'],\n",
        "                    freq='W-FRI',\n",
        "                    one_model=True\n",
        "                )\n",
        "\n",
        "                # Perform cross-validation - FIXED: removed h parameter\n",
        "                cv_df = nf_model.cross_validation(df, n_windows=1)\n",
        "\n",
        "                # Extract predictions and actual values\n",
        "                y_true = cv_df['y'].values\n",
        "                y_pred = cv_df['NBEATS'].values\n",
        "\n",
        "                # Get holiday information if available\n",
        "                is_holiday = None\n",
        "                if 'IsHoliday' in df.columns:\n",
        "                    # Map holiday information to CV results\n",
        "                    cv_df_with_holiday = cv_df.merge(\n",
        "                        df[['unique_id', 'ds', 'IsHoliday']],\n",
        "                        on=['unique_id', 'ds'],\n",
        "                        how='left'\n",
        "                    )\n",
        "                    is_holiday = cv_df_with_holiday['IsHoliday'].fillna(False).values\n",
        "\n",
        "                score = compute_wmae(y_true, y_pred, is_holiday)\n",
        "\n",
        "                mlflow.log_metric(\"val_wmae\", score)\n",
        "\n",
        "                param_str = \" â†’ \".join(f\"{k}={v}\" for k, v in params.items()\n",
        "                                     if k not in ['enable_progress_bar', 'enable_model_summary'])\n",
        "                print(f\"{param_str} â†’ WMAE={score:.4f}\")\n",
        "\n",
        "                results.append({'wmae': score, **params})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with params {params}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    if not results:\n",
        "        raise ValueError(\"No successful runs completed\")\n",
        "\n",
        "    return min(results, key=lambda r: r['wmae']) if len(results) > 1 else results[0]\n",
        "\n",
        "# --- Hyperparameter tuning steps ---\n",
        "print(\"Starting hyperparameter tuning...\")\n",
        "\n",
        "print(\"Tuning input_size...\")\n",
        "param_grid = {'input_size': [40, 52, 60, 72]}\n",
        "fixed_params = {\n",
        "    'max_steps': 25 * 104,\n",
        "    'h': 53,\n",
        "    'random_seed': 42,\n",
        "    'batch_size': 64,\n",
        "}\n",
        "\n",
        "best_result = run_nbeats_cv(df_cv, param_grid, fixed_params)\n",
        "print(f\"\\nBest input_size: {best_result['input_size']} with WMAE: {best_result['wmae']:.4f}\")\n",
        "\n",
        "print(\"\\nTuning batch_size...\")\n",
        "param_grid = {'batch_size': [32, 64, 128, 256]}\n",
        "fixed_params.update({'input_size': best_result['input_size']})\n",
        "best_result = run_nbeats_cv(df_cv, param_grid, fixed_params)\n",
        "print(f\"\\nBest batch_size: {best_result['batch_size']} with WMAE: {best_result['wmae']:.4f}\")\n",
        "\n",
        "print(\"\\nTuning learning_rate...\")\n",
        "param_grid = {'learning_rate': [1e-3, 2e-3, 4e-3]}\n",
        "fixed_params.update({'batch_size': best_result['batch_size']})\n",
        "best_result = run_nbeats_cv(df_cv, param_grid, fixed_params)\n",
        "print(f\"\\nBest learning_rate: {best_result['learning_rate']} with WMAE: {best_result['wmae']:.4f}\")\n",
        "\n",
        "print(\"\\nTuning n_blocks and optimizer weight_decay...\")\n",
        "param_grid = {\n",
        "    'n_blocks': [[1,1,1], [2,2,2], [3,3,3]],\n",
        "    'optimizer_kwargs': [\n",
        "        {'weight_decay': 1e-4},\n",
        "        {'weight_decay': 1e-3},\n",
        "        {'weight_decay': 1e-2}\n",
        "    ]\n",
        "}\n",
        "fixed_params.update({'learning_rate': best_result['learning_rate'], 'optimizer': torch.optim.AdamW})\n",
        "best_result = run_nbeats_cv(df_cv, param_grid, fixed_params)\n",
        "print(f\"\\nBest n_blocks: {best_result['n_blocks']} and optimizer_kwargs: {best_result['optimizer_kwargs']} with WMAE: {best_result['wmae']:.4f}\")\n",
        "\n",
        "print(\"\\nTuning activation functions...\")\n",
        "param_grid = {'activation': ['LeakyReLU', 'ReLU', 'Tanh']}\n",
        "fixed_params.update({\n",
        "    'n_blocks': best_result['n_blocks'],\n",
        "    'optimizer_kwargs': best_result['optimizer_kwargs']\n",
        "})\n",
        "best_result = run_nbeats_cv(df_cv, param_grid, fixed_params)\n",
        "print(f\"\\nBest activation: {best_result['activation']} with WMAE: {best_result['wmae']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pbkbm4lJDCh6",
        "outputId": "64da6934-f5d6-4628-a501-7e4daeca7823"
      },
      "id": "pbkbm4lJDCh6",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original series: 3331, Valid series for CV: 2827\n",
            "Original data points: 421570, Filtered data points: 402458\n",
            "Starting hyperparameter tuning...\n",
            "Tuning input_size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:33:48 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_size=40 â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ batch_size=64 â†’ WMAE=2879.1875\n",
            "ğŸƒ View run unruly-eel-19 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/4e4989b063ac40c490c0131a6b7f43b4\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:34:27 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_size=52 â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ batch_size=64 â†’ WMAE=2815.2565\n",
            "ğŸƒ View run industrious-ape-329 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/e3a209d7c8cb486a98179094142f6d44\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:35:01 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_size=60 â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ batch_size=64 â†’ WMAE=2837.5293\n",
            "ğŸƒ View run inquisitive-colt-973 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/6678dbda455a408c85321f44d157b861\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:35:33 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_size=72 â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ batch_size=64 â†’ WMAE=3005.6063\n",
            "ğŸƒ View run righteous-jay-901 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/4adb80a10aab41698d8b058a1d236364\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n",
            "\n",
            "Best input_size: 52 with WMAE: 2815.2565\n",
            "\n",
            "Tuning batch_size...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:36:12 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size=64 â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ input_size=52 â†’ WMAE=2815.2565\n",
            "ğŸƒ View run enchanting-colt-115 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/da98f6ad941544c1bc4f6a7a6773796c\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:36:49 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size=64 â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ input_size=52 â†’ WMAE=2815.2565\n",
            "ğŸƒ View run vaunted-crane-822 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/401ffaceca58463c899c2a6a8620940d\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:37:26 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size=64 â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ input_size=52 â†’ WMAE=2815.2565\n",
            "ğŸƒ View run selective-mink-84 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/97ff440f74924f5f944da7abab47d922\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:38:04 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size=64 â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ input_size=52 â†’ WMAE=2815.2565\n",
            "ğŸƒ View run angry-chimp-445 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/ce0df4b314194800a770a90abc5b4575\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n",
            "\n",
            "Best batch_size: 64 with WMAE: 2815.2565\n",
            "\n",
            "Tuning learning_rate...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:38:43 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning_rate=0.001 â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ batch_size=64 â†’ input_size=52 â†’ WMAE=2815.2565\n",
            "ğŸƒ View run legendary-cod-371 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/3764b4195d2840caaf12f53138f2b961\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:39:19 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning_rate=0.002 â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ batch_size=64 â†’ input_size=52 â†’ WMAE=2681.5312\n",
            "ğŸƒ View run blushing-shrew-420 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/1dd35cbadf5b49a89124b23825a86dd2\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:39:56 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning_rate=0.004 â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ batch_size=64 â†’ input_size=52 â†’ WMAE=2980.6578\n",
            "ğŸƒ View run overjoyed-hound-785 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/be9da4c7e671457bb21b5a53834fec05\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n",
            "\n",
            "Best learning_rate: 0.002 with WMAE: 2681.5312\n",
            "\n",
            "Tuning n_blocks and optimizer weight_decay...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:40:38 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_blocks=[1, 1, 1] â†’ optimizer_kwargs={'weight_decay': 0.0001} â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ batch_size=64 â†’ input_size=52 â†’ learning_rate=0.002 â†’ optimizer=<class 'torch.optim.adamw.AdamW'> â†’ WMAE=2818.4835\n",
            "ğŸƒ View run auspicious-hare-887 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/1091a1247a384751a2115738eea7197b\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:41:17 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_blocks=[1, 1, 1] â†’ optimizer_kwargs={'weight_decay': 0.001} â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ batch_size=64 â†’ input_size=52 â†’ learning_rate=0.002 â†’ optimizer=<class 'torch.optim.adamw.AdamW'> â†’ WMAE=2780.8924\n",
            "ğŸƒ View run big-koi-206 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/49ae74b3790843f1b2eacfa653fb1447\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:41:55 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_blocks=[1, 1, 1] â†’ optimizer_kwargs={'weight_decay': 0.01} â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ batch_size=64 â†’ input_size=52 â†’ learning_rate=0.002 â†’ optimizer=<class 'torch.optim.adamw.AdamW'> â†’ WMAE=2749.2158\n",
            "ğŸƒ View run intrigued-lark-247 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/ee86f0046f0546a3b179629f4e194f96\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:42:51 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_blocks=[2, 2, 2] â†’ optimizer_kwargs={'weight_decay': 0.0001} â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ batch_size=64 â†’ input_size=52 â†’ learning_rate=0.002 â†’ optimizer=<class 'torch.optim.adamw.AdamW'> â†’ WMAE=2765.0085\n",
            "ğŸƒ View run able-midge-592 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/759485f2b8544f00a173ef61bacb758c\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:43:45 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_blocks=[2, 2, 2] â†’ optimizer_kwargs={'weight_decay': 0.001} â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ batch_size=64 â†’ input_size=52 â†’ learning_rate=0.002 â†’ optimizer=<class 'torch.optim.adamw.AdamW'> â†’ WMAE=2751.6410\n",
            "ğŸƒ View run learned-bass-232 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/26173db3527a4b0ba71fdd68a7bcb494\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:44:45 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_blocks=[2, 2, 2] â†’ optimizer_kwargs={'weight_decay': 0.01} â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ batch_size=64 â†’ input_size=52 â†’ learning_rate=0.002 â†’ optimizer=<class 'torch.optim.adamw.AdamW'> â†’ WMAE=2740.0934\n",
            "ğŸƒ View run caring-stoat-309 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/42bdf24f16bc4234a211e8ee9fa4868b\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:46:22 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_blocks=[3, 3, 3] â†’ optimizer_kwargs={'weight_decay': 0.0001} â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ batch_size=64 â†’ input_size=52 â†’ learning_rate=0.002 â†’ optimizer=<class 'torch.optim.adamw.AdamW'> â†’ WMAE=2682.8072\n",
            "ğŸƒ View run painted-panda-862 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/127451d9d19948429b7e0aa960630e64\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:47:33 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_blocks=[3, 3, 3] â†’ optimizer_kwargs={'weight_decay': 0.001} â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ batch_size=64 â†’ input_size=52 â†’ learning_rate=0.002 â†’ optimizer=<class 'torch.optim.adamw.AdamW'> â†’ WMAE=2738.3392\n",
            "ğŸƒ View run bald-deer-344 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/2aa06ec363b3470fb5b7a423d25b5955\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:48:59 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_blocks=[3, 3, 3] â†’ optimizer_kwargs={'weight_decay': 0.01} â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ batch_size=64 â†’ input_size=52 â†’ learning_rate=0.002 â†’ optimizer=<class 'torch.optim.adamw.AdamW'> â†’ WMAE=2527.9716\n",
            "ğŸƒ View run enchanting-whale-381 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/fba23695df67443a9e6715bc8c84aac0\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n",
            "\n",
            "Best n_blocks: [3, 3, 3] and optimizer_kwargs: {'weight_decay': 0.01} with WMAE: 2527.9716\n",
            "\n",
            "Tuning activation functions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:50:10 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "activation=LeakyReLU â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ batch_size=64 â†’ input_size=52 â†’ learning_rate=0.002 â†’ optimizer=<class 'torch.optim.adamw.AdamW'> â†’ n_blocks=[3, 3, 3] â†’ optimizer_kwargs={'weight_decay': 0.01} â†’ WMAE=2721.0448\n",
            "ğŸƒ View run dashing-bee-504 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/73a43fb029bf4322845971fea33c5518\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:51:20 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "activation=ReLU â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ batch_size=64 â†’ input_size=52 â†’ learning_rate=0.002 â†’ optimizer=<class 'torch.optim.adamw.AdamW'> â†’ n_blocks=[3, 3, 3] â†’ optimizer_kwargs={'weight_decay': 0.01} â†’ WMAE=2527.9716\n",
            "ğŸƒ View run stately-midge-124 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/21f2134d2cd84c899775766b7a90eef3\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:52:29 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "activation=Tanh â†’ max_steps=2600 â†’ h=53 â†’ random_seed=42 â†’ batch_size=64 â†’ input_size=52 â†’ learning_rate=0.002 â†’ optimizer=<class 'torch.optim.adamw.AdamW'> â†’ n_blocks=[3, 3, 3] â†’ optimizer_kwargs={'weight_decay': 0.01} â†’ WMAE=3471.1916\n",
            "ğŸƒ View run rebellious-skunk-485 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/3edcfc63a8c84929aee47b0fc8d055b0\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n",
            "\n",
            "Best activation: ReLU with WMAE: 2527.9716\n",
            "\n",
            "Training final model with best params...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/03 05:53:31 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n",
            "2025/08/03 05:54:33 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pytorch autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final model WMAE: 2607.1690\n",
            "Model saved and logged to MLflow.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mellekvirikashvili\u001b[0m (\u001b[33mellekvirikashvili-free-university-of-tbilisi-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250803_055444-p2wioa0z</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ellekvirikashvili-free-university-of-tbilisi-/Walmart%20Recruiting%20-%20Store%20Sales%20Forecasting/runs/p2wioa0z' target=\"_blank\">nbeats_final_run</a></strong> to <a href='https://wandb.ai/ellekvirikashvili-free-university-of-tbilisi-/Walmart%20Recruiting%20-%20Store%20Sales%20Forecasting' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ellekvirikashvili-free-university-of-tbilisi-/Walmart%20Recruiting%20-%20Store%20Sales%20Forecasting' target=\"_blank\">https://wandb.ai/ellekvirikashvili-free-university-of-tbilisi-/Walmart%20Recruiting%20-%20Store%20Sales%20Forecasting</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ellekvirikashvili-free-university-of-tbilisi-/Walmart%20Recruiting%20-%20Store%20Sales%20Forecasting/runs/p2wioa0z' target=\"_blank\">https://wandb.ai/ellekvirikashvili-free-university-of-tbilisi-/Walmart%20Recruiting%20-%20Store%20Sales%20Forecasting/runs/p2wioa0z</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>val_wmae</td><td>â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>val_wmae</td><td>2607.16898</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">nbeats_final_run</strong> at: <a href='https://wandb.ai/ellekvirikashvili-free-university-of-tbilisi-/Walmart%20Recruiting%20-%20Store%20Sales%20Forecasting/runs/p2wioa0z' target=\"_blank\">https://wandb.ai/ellekvirikashvili-free-university-of-tbilisi-/Walmart%20Recruiting%20-%20Store%20Sales%20Forecasting/runs/p2wioa0z</a><br> View project at: <a href='https://wandb.ai/ellekvirikashvili-free-university-of-tbilisi-/Walmart%20Recruiting%20-%20Store%20Sales%20Forecasting' target=\"_blank\">https://wandb.ai/ellekvirikashvili-free-university-of-tbilisi-/Walmart%20Recruiting%20-%20Store%20Sales%20Forecasting</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250803_055444-p2wioa0z/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved and logged to W&B.\n",
            "ğŸƒ View run Final NBEATS Model Training at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/1dcafbea3ad04ef095a5ac4acda485d7\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n",
            "\n",
            "Hyperparameter tuning completed!\n",
            "Best parameters: {'wmae': np.float64(2527.9716201180972), 'activation': 'ReLU', 'max_steps': 2600, 'h': 53, 'random_seed': 42, 'batch_size': 64, 'input_size': 52, 'learning_rate': 0.002, 'optimizer': <class 'torch.optim.adamw.AdamW'>, 'n_blocks': [3, 3, 3], 'optimizer_kwargs': {'weight_decay': 0.01}, 'enable_progress_bar': False, 'enable_model_summary': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Train final model ---\n",
        "print(\"\\nTraining final model with best params...\")\n",
        "with mlflow.start_run(run_name=\"Final NBEATS Model Training\"):\n",
        "    final_params = {\n",
        "        'max_steps': fixed_params['max_steps'],\n",
        "        'h': fixed_params['h'],\n",
        "        'random_seed': fixed_params['random_seed'],\n",
        "        'input_size': best_result.get('input_size', fixed_params['input_size']),\n",
        "        'batch_size': best_result.get('batch_size', fixed_params['batch_size']),\n",
        "        'learning_rate': best_result.get('learning_rate', 1e-3),\n",
        "        'optimizer': torch.optim.AdamW,\n",
        "        'activation': best_result.get('activation', 'ReLU'),\n",
        "        'n_blocks': best_result.get('n_blocks', [1,1,1]),\n",
        "        'optimizer_kwargs': best_result.get('optimizer_kwargs', {'weight_decay': 1e-4}),\n",
        "        'enable_progress_bar': False,\n",
        "        'enable_model_summary': False\n",
        "    }\n",
        "\n",
        "    final_model = NBEATS(**final_params)\n",
        "    nf_model = NeuralForecastModels(\n",
        "        models=[final_model],\n",
        "        model_names=['NBEATS'],\n",
        "        freq='W-FRI',\n",
        "        one_model=True\n",
        "    )\n",
        "\n",
        "    # Fit on full training data\n",
        "    nf_model.fit(df_cv)\n",
        "\n",
        "    # Perform final cross-validation to get score - FIXED: removed h parameter\n",
        "    final_cv_df = nf_model.cross_validation(df_cv, n_windows=1)\n",
        "\n",
        "    y_true = final_cv_df['y'].values\n",
        "    y_pred = final_cv_df['NBEATS'].values\n",
        "\n",
        "    # Get holiday information if available\n",
        "    is_holiday = None\n",
        "    if 'IsHoliday' in df.columns:\n",
        "        final_cv_df_with_holiday = final_cv_df.merge(\n",
        "            df[['unique_id', 'ds', 'IsHoliday']],\n",
        "            on=['unique_id', 'ds'],\n",
        "            how='left'\n",
        "        )\n",
        "        is_holiday = final_cv_df_with_holiday['IsHoliday'].fillna(False).values\n",
        "\n",
        "    wmae_score = compute_wmae(y_true, y_pred, is_holiday)\n",
        "\n",
        "    mlflow.log_metric(\"val_wmae_final\", wmae_score)\n",
        "\n",
        "    # Log final parameters\n",
        "    for k, v in final_params.items():\n",
        "        if not isinstance(v, (list, dict, type)):\n",
        "            mlflow.log_param(f\"final_{k}\", v)\n",
        "        else:\n",
        "            mlflow.log_param(f\"final_{k}\", str(v))\n",
        "\n",
        "    # Save and log model artifact\n",
        "    model_file = \"nbeats_final_model.pkl\"\n",
        "    joblib.dump(nf_model, model_file)\n",
        "    mlflow.log_artifact(model_file)\n",
        "\n",
        "    print(f\"Final model WMAE: {wmae_score:.4f}\")\n",
        "    print(\"Model saved and logged to MLflow.\")\n",
        "\n",
        "    # --- Save and log with W&B ---\n",
        "    wandb.init(project=\"Walmart Recruiting - Store Sales Forecasting\", name=\"nbeats_final_run\")\n",
        "    wandb.config.update({\n",
        "        'score_metric': 'WMAE',\n",
        "        'score_policy': {'weight_on_holidays': 5, 'weight_on_non_holidays': 1},\n",
        "        'model': 'NBEATS',\n",
        "        'learning_rate': best_result.get('learning_rate', 1e-3),\n",
        "        'weight_decay': best_result.get('optimizer_kwargs', {}).get('weight_decay', 1e-4),\n",
        "        'batch_size': best_result.get('batch_size', 64),\n",
        "        'max_steps': fixed_params['max_steps'],\n",
        "        'input_size': best_result.get('input_size', 52),\n",
        "        'horizon': fixed_params['h'],\n",
        "        'architecture': ['identity', 'trend', 'seasonality'],\n",
        "        'n_blocks': best_result.get('n_blocks', [1,1,1]),\n",
        "        'random_seed': fixed_params['random_seed'],\n",
        "        'activation': best_result.get('activation', 'ReLU'),\n",
        "        'optimizer': 'torch.optim.AdamW'\n",
        "    })\n",
        "\n",
        "    wandb.log({'val_wmae': wmae_score})\n",
        "\n",
        "    artifact = wandb.Artifact(name=\"nbeats_final_model\", type=\"model\")\n",
        "    artifact.add_file(model_file)\n",
        "    wandb.log_artifact(artifact)\n",
        "    wandb.finish()\n",
        "\n",
        "    # Clean up\n",
        "    if os.path.exists(model_file):\n",
        "        os.remove(model_file)\n",
        "\n",
        "    print(\"Model saved and logged to W&B.\")\n",
        "\n",
        "print(\"\\nHyperparameter tuning completed!\")\n",
        "print(f\"Best parameters: {best_result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "kS3QQO7agDiM",
        "outputId": "660a244a-8c2c-4c23-8a93-ddb170ff7173"
      },
      "id": "kS3QQO7agDiM",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training final model with best params...\n",
            "ğŸƒ View run Final NBEATS Model Training at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11/runs/7cc00c699cab498dafcb0c58de0e9196\n",
            "ğŸ§ª View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/11\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'input_size'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1696522133.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# 'h': fixed_params['h'],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# 'random_seed': fixed_params['random_seed'],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;34m'input_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbest_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input_size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbest_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbest_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'input_size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(df):\n",
        "    df = df.copy()\n",
        "    if 'ds' in df.columns and 'Date' in df.columns:\n",
        "        df = df.drop(columns=['ds'])  # Drop conflicting 'ds'\n",
        "\n",
        "    df['unique_id'] = df['Store'].astype(str) + \"_\" + df['Dept'].astype(str)\n",
        "    df.rename(columns={'Date': 'ds', 'Weekly_Sales': 'y'}, inplace=True)\n",
        "\n",
        "    df['ds'] = pd.to_datetime(df['ds'])\n",
        "    df = df.sort_values(['unique_id', 'ds'])\n",
        "    return df\n",
        "\n",
        "\n",
        "def shift_ds_by_days(df: pd.DataFrame, days: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a copy of `df` where the 'ds' column has been shifted by `days`.\n",
        "    Positive `days` moves dates forward; negative moves them backward.\n",
        "    \"\"\"\n",
        "    df_shifted = df.copy()\n",
        "    df_shifted['ds'] = df_shifted['ds'] + pd.Timedelta(days=days)\n",
        "    return df_shifted"
      ],
      "metadata": {
        "id": "D6v5ojKB7N7r"
      },
      "id": "D6v5ojKB7N7r",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_final_cv_df(train, test):\n",
        "    \"\"\"\n",
        "    Concatenates train and test to form the input to prediction,\n",
        "    ensuring 'unique_id', 'ds', and 'y' are present.\n",
        "    \"\"\"\n",
        "    df_train = train.copy()\n",
        "    df_test = test.copy()\n",
        "\n",
        "    df_train['unique_id'] = df_train['Store'].astype(str) + \"_\" + df_train['Dept'].astype(str)\n",
        "    df_train.rename(columns={'Date': 'ds', 'Weekly_Sales': 'y'}, inplace=True)\n",
        "    df_train['ds'] = pd.to_datetime(df_train['ds'])\n",
        "\n",
        "    df_test['unique_id'] = df_test['Store'].astype(str) + \"_\" + df_test['Dept'].astype(str)\n",
        "    df_test.rename(columns={'Date': 'ds'}, inplace=True)\n",
        "    df_test['ds'] = pd.to_datetime(df_test['ds'])\n",
        "\n",
        "    # Include y only for training, not test\n",
        "    df_test['y'] = np.nan  # Needed by model\n",
        "\n",
        "    final_df = pd.concat([df_train[['unique_id', 'ds', 'y']], df_test[['unique_id', 'ds', 'y']]])\n",
        "    final_df = final_df.sort_values(by=['unique_id', 'ds']).reset_index(drop=True)\n",
        "\n",
        "    return final_df\n"
      ],
      "metadata": {
        "id": "jDdQ-ZMR--W-"
      },
      "id": "jDdQ-ZMR--W-",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Remove 'h' from final_params safely\n",
        "model_params = {k: v for k, v in final_params.items() if k != 'h'}\n",
        "horizon = final_params['h']\n",
        "\n",
        "# 2. Initialize PatchTST model\n",
        "from neuralforecast.models import PatchTST\n",
        "from neuralforecast.core import NeuralForecast\n",
        "\n",
        "model = PatchTST(h=horizon, **model_params)\n",
        "\n",
        "# 3. Wrap in NeuralForecast\n",
        "nf_model = NeuralForecast(models=[model], freq='W-FRI')\n",
        "# Fit the model\n",
        "nf_model.fit(df=df_cv)\n"
      ],
      "metadata": {
        "id": "GqUpg-nE_O0n",
        "outputId": "02ecb1d4-cb9d-438c-d7e5-74cdecd91fdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "id": "GqUpg-nE_O0n",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Trainer.__init__() got an unexpected keyword argument 'n_blocks'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2401990421.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mnf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralForecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'W-FRI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mnf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/neuralforecast/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, df, static_df, val_size, use_init_models, verbose, id_col, time_col, target_col, distributed_config, prediction_intervals)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             self.models[i] = model.fit(\n\u001b[0m\u001b[1;32m    563\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistributed_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/neuralforecast/common/_base_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, val_size, test_size, random_seed, distributed_config)\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0msize\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtemporal\u001b[0m \u001b[0mcross\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mbr\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m         \"\"\"\n\u001b[0;32m-> 1486\u001b[0;31m         return self._fit(\n\u001b[0m\u001b[1;32m   1487\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/neuralforecast/common/_base_model.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset, batch_size, valid_batch_size, val_size, test_size, random_seed, shuffle_train, distributed_config)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_local\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/argparse.py\u001b[0m in \u001b[0;36minsert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# all args were already moved to kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minsert_env_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Trainer.__init__() got an unexpected keyword argument 'n_blocks'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Predict using trained model\n",
        "final_cv_df = final_cv_df[['unique_id', 'ds']]  # Drop anything else\n",
        "final_preds = nf_model.predict(df=final_cv_df)\n",
        "\n",
        "# Optional: Shift the timestamps if needed (e.g. 5 days forward)\n",
        "# If your forecast needs to align with test dates\n",
        "final_preds['ds'] = final_preds['ds'] + pd.Timedelta(days=5)\n"
      ],
      "metadata": {
        "id": "HqVa0FNC6teB",
        "outputId": "3fa2e008-9040-4b0c-e279-9cb3ff32f570",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "id": "HqVa0FNC6teB",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "You must fit the model before predicting.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3437098755.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 1. Predict using trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfinal_cv_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_cv_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unique_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ds'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Drop anything else\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfinal_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_cv_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Optional: Shift the timestamps if needed (e.g. 5 days forward)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/neuralforecast/core.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, df, static_df, futr_df, verbose, engine, level, quantiles, **data_kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fitted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You must fit the model before predicting.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0mquantiles_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: You must fit the model before predicting."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}