{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d91bb8cb",
      "metadata": {
        "id": "d91bb8cb",
        "outputId": "c913c396-0ebf-4203-c8f7-c452bc81a049",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "37e1ecd3",
      "metadata": {
        "id": "37e1ecd3",
        "outputId": "b5be20de-f101-4f86-ae0d-7f8c0c7b7b66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (24.2)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn mlflow\n",
        "\n",
        "# Set up Kaggle API\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9a2fe659",
      "metadata": {
        "id": "9a2fe659"
      },
      "outputs": [],
      "source": [
        "# Upload your kaggle.json to Colab and run:\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fd383740",
      "metadata": {
        "id": "fd383740",
        "outputId": "f73949c6-ac4a-47d7-8a8c-72161bf10c9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 711MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "df11d311",
      "metadata": {
        "id": "df11d311",
        "outputId": "a8c6831e-7bfb-41db-9c87-571e11d1e9e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open stores.csv.zip, stores.csv.zip.zip or stores.csv.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!unzip -q train.csv.zip\n",
        "!unzip -q stores.csv.zip\n",
        "!unzip -q test.csv.zip\n",
        "!unzip -q features.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_experiment_nbeats.ipynb\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class NBEATSBlock(nn.Module):\n",
        "    def __init__(self, input_size, theta_size, basis_function, layers, layer_size):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([nn.Linear(input_size, layer_size)] +\n",
        "                                    [nn.Linear(layer_size, layer_size) for _ in range(layers-1)])\n",
        "        self.basis_function = basis_function\n",
        "        self.backcast_fc = nn.Linear(layer_size, theta_size)\n",
        "        self.forecast_fc = nn.Linear(layer_size, theta_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward through fully connected layers\n",
        "        for layer in self.layers:\n",
        "            x = torch.relu(layer(x))\n",
        "\n",
        "        # Generate theta parameters\n",
        "        backcast_theta = self.backcast_fc(x)\n",
        "        forecast_theta = self.forecast_fc(x)\n",
        "\n",
        "        # Apply basis functions\n",
        "        backcast, forecast = self.basis_function(backcast_theta, forecast_theta)\n",
        "\n",
        "        return backcast, forecast\n",
        "\n",
        "class GenericBasis(nn.Module):\n",
        "    def __init__(self, backcast_size, forecast_size):\n",
        "        super().__init__()\n",
        "        self.backcast_size = backcast_size\n",
        "        self.forecast_size = forecast_size\n",
        "\n",
        "    def forward(self, backcast_theta, forecast_theta):\n",
        "        # Generic basis function - fixed size handling\n",
        "        batch_size = backcast_theta.shape[0]\n",
        "        backcast = backcast_theta[:, :self.backcast_size]\n",
        "        forecast = forecast_theta[:, :self.forecast_size]\n",
        "\n",
        "        # Ensure correct dimensions\n",
        "        if backcast.shape[1] < self.backcast_size:\n",
        "            padding = torch.zeros(batch_size, self.backcast_size - backcast.shape[1], device=backcast.device)\n",
        "            backcast = torch.cat([backcast, padding], dim=1)\n",
        "\n",
        "        if forecast.shape[1] < self.forecast_size:\n",
        "            padding = torch.zeros(batch_size, self.forecast_size - forecast.shape[1], device=forecast.device)\n",
        "            forecast = torch.cat([forecast, padding], dim=1)\n",
        "\n",
        "        return backcast, forecast\n",
        "\n",
        "class SeasonalityBasis(nn.Module):\n",
        "    def __init__(self, harmonics, backcast_size, forecast_size):\n",
        "        super().__init__()\n",
        "        self.harmonics = harmonics\n",
        "        self.backcast_size = backcast_size\n",
        "        self.forecast_size = forecast_size\n",
        "\n",
        "    def forward(self, backcast_theta, forecast_theta):\n",
        "        # Seasonality basis using Fourier series\n",
        "        batch_size = backcast_theta.shape[0]\n",
        "        device = backcast_theta.device\n",
        "\n",
        "        # Create time indices on the correct device\n",
        "        backcast_time = torch.linspace(0, 1, self.backcast_size, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
        "        forecast_time = torch.linspace(0, 1, self.forecast_size, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
        "\n",
        "        backcast = torch.zeros_like(backcast_time)\n",
        "        forecast = torch.zeros_like(forecast_time)\n",
        "\n",
        "        for i in range(self.harmonics):\n",
        "            cos_coef = backcast_theta[:, i].unsqueeze(-1)\n",
        "            sin_coef = backcast_theta[:, i + self.harmonics].unsqueeze(-1)\n",
        "\n",
        "            # Apply Fourier basis\n",
        "            backcast += (cos_coef * torch.cos(2 * np.pi * (i + 1) * backcast_time) +\n",
        "                        sin_coef * torch.sin(2 * np.pi * (i + 1) * backcast_time))\n",
        "\n",
        "            cos_coef_f = forecast_theta[:, i].unsqueeze(-1)\n",
        "            sin_coef_f = forecast_theta[:, i + self.harmonics].unsqueeze(-1)\n",
        "\n",
        "            forecast += (cos_coef_f * torch.cos(2 * np.pi * (i + 1) * forecast_time) +\n",
        "                        sin_coef_f * torch.sin(2 * np.pi * (i + 1) * forecast_time))\n",
        "\n",
        "        return backcast, forecast\n",
        "\n",
        "class TrendBasis(nn.Module):\n",
        "    def __init__(self, degree_of_polynomial, backcast_size, forecast_size):\n",
        "        super().__init__()\n",
        "        self.degree = degree_of_polynomial\n",
        "        self.backcast_size = backcast_size\n",
        "        self.forecast_size = forecast_size\n",
        "\n",
        "    def forward(self, backcast_theta, forecast_theta):\n",
        "        # Polynomial trend basis\n",
        "        batch_size = backcast_theta.shape[0]\n",
        "        device = backcast_theta.device\n",
        "\n",
        "        backcast_time = torch.linspace(-1, 1, self.backcast_size, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
        "        forecast_time = torch.linspace(-1, 1, self.forecast_size, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
        "\n",
        "        backcast = torch.zeros_like(backcast_time)\n",
        "        forecast = torch.zeros_like(forecast_time)\n",
        "\n",
        "        for i in range(self.degree + 1):\n",
        "            backcast += backcast_theta[:, i].unsqueeze(-1) * (backcast_time ** i)\n",
        "            forecast += forecast_theta[:, i].unsqueeze(-1) * (forecast_time ** i)\n",
        "\n",
        "        return backcast, forecast\n",
        "\n",
        "\n",
        "class NBEATS(nn.Module):\n",
        "    def __init__(self, backcast_length, forecast_length,\n",
        "                 stack_types=['generic', 'seasonality', 'trend'],\n",
        "                 nb_blocks_per_stack=3, hidden_layer_units=128,\n",
        "                 nb_harmonics=10, polynomial_degree=3):\n",
        "        super().__init__()\n",
        "        self.backcast_length = backcast_length\n",
        "        self.forecast_length = forecast_length\n",
        "        self.hidden_layer_units = hidden_layer_units\n",
        "        self.nb_blocks_per_stack = nb_blocks_per_stack\n",
        "        self.stack_types = stack_types\n",
        "\n",
        "        self.stacks = nn.ModuleList()\n",
        "\n",
        "        for stack_type in stack_types:\n",
        "            stack_blocks = nn.ModuleList()\n",
        "\n",
        "            for _ in range(nb_blocks_per_stack):\n",
        "                if stack_type == 'generic':\n",
        "                    theta_size = backcast_length + forecast_length\n",
        "                    basis_function = GenericBasis(backcast_length, forecast_length)\n",
        "                elif stack_type == 'seasonality':\n",
        "                    theta_size = 2 * nb_harmonics\n",
        "                    basis_function = SeasonalityBasis(nb_harmonics, backcast_length, forecast_length)\n",
        "                elif stack_type == 'trend':\n",
        "                    theta_size = polynomial_degree + 1\n",
        "                    basis_function = TrendBasis(polynomial_degree, backcast_length, forecast_length)\n",
        "\n",
        "                block = NBEATSBlock(\n",
        "                    input_size=backcast_length,\n",
        "                    theta_size=theta_size,\n",
        "                    basis_function=basis_function,\n",
        "                    layers=4,\n",
        "                    layer_size=hidden_layer_units\n",
        "                )\n",
        "                stack_blocks.append(block)\n",
        "\n",
        "            self.stacks.append(stack_blocks)\n",
        "\n",
        "    def forward(self, backcast):\n",
        "        # Initialize forecast with correct device\n",
        "        forecast = torch.zeros(backcast.size(0), self.forecast_length, device=backcast.device)\n",
        "\n",
        "        for stack in self.stacks:\n",
        "            for block in stack:\n",
        "                b, f = block(backcast)\n",
        "                backcast = backcast - b\n",
        "                forecast = forecast + f\n",
        "\n",
        "        return forecast\n",
        "\n",
        "class WalmartDataset(Dataset):\n",
        "    def __init__(self, data, backcast_length, forecast_length):\n",
        "        self.data = data\n",
        "        self.backcast_length = backcast_length\n",
        "        self.forecast_length = forecast_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.backcast_length - self.forecast_length + 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        backcast = self.data[idx:idx + self.backcast_length]\n",
        "        forecast = self.data[idx + self.backcast_length:idx + self.backcast_length + self.forecast_length]\n",
        "        return torch.FloatTensor(backcast), torch.FloatTensor(forecast)\n",
        "\n",
        "def train_nbeats(model, train_loader, val_loader, epochs=100, lr=0.001, device='cpu'):\n",
        "    \"\"\"\n",
        "    Training loop for the N-BEATS model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The N-BEATS model instance.\n",
        "        train_loader (DataLoader): DataLoader for the training set.\n",
        "        val_loader (DataLoader): DataLoader for the validation set.\n",
        "        epochs (int): Number of training epochs.\n",
        "        lr (float): Learning rate.\n",
        "        device (str): Device to use for training.\n",
        "\n",
        "    Returns:\n",
        "        train_losses (list): Epoch-wise training losses.\n",
        "        val_losses (list): Epoch-wise validation losses.\n",
        "    \"\"\"\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "\n",
        "        # Training loop\n",
        "        for backcast, forecast in train_loader:\n",
        "            # Move tensors to the correct device\n",
        "            backcast, forecast = backcast.to(device), forecast.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(backcast)\n",
        "            loss = criterion(outputs, forecast)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_train_loss / max(1, len(train_loader))\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for backcast, forecast in val_loader:\n",
        "                # Move tensors to the correct device\n",
        "                backcast, forecast = backcast.to(device), forecast.to(device)\n",
        "\n",
        "                outputs = model(backcast)\n",
        "                loss = criterion(outputs, forecast)\n",
        "                running_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = running_val_loss / max(1, len(val_loader))\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Periodic logging\n",
        "        if epoch % 10 == 0 or epoch == 1 or epoch == epochs:\n",
        "            print(f\"[Epoch {epoch}/{epochs}] Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "def run_nbeats_experiment():\n",
        "    \"\"\"N-BEATS experiment runner\"\"\"\n",
        "\n",
        "    # Check if CUDA is available\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load data - add error handling\n",
        "    try:\n",
        "        df = pd.read_csv('train.csv')\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: train.csv file not found. Please ensure the file exists in the current directory.\")\n",
        "        return None, None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "    # MLflow experiment setup\n",
        "    mlflow.set_experiment(\"Walmart_NBEATS_Forecasting\")\n",
        "\n",
        "    with mlflow.start_run():\n",
        "        # Parameters\n",
        "        BACKCAST_LENGTH = 52  # 52 weeks\n",
        "        FORECAST_LENGTH = 4   # 4 weeks forecast\n",
        "        BATCH_SIZE = 32\n",
        "        EPOCHS = 200\n",
        "        LEARNING_RATE = 0.001\n",
        "\n",
        "        # Log parameters\n",
        "        mlflow.log_param(\"backcast_length\", BACKCAST_LENGTH)\n",
        "        mlflow.log_param(\"forecast_length\", FORECAST_LENGTH)\n",
        "        mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
        "        mlflow.log_param(\"epochs\", EPOCHS)\n",
        "        mlflow.log_param(\"learning_rate\", LEARNING_RATE)\n",
        "        mlflow.log_param(\"device\", str(device))\n",
        "\n",
        "        # Prepare dataset\n",
        "        weekly_sales = df.groupby('Date')['Weekly_Sales'].sum().values\n",
        "        print(f\"Weekly sales data points: {len(weekly_sales)}\")\n",
        "\n",
        "        # Check if we have enough data\n",
        "        min_required = BACKCAST_LENGTH + FORECAST_LENGTH\n",
        "        if len(weekly_sales) < min_required:\n",
        "            print(f\"Error: Not enough data points. Need at least {min_required}, got {len(weekly_sales)}\")\n",
        "            return None, None, None\n",
        "\n",
        "        # Normalize data\n",
        "        sales_mean = weekly_sales.mean()\n",
        "        sales_std = weekly_sales.std()\n",
        "        weekly_sales_normalized = (weekly_sales - sales_mean) / sales_std\n",
        "\n",
        "        mlflow.log_param(\"sales_mean\", sales_mean)\n",
        "        mlflow.log_param(\"sales_std\", sales_std)\n",
        "\n",
        "        # Train/Val/Test split\n",
        "        train_size = int(0.7 * len(weekly_sales_normalized))\n",
        "        val_size = int(0.15 * len(weekly_sales_normalized))\n",
        "\n",
        "        train_data = weekly_sales_normalized[:train_size]\n",
        "        val_data = weekly_sales_normalized[train_size:train_size + val_size]\n",
        "        test_data = weekly_sales_normalized[train_size + val_size:]\n",
        "\n",
        "        print(f\"Train size: {len(train_data)}, Val size: {len(val_data)}, Test size: {len(test_data)}\")\n",
        "\n",
        "        # DataLoaders\n",
        "        train_dataset = WalmartDataset(train_data, BACKCAST_LENGTH, FORECAST_LENGTH)\n",
        "        val_dataset = WalmartDataset(val_data, BACKCAST_LENGTH, FORECAST_LENGTH)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "        # Model initialization\n",
        "        model = NBEATS(\n",
        "            backcast_length=BACKCAST_LENGTH,\n",
        "            forecast_length=FORECAST_LENGTH,\n",
        "            stack_types=['trend', 'seasonality', 'generic'],\n",
        "            nb_blocks_per_stack=3,\n",
        "            hidden_layer_units=128\n",
        "        ).to(device)\n",
        "\n",
        "        print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "\n",
        "        # Training\n",
        "        train_losses, val_losses = train_nbeats(model, train_loader, val_loader, EPOCHS, LEARNING_RATE, device)\n",
        "\n",
        "        # Log metrics\n",
        "        mlflow.log_metric(\"final_train_loss\", train_losses[-1])\n",
        "        mlflow.log_metric(\"final_val_loss\", val_losses[-1])\n",
        "\n",
        "        # Save model\n",
        "        mlflow.pytorch.log_model(model, \"nbeats_model\")\n",
        "\n",
        "        # Testing\n",
        "        if len(test_data) >= BACKCAST_LENGTH + FORECAST_LENGTH:\n",
        "            test_dataset = WalmartDataset(test_data, BACKCAST_LENGTH, FORECAST_LENGTH)\n",
        "            test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "            model.eval()\n",
        "            predictions = []\n",
        "            actuals = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for backcast, forecast in test_loader:\n",
        "                    backcast, forecast = backcast.to(device), forecast.to(device)\n",
        "                    pred = model(backcast)\n",
        "\n",
        "                    # Denormalize predictions and actuals\n",
        "                    pred_denorm = pred.cpu().numpy() * sales_std + sales_mean\n",
        "                    actual_denorm = forecast.cpu().numpy() * sales_std + sales_mean\n",
        "\n",
        "                    predictions.extend(pred_denorm.flatten())\n",
        "                    actuals.extend(actual_denorm.flatten())\n",
        "\n",
        "            # Test metrics\n",
        "            mae = mean_absolute_error(actuals, predictions)\n",
        "            mse = mean_squared_error(actuals, predictions)\n",
        "            rmse = np.sqrt(mse)\n",
        "\n",
        "            mlflow.log_metric(\"test_mae\", mae)\n",
        "            mlflow.log_metric(\"test_mse\", mse)\n",
        "            mlflow.log_metric(\"test_rmse\", rmse)\n",
        "\n",
        "            print(f\"Test MAE: {mae:.2f}\")\n",
        "            print(f\"Test RMSE: {rmse:.2f}\")\n",
        "\n",
        "            return model, predictions, actuals\n",
        "        else:\n",
        "            print(\"Not enough test data for evaluation\")\n",
        "            return model, [], []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, predictions, actuals = run_nbeats_experiment()"
      ],
      "metadata": {
        "id": "O0mChVZ2qK-N",
        "outputId": "c30ffabf-0e53-4fa9-96dd-1602d2676764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "id": "O0mChVZ2qK-N",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Data loaded successfully. Shape: (421570, 5)\n",
            "Weekly sales data points: 143\n",
            "Train size: 100, Val size: 21, Test size: 22\n",
            "Model initialized with 568800 parameters\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "__len__() should return >= 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-650872870.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactuals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_nbeats_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-8-650872870.py\u001b[0m in \u001b[0;36mrun_nbeats_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_nbeats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;31m# Log metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-8-650872870.py\u001b[0m in \u001b[0;36mtrain_nbeats\u001b[0;34m(model, train_loader, val_loader, epochs, lr, device)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbackcast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforecast\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m                 \u001b[0;31m# Move tensors to the correct device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0mbackcast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackcast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforecast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;31m# Implemented based on the benchmarking in https://github.com/pytorch/pytorch/pull/76951\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0msampler_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_last\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;31m# Create multiple references to the same iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: __len__() should return >= 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B53a7-sore9p"
      },
      "id": "B53a7-sore9p",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}