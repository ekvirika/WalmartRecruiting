{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekvirika/WalmartRecruiting/blob/main/notebooks/model_experiment_patchtst.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c5181808",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5181808",
        "outputId": "f3b3d34a-7bcb-4d71-9f8c-dad308fa6797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ada3a352",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ada3a352",
        "outputId": "08a54ffd-fd1d-4b68-86bf-170937520388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting mlflow\n",
            "  Downloading mlflow-3.1.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (24.2)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting mlflow-skinny==3.1.1 (from mlflow)\n",
            "  Downloading mlflow_skinny-3.1.1-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.41)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.1.1->mlflow)\n",
            "  Downloading databricks_sdk-0.57.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.115.14)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.7.0)\n",
            "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.1.1->mlflow)\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.1.1->mlflow)\n",
            "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.5.3)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.35.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.4.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==3.1.1->mlflow) (0.46.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.1->mlflow) (3.23.0)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow)\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==3.1.1->mlflow) (0.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.6.1)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow-3.1.1-py3-none-any.whl (24.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-3.1.1-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.57.0-py3-none-any.whl (733 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m733.8/733.8 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gunicorn, graphql-core, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, graphql-relay, docker, alembic, opentelemetry-semantic-conventions, nvidia-cusolver-cu12, graphene, databricks-sdk, opentelemetry-sdk, mlflow-skinny, mlflow\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed alembic-1.16.2 databricks-sdk-0.57.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-3.1.1 mlflow-skinny-3.1.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 opentelemetry-api-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn mlflow\n",
        "\n",
        "# Set up Kaggle API\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "876aa455",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "876aa455"
      },
      "outputs": [],
      "source": [
        "# Upload your kaggle.json to Colab and run:\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6315b2e3",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6315b2e3",
        "outputId": "0cabaaf3-fe8f-4bdd-e612-f6f69c2a26f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/competitions/data/download-all/walmart-recruiting-store-sales-forecasting\n",
            "unzip:  cannot find or open walmart-recruiting-store-sales-forecasting.zip, walmart-recruiting-store-sales-forecasting.zip.zip or walmart-recruiting-store-sales-forecasting.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "296fec16",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "296fec16",
        "outputId": "a3fc5e5b-02ad-436f-b978-01e4487773b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open train.csv.zip, train.csv.zip.zip or train.csv.zip.ZIP.\n",
            "unzip:  cannot find or open stores.csv.zip, stores.csv.zip.zip or stores.csv.zip.ZIP.\n",
            "unzip:  cannot find or open test.csv.zip, test.csv.zip.zip or test.csv.zip.ZIP.\n",
            "unzip:  cannot find or open features.csv.zip, features.csv.zip.zip or features.csv.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!unzip -q train.csv.zip\n",
        "!unzip -q stores.csv.zip\n",
        "!unzip -q test.csv.zip\n",
        "!unzip -q features.csv.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup and Imports"
      ],
      "metadata": {
        "id": "HwNkbWN8ZO9t"
      },
      "id": "HwNkbWN8ZO9t"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a15c3d35",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a15c3d35",
        "outputId": "ba3358ea-bd59-4ce7-b15a-0c17b9e8009a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/07 05:42:32 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
            "2025/07/07 05:42:33 INFO mlflow.store.db.utils: Updating database tables\n",
            "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
            "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
            "INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step\n",
            "INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
            "INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
            "INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
            "INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
            "INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
            "INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!\n",
            "INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
            "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.\n",
            "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!\n",
            "INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
            "INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
            "INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
            "INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
            "INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
            "INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
            "INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
            "INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
            "INFO  [alembic.runtime.migration] Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
            "INFO  [alembic.runtime.migration] Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
            "INFO  [alembic.runtime.migration] Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
            "INFO  [alembic.runtime.migration] Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
            "INFO  [alembic.runtime.migration] Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
            "INFO  [alembic.runtime.migration] Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
            "INFO  [alembic.runtime.migration] Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
            "INFO  [alembic.runtime.migration] Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
            "INFO  [alembic.runtime.migration] Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
            "INFO  [alembic.runtime.migration] Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
            "INFO  [alembic.runtime.migration] Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
            "INFO  [alembic.runtime.migration] Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
            "INFO  [alembic.runtime.migration] Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
            "INFO  [alembic.runtime.migration] Running upgrade 0584bdc529eb -> 400f98739977, add logged model tables\n",
            "INFO  [alembic.runtime.migration] Running upgrade 400f98739977 -> 6953534de441, add step to inputs table\n",
            "INFO  [alembic.runtime.migration] Running upgrade 6953534de441 -> bda7b8c39065, increase_model_version_tag_value_limit\n",
            "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
            "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
            "2025/07/07 05:42:34 INFO mlflow.tracking.fluent: Experiment with name 'PatchTST_Training' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Setup completed successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import mlflow.sklearn\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "# MLflow setup\n",
        "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
        "mlflow.set_experiment(\"PatchTST_Training\")\n",
        "\n",
        "print(\"üöÄ Setup completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Loading"
      ],
      "metadata": {
        "id": "6XGNuXREZRaP"
      },
      "id": "6XGNuXREZRaP"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d4bf6637",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "d4bf6637",
        "outputId": "e8a4566b-1d8f-43d4-e21b-de317de20936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Loading Walmart datasets...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-2555555087.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstores_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_walmart_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-7-2555555087.py\u001b[0m in \u001b[0;36mload_walmart_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Load datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfeatures_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'features.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
          ]
        }
      ],
      "source": [
        "def load_walmart_data():\n",
        "    \"\"\"Load Walmart sales data with proper data types\"\"\"\n",
        "    print(\"üìä Loading Walmart datasets...\")\n",
        "\n",
        "    # Load datasets\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    features_df = pd.read_csv('features.csv')\n",
        "    stores_df = pd.read_csv('stores.csv')\n",
        "\n",
        "    # Convert date columns\n",
        "    train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "    test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
        "    features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
        "\n",
        "    print(f\"‚úÖ Data loaded successfully!\")\n",
        "    print(f\"üìà Train shape: {train_df.shape}\")\n",
        "    print(f\"üîÆ Test shape: {test_df.shape}\")\n",
        "    print(f\"üè™ Stores shape: {stores_df.shape}\")\n",
        "    print(f\"üìã Features shape: {features_df.shape}\")\n",
        "\n",
        "    return train_df, test_df, features_df, stores_df\n",
        "\n",
        "# Load data\n",
        "train_df, test_df, features_df, stores_df = load_walmart_data()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data Cleaning Pipeline"
      ],
      "metadata": {
        "id": "s04eoCoSZTX5"
      },
      "id": "s04eoCoSZTX5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85400e94",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "85400e94"
      },
      "outputs": [],
      "source": [
        "class DataCleaner(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Data cleaning transformer for Walmart sales data\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.outlier_bounds = {}\n",
        "        self.fill_values = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit the data cleaner\"\"\"\n",
        "        X_clean = X.copy()\n",
        "\n",
        "        # Calculate outlier bounds for Weekly_Sales (if present)\n",
        "        if 'Weekly_Sales' in X_clean.columns:\n",
        "            Q1 = X_clean['Weekly_Sales'].quantile(0.25)\n",
        "            Q3 = X_clean['Weekly_Sales'].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            self.outlier_bounds['Weekly_Sales'] = {\n",
        "                'lower': Q1 - 1.5 * IQR,\n",
        "                'upper': Q3 + 1.5 * IQR\n",
        "            }\n",
        "\n",
        "        # Calculate fill values for missing data\n",
        "        numeric_columns = X_clean.select_dtypes(include=[np.number]).columns\n",
        "        for col in numeric_columns:\n",
        "            if col in ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']:\n",
        "                self.fill_values[col] = 0.0  # Markdowns are 0 when not present\n",
        "            else:\n",
        "                self.fill_values[col] = X_clean[col].median()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform the data\"\"\"\n",
        "        X_clean = X.copy()\n",
        "\n",
        "        # Handle missing values\n",
        "        for col, fill_value in self.fill_values.items():\n",
        "            if col in X_clean.columns:\n",
        "                X_clean[col] = X_clean[col].fillna(fill_value)\n",
        "\n",
        "        # Handle outliers in Weekly_Sales (only for training data)\n",
        "        if 'Weekly_Sales' in X_clean.columns and 'Weekly_Sales' in self.outlier_bounds:\n",
        "            bounds = self.outlier_bounds['Weekly_Sales']\n",
        "            # Cap outliers instead of removing them\n",
        "            X_clean['Weekly_Sales'] = X_clean['Weekly_Sales'].clip(\n",
        "                lower=bounds['lower'], upper=bounds['upper']\n",
        "            )\n",
        "\n",
        "        # Handle negative sales (set to 0)\n",
        "        if 'Weekly_Sales' in X_clean.columns:\n",
        "            X_clean['Weekly_Sales'] = X_clean['Weekly_Sales'].clip(lower=0)\n",
        "\n",
        "        return X_clean\n",
        "\n",
        "# Run data cleaning experiment\n",
        "with mlflow.start_run(run_name=\"PatchTST_Cleaning\"):\n",
        "    print(\"üßπ Starting data cleaning process...\")\n",
        "\n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"cleaning_method\", \"IQR_outlier_detection\")\n",
        "    mlflow.log_param(\"missing_value_strategy\", \"median_fill\")\n",
        "\n",
        "    # Initialize cleaner\n",
        "    cleaner = DataCleaner()\n",
        "\n",
        "    # Merge datasets for comprehensive cleaning\n",
        "    train_merged = train_df.merge(stores_df, on='Store', how='left')\n",
        "    train_merged = train_merged.merge(features_df, on=['Store', 'Date'], how='left')\n",
        "\n",
        "    # Fit and transform training data\n",
        "    train_cleaned = cleaner.fit_transform(train_merged)\n",
        "\n",
        "    # Log cleaning statistics\n",
        "    original_sales_std = train_df['Weekly_Sales'].std()\n",
        "    cleaned_sales_std = train_cleaned['Weekly_Sales'].std()\n",
        "\n",
        "    mlflow.log_metric(\"original_sales_std\", original_sales_std)\n",
        "    mlflow.log_metric(\"cleaned_sales_std\", cleaned_sales_std)\n",
        "    mlflow.log_metric(\"outlier_reduction\", (original_sales_std - cleaned_sales_std) / original_sales_std)\n",
        "\n",
        "    # Log missing value statistics\n",
        "    missing_before = train_merged.isnull().sum().sum()\n",
        "    missing_after = train_cleaned.isnull().sum().sum()\n",
        "\n",
        "    mlflow.log_metric(\"missing_values_before\", missing_before)\n",
        "    mlflow.log_metric(\"missing_values_after\", missing_after)\n",
        "\n",
        "    print(f\"‚úÖ Data cleaning completed!\")\n",
        "    print(f\"üìä Missing values reduced from {missing_before} to {missing_after}\")\n",
        "    print(f\"üìà Sales volatility reduced by {((original_sales_std - cleaned_sales_std) / original_sales_std) * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Feature Engineering Pipeline"
      ],
      "metadata": {
        "id": "WTMMRHNvZV0Z"
      },
      "id": "WTMMRHNvZV0Z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c12976e",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "8c12976e"
      },
      "outputs": [],
      "source": [
        "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Comprehensive feature engineering for time series forecasting\"\"\"\n",
        "\n",
        "    def __init__(self, lag_periods=[1, 2, 3, 4, 5, 8, 12, 52],\n",
        "                 rolling_windows=[3, 4, 8, 12, 26, 52]):\n",
        "        self.lag_periods = lag_periods\n",
        "        self.rolling_windows = rolling_windows\n",
        "        self.label_encoders = {}\n",
        "        self.fitted = False\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit the feature engineer\"\"\"\n",
        "        X_features = X.copy()\n",
        "\n",
        "        # Fit label encoders\n",
        "        for col in ['Type']:\n",
        "            if col in X_features.columns:\n",
        "                self.label_encoders[col] = LabelEncoder()\n",
        "                self.label_encoders[col].fit(X_features[col].astype(str))\n",
        "\n",
        "        self.fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform data with comprehensive feature engineering\"\"\"\n",
        "        X_features = X.copy()\n",
        "\n",
        "        # 1. Time-based features\n",
        "        X_features = self._create_time_features(X_features)\n",
        "\n",
        "        # 2. Lag features (only for training data with Weekly_Sales)\n",
        "        if 'Weekly_Sales' in X_features.columns:\n",
        "            X_features = self._create_lag_features(X_features)\n",
        "            X_features = self._create_rolling_features(X_features)\n",
        "\n",
        "        # 3. Store and department features\n",
        "        X_features = self._create_store_features(X_features)\n",
        "\n",
        "        # 4. Economic and promotional features\n",
        "        X_features = self._create_economic_features(X_features)\n",
        "\n",
        "        # 5. Holiday and seasonal features\n",
        "        X_features = self._create_holiday_features(X_features)\n",
        "\n",
        "        # 6. Interaction features\n",
        "        X_features = self._create_interaction_features(X_features)\n",
        "\n",
        "        # 7. Encode categorical variables\n",
        "        X_features = self._encode_categorical(X_features)\n",
        "\n",
        "        # Remove rows with too many NaN values (mainly from lag features)\n",
        "        if 'Weekly_Sales' in X_features.columns:\n",
        "            # For training data, we can afford to lose some rows\n",
        "            X_features = X_features.dropna(subset=['Weekly_Sales'])\n",
        "            # Fill remaining NaN values with forward fill then backward fill\n",
        "            X_features = X_features.fillna(method='ffill').fillna(method='bfill')\n",
        "        else:\n",
        "            # For test data, we need to keep all rows\n",
        "            X_features = X_features.fillna(method='ffill').fillna(method='bfill')\n",
        "            # Fill any remaining NaN with median\n",
        "            numeric_cols = X_features.select_dtypes(include=[np.number]).columns\n",
        "            X_features[numeric_cols] = X_features[numeric_cols].fillna(X_features[numeric_cols].median())\n",
        "\n",
        "        return X_features\n",
        "\n",
        "    def _create_time_features(self, df):\n",
        "        \"\"\"Create time-based features\"\"\"\n",
        "        df['Year'] = df['Date'].dt.year\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Week'] = df['Date'].dt.isocalendar().week\n",
        "        df['Day'] = df['Date'].dt.day\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        df['Quarter'] = df['Date'].dt.quarter\n",
        "        df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(int)\n",
        "        df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "\n",
        "        # Cyclical features\n",
        "        df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
        "        df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
        "        df['Week_sin'] = np.sin(2 * np.pi * df['Week'] / 52)\n",
        "        df['Week_cos'] = np.cos(2 * np.pi * df['Week'] / 52)\n",
        "        df['DayOfWeek_sin'] = np.sin(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "        df['DayOfWeek_cos'] = np.cos(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "        df['Quarter_sin'] = np.sin(2 * np.pi * df['Quarter'] / 4)\n",
        "        df['Quarter_cos'] = np.cos(2 * np.pi * df['Quarter'] / 4)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_lag_features(self, df):\n",
        "        \"\"\"Create lag features for time series\"\"\"\n",
        "        df = df.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "        for lag in self.lag_periods:\n",
        "            df[f'Weekly_Sales_lag_{lag}'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(lag)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_rolling_features(self, df):\n",
        "        \"\"\"Create rolling statistical features\"\"\"\n",
        "        df = df.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "        for window in self.rolling_windows:\n",
        "            # Rolling statistics for Weekly_Sales\n",
        "            df[f'Weekly_Sales_rolling_mean_{window}'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].rolling(window=window).mean().reset_index(0, drop=True)\n",
        "            df[f'Weekly_Sales_rolling_std_{window}'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].rolling(window=window).std().reset_index(0, drop=True)\n",
        "            df[f'Weekly_Sales_rolling_min_{window}'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].rolling(window=window).min().reset_index(0, drop=True)\n",
        "            df[f'Weekly_Sales_rolling_max_{window}'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].rolling(window=window).max().reset_index(0, drop=True)\n",
        "\n",
        "            # Rolling statistics for Temperature\n",
        "            if 'Temperature' in df.columns:\n",
        "                df[f'Temperature_rolling_mean_{window}'] = df.groupby(['Store'])['Temperature'].rolling(window=window).mean().reset_index(0, drop=True)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_store_features(self, df):\n",
        "        \"\"\"Create store-specific features\"\"\"\n",
        "        if 'Size' in df.columns:\n",
        "            # Store size categories\n",
        "            df['Size_category'] = pd.cut(df['Size'], bins=3, labels=['Small', 'Medium', 'Large'])\n",
        "\n",
        "            # Store size relative to average\n",
        "            df['Size_relative'] = df['Size'] / df['Size'].mean()\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_economic_features(self, df):\n",
        "        \"\"\"Create economic and promotional features\"\"\"\n",
        "        # Total markdowns\n",
        "        markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "        available_markdowns = [col for col in markdown_cols if col in df.columns]\n",
        "\n",
        "        if available_markdowns:\n",
        "            df['Total_MarkDown'] = df[available_markdowns].sum(axis=1)\n",
        "            df['Has_MarkDown'] = (df['Total_MarkDown'] > 0).astype(int)\n",
        "            df['MarkDown_Count'] = (df[available_markdowns] > 0).sum(axis=1)\n",
        "\n",
        "        # Economic indicators\n",
        "        if 'CPI' in df.columns and 'Unemployment' in df.columns:\n",
        "            df['Economic_Index'] = df['CPI'] / (df['Unemployment'] + 1)  # Add 1 to avoid division by zero\n",
        "\n",
        "        # Fuel price impact\n",
        "        if 'Fuel_Price' in df.columns:\n",
        "            df['Fuel_Price_High'] = (df['Fuel_Price'] > df['Fuel_Price'].quantile(0.75)).astype(int)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_holiday_features(self, df):\n",
        "        \"\"\"Create holiday and seasonal features\"\"\"\n",
        "        # Holiday impact\n",
        "        df['IsHoliday_int'] = df['IsHoliday'].astype(int)\n",
        "\n",
        "        # Seasonal patterns\n",
        "        df['Is_BackToSchool'] = ((df['Month'] == 8) | (df['Month'] == 9)).astype(int)\n",
        "        df['Is_Christmas'] = (df['Month'] == 12).astype(int)\n",
        "        df['Is_Thanksgiving'] = ((df['Month'] == 11) & (df['Day'] >= 22)).astype(int)\n",
        "        df['Is_Summer'] = ((df['Month'] >= 6) & (df['Month'] <= 8)).astype(int)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_interaction_features(self, df):\n",
        "        \"\"\"Create interaction features\"\"\"\n",
        "        # Store type and size interaction\n",
        "        if 'Type' in df.columns and 'Size' in df.columns:\n",
        "            df['Type_Size_interaction'] = df['Type'].astype(str) + '_' + df['Size'].astype(str)\n",
        "\n",
        "        # Holiday and markdown interaction\n",
        "        if 'IsHoliday' in df.columns and 'Total_MarkDown' in df.columns:\n",
        "            df['Holiday_MarkDown'] = df['IsHoliday'].astype(int) * df['Total_MarkDown']\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _encode_categorical(self, df):\n",
        "        \"\"\"Encode categorical variables\"\"\"\n",
        "        for col, encoder in self.label_encoders.items():\n",
        "            if col in df.columns:\n",
        "                df[f'{col}_encoded'] = encoder.transform(df[col].astype(str))\n",
        "\n",
        "        return df\n",
        "\n",
        "# Run feature engineering experiment\n",
        "with mlflow.start_run(run_name=\"PatchTST_Feature_Engineering\"):\n",
        "    print(\"‚öôÔ∏è Starting feature engineering process...\")\n",
        "\n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"lag_periods\", [1, 2, 3, 4, 5, 8, 12, 52])\n",
        "    mlflow.log_param(\"rolling_windows\", [3, 4, 8, 12, 26, 52])\n",
        "    mlflow.log_param(\"feature_types\", [\"time\", \"lag\", \"rolling\", \"store\", \"economic\", \"holiday\", \"interaction\"])\n",
        "\n",
        "    # Initialize feature engineer\n",
        "    feature_engineer = FeatureEngineer()\n",
        "\n",
        "    # Fit and transform training data\n",
        "    train_features = feature_engineer.fit_transform(train_cleaned)\n",
        "\n",
        "    # Log feature engineering statistics\n",
        "    original_features = train_cleaned.shape[1]\n",
        "    new_features = train_features.shape[1]\n",
        "\n",
        "    mlflow.log_metric(\"original_features\", original_features)\n",
        "    mlflow.log_metric(\"engineered_features\", new_features)\n",
        "    mlflow.log_metric(\"features_added\", new_features - original_features)\n",
        "\n",
        "    # Log data quality metrics\n",
        "    data_quality_score = (train_features.notna().sum().sum()) / (train_features.shape[0] * train_features.shape[1])\n",
        "    mlflow.log_metric(\"data_quality_score\", data_quality_score)\n",
        "\n",
        "    print(f\"‚úÖ Feature engineering completed!\")\n",
        "    print(f\"üìä Features increased from {original_features} to {new_features}\")\n",
        "    print(f\"üéØ Data quality score: {data_quality_score:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Feature Selection and Preprocessing"
      ],
      "metadata": {
        "id": "F3cJ3_e-ZYbO"
      },
      "id": "F3cJ3_e-ZYbO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73ca3bab",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "73ca3bab"
      },
      "outputs": [],
      "source": [
        "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Feature selection and preprocessing for PatchTST\"\"\"\n",
        "\n",
        "    def __init__(self, max_features=50):\n",
        "        self.max_features = max_features\n",
        "        self.selected_features = None\n",
        "        self.scaler = RobustScaler()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Select features based on importance and correlation\"\"\"\n",
        "        X_select = X.copy()\n",
        "\n",
        "        # Remove non-numeric columns and ID columns\n",
        "        exclude_cols = ['Date', 'Store', 'Dept', 'Type', 'Size_category', 'Type_Size_interaction']\n",
        "        numeric_cols = X_select.select_dtypes(include=[np.number]).columns\n",
        "        feature_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
        "\n",
        "        if y is not None:\n",
        "            # Calculate correlation with target\n",
        "            correlations = {}\n",
        "            for col in feature_cols:\n",
        "                if col != 'Weekly_Sales':\n",
        "                    corr = abs(X_select[col].corr(y))\n",
        "                    if not np.isnan(corr):\n",
        "                        correlations[col] = corr\n",
        "\n",
        "            # Sort by correlation and select top features\n",
        "            sorted_features = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
        "            self.selected_features = [feat[0] for feat in sorted_features[:self.max_features]]\n",
        "        else:\n",
        "            # For test data, use all available numeric features\n",
        "            self.selected_features = feature_cols[:self.max_features]\n",
        "\n",
        "        # Always include basic features\n",
        "        basic_features = ['Store', 'Dept', 'IsHoliday_int', 'Month', 'Week', 'DayOfWeek']\n",
        "        for feat in basic_features:\n",
        "            if feat in X_select.columns and feat not in self.selected_features:\n",
        "                self.selected_features.append(feat)\n",
        "\n",
        "        # Fit scaler on selected features\n",
        "        if self.selected_features:\n",
        "            self.scaler.fit(X_select[self.selected_features])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform data with selected features\"\"\"\n",
        "        X_transformed = X[self.selected_features].copy()\n",
        "        X_scaled = self.scaler.transform(X_transformed)\n",
        "\n",
        "        # Return as DataFrame to maintain feature names\n",
        "        return pd.DataFrame(X_scaled, columns=self.selected_features, index=X.index)\n",
        "\n",
        "# Run feature selection experiment\n",
        "with mlflow.start_run(run_name=\"PatchTST_Feature_Selection\"):\n",
        "    print(\"üéØ Starting feature selection process...\")\n",
        "\n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"selection_method\", \"correlation_based\")\n",
        "    mlflow.log_param(\"max_features\", 50)\n",
        "    mlflow.log_param(\"scaler_type\", \"RobustScaler\")\n",
        "\n",
        "    # Initialize feature selector\n",
        "    feature_selector = FeatureSelector(max_features=50)\n",
        "\n",
        "    # Fit and transform\n",
        "    X_selected = feature_selector.fit_transform(train_features, train_features['Weekly_Sales'])\n",
        "\n",
        "    # Log feature selection statistics\n",
        "    mlflow.log_metric(\"selected_features_count\", len(feature_selector.selected_features))\n",
        "    mlflow.log_metric(\"feature_reduction_ratio\", len(feature_selector.selected_features) / train_features.shape[1])\n",
        "\n",
        "    # Log top features\n",
        "    top_features = feature_selector.selected_features[:10]\n",
        "    mlflow.log_param(\"top_10_features\", top_features)\n",
        "\n",
        "    print(f\"‚úÖ Feature selection completed!\")\n",
        "    print(f\"üìä Selected {len(feature_selector.selected_features)} features from {train_features.shape[1]}\")\n",
        "    print(f\"üèÜ Top 10 features: {top_features}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. PatchTST Model Implementation"
      ],
      "metadata": {
        "id": "7ulW-cgCZay6"
      },
      "id": "7ulW-cgCZay6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c23b351d",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "c23b351d"
      },
      "outputs": [],
      "source": [
        "class PatchTSTDataset(Dataset):\n",
        "    \"\"\"Dataset class for PatchTST model\"\"\"\n",
        "\n",
        "    def __init__(self, X, y, seq_len=52, pred_len=1, stride=1):\n",
        "        self.X = X.values if hasattr(X, 'values') else X\n",
        "        self.y = y.values if hasattr(y, 'values') else y\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.stride = stride\n",
        "\n",
        "        # Create sequences\n",
        "        self.sequences = []\n",
        "        self.targets = []\n",
        "\n",
        "        for i in range(0, len(self.X) - seq_len - pred_len + 1, stride):\n",
        "            self.sequences.append(self.X[i:i + seq_len])\n",
        "            self.targets.append(self.y[i + seq_len:i + seq_len + pred_len])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.FloatTensor(self.sequences[idx]), torch.FloatTensor(self.targets[idx])\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Patch embedding layer for time series\"\"\"\n",
        "\n",
        "    def __init__(self, patch_len, stride, padding, in_channels, embed_dim):\n",
        "        super().__init__()\n",
        "        self.patch_len = patch_len\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "        self.conv = nn.Conv1d(in_channels, embed_dim, kernel_size=patch_len, stride=stride, padding=padding)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, n_features)\n",
        "        x = x.transpose(1, 2)  # (batch_size, n_features, seq_len)\n",
        "        x = self.conv(x)  # (batch_size, embed_dim, n_patches)\n",
        "        x = x.transpose(1, 2)  # (batch_size, n_patches, embed_dim)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "class PatchTST(nn.Module):\n",
        "    \"\"\"PatchTST model for time series forecasting\"\"\"\n",
        "\n",
        "    def __init__(self, seq_len, pred_len, patch_len, stride, n_features,\n",
        "                 embed_dim=128, num_heads=8, num_layers=3, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.patch_len = patch_len\n",
        "        self.stride = stride\n",
        "        self.n_features = n_features\n",
        "\n",
        "        # Calculate number of patches\n",
        "        self.n_patches = (seq_len - patch_len) // stride + 1\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embedding = PatchEmbedding(patch_len, stride, 0, n_features, embed_dim)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, self.n_patches, embed_dim))\n",
        "\n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=embed_dim * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            activation='gelu'\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Prediction head\n",
        "        self.prediction_head = nn.Sequential(\n",
        "            nn.Linear(embed_dim * self.n_patches, embed_dim * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim * 2, embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim, pred_len)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize weights\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Conv1d):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, n_features)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Patch embedding\n",
        "        x = self.patch_embedding(x)  # (batch_size, n_patches, embed_dim)\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = x + self.positional_encoding\n",
        "\n",
        "        # Transformer encoder\n",
        "        x = self.transformer(x)  # (batch_size, n_patches, embed_dim)\n",
        "\n",
        "        # Flatten and predict\n",
        "        x = x.view(batch_size, -1)  # (batch_size, n_patches * embed_dim)\n",
        "        output = self.prediction_head(x)  # (batch_size, pred_len)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Training Pipeline"
      ],
      "metadata": {
        "id": "jQQAw69PZcrf"
      },
      "id": "jQQAw69PZcrf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b72963d1",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "b72963d1"
      },
      "outputs": [],
      "source": [
        "def train_patchtst(model, train_loader, val_loader, epochs=100, lr=0.001, device='cpu'):\n",
        "    \"\"\"Train PatchTST model with advanced training techniques\"\"\"\n",
        "\n",
        "    model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_loader), epochs=epochs)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 15\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_batches = 0\n",
        "\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch_x)\n",
        "            loss = criterion(output, batch_y)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_batches += 1\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "                output = model(batch_x)\n",
        "                loss = criterion(output, batch_y)\n",
        "                val_loss += loss.item()\n",
        "                val_batches += 1\n",
        "\n",
        "        train_loss /= train_batches\n",
        "        val_loss /= val_batches\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), 'best_patchtst_model.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load('best_patchtst_model.pth'))\n",
        "\n",
        "    return model, train_losses, val_losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Complete Pipeline Class"
      ],
      "metadata": {
        "id": "mvvq8Rq1Zere"
      },
      "id": "mvvq8Rq1Zere"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e043cb93",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "e043cb93"
      },
      "outputs": [],
      "source": [
        "class PatchTSTCompletePipeline:\n",
        "    \"\"\"Complete end-to-end pipeline for PatchTST model\"\"\"\n",
        "\n",
        "    def __init__(self, seq_len=52, pred_len=1, patch_len=8"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}