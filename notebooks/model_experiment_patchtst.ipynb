{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3d76b1de",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekvirika/WalmartRecruiting/blob/main/notebooks/model_experiment_patchtst.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c5181808",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5181808",
        "outputId": "a557eb38-8722-47b8-cf31-dc4b36609c99",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ada3a352",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ada3a352",
        "outputId": "84e845ad-2cea-4fbf-9a65-b961a032b579",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (24.2)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: mlflow-skinny==3.1.1 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.16.2)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.11/dist-packages (from mlflow) (23.0.0)\n",
            "Requirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.41)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.1)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.57.0)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.115.14)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (1.34.1)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.5.3)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.35.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.4.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.6)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==3.1.1->mlflow) (0.46.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.1->mlflow) (3.23.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow) (0.55b1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==3.1.1->mlflow) (0.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.6.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn mlflow\n",
        "\n",
        "# Set up Kaggle API\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "876aa455",
      "metadata": {
        "id": "876aa455",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Upload your kaggle.json to Colab and run:\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6315b2e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6315b2e3",
        "outputId": "40f7b680-f4a1-47de-a01d-5b60bc2bc702",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "walmart-recruiting-store-sales-forecasting.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "replace features.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace sampleSubmission.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace stores.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace test.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace train.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "296fec16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "296fec16",
        "outputId": "b2c2e74c-33e8-4965-c9ae-1a28517c4755",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "unzip:  cannot find or open stores.csv.zip, stores.csv.zip.zip or stores.csv.zip.ZIP.\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace features.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n"
          ]
        }
      ],
      "source": [
        "!unzip -q train.csv.zip\n",
        "!unzip -q stores.csv.zip\n",
        "!unzip -q test.csv.zip\n",
        "!unzip -q features.csv.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HwNkbWN8ZO9t",
      "metadata": {
        "id": "HwNkbWN8ZO9t"
      },
      "source": [
        "# 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a15c3d35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a15c3d35",
        "outputId": "593f99d2-127a-47a7-dea7-f0e623bf7a3d",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Setup completed successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import mlflow.sklearn\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "# MLflow setup\n",
        "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
        "mlflow.set_experiment(\"PatchTST_Training\")\n",
        "\n",
        "print(\"🚀 Setup completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6XGNuXREZRaP",
      "metadata": {
        "id": "6XGNuXREZRaP"
      },
      "source": [
        "# 2. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "d4bf6637",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4bf6637",
        "outputId": "7fac8ad0-ecc0-4f00-c329-b29f5ee3bb50",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Loading Walmart datasets...\n",
            "✅ Data loaded successfully!\n",
            "📈 Train shape: (421570, 5)\n",
            "🔮 Test shape: (115064, 4)\n",
            "🏪 Stores shape: (45, 3)\n",
            "📋 Features shape: (8190, 12)\n"
          ]
        }
      ],
      "source": [
        "def load_walmart_data():\n",
        "    \"\"\"Load Walmart sales data with proper data types\"\"\"\n",
        "    print(\"📊 Loading Walmart datasets...\")\n",
        "\n",
        "    # Load datasets\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    features_df = pd.read_csv('features.csv')\n",
        "    stores_df = pd.read_csv('stores.csv')\n",
        "\n",
        "    # Convert date columns\n",
        "    train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "    test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
        "    features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
        "\n",
        "    print(f\"✅ Data loaded successfully!\")\n",
        "    print(f\"📈 Train shape: {train_df.shape}\")\n",
        "    print(f\"🔮 Test shape: {test_df.shape}\")\n",
        "    print(f\"🏪 Stores shape: {stores_df.shape}\")\n",
        "    print(f\"📋 Features shape: {features_df.shape}\")\n",
        "\n",
        "    return train_df, test_df, features_df, stores_df\n",
        "\n",
        "# Load data\n",
        "train_df, test_df, features_df, stores_df = load_walmart_data()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s04eoCoSZTX5",
      "metadata": {
        "id": "s04eoCoSZTX5"
      },
      "source": [
        "# 3. Data Cleaning Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "85400e94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85400e94",
        "outputId": "e4aea44d-7902-449b-b1ae-578ac7c6aeb7",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧹 Starting data cleaning process...\n",
            "✅ Data cleaning completed!\n",
            "📊 Missing values reduced from 1422431 to 0\n",
            "📈 Sales volatility reduced by 34.36%\n"
          ]
        }
      ],
      "source": [
        "class DataCleaner(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Data cleaning transformer for Walmart sales data\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.outlier_bounds = {}\n",
        "        self.fill_values = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit the data cleaner\"\"\"\n",
        "        X_clean = X.copy()\n",
        "\n",
        "        # Calculate outlier bounds for Weekly_Sales (if present)\n",
        "        if 'Weekly_Sales' in X_clean.columns:\n",
        "            Q1 = X_clean['Weekly_Sales'].quantile(0.25)\n",
        "            Q3 = X_clean['Weekly_Sales'].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            self.outlier_bounds['Weekly_Sales'] = {\n",
        "                'lower': Q1 - 1.5 * IQR,\n",
        "                'upper': Q3 + 1.5 * IQR\n",
        "            }\n",
        "\n",
        "        # Calculate fill values for missing data\n",
        "        numeric_columns = X_clean.select_dtypes(include=[np.number]).columns\n",
        "        for col in numeric_columns:\n",
        "            if col in ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']:\n",
        "                self.fill_values[col] = 0.0  # Markdowns are 0 when not present\n",
        "            else:\n",
        "                self.fill_values[col] = X_clean[col].median()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform the data\"\"\"\n",
        "        X_clean = X.copy()\n",
        "\n",
        "        # Handle missing values\n",
        "        for col, fill_value in self.fill_values.items():\n",
        "            if col in X_clean.columns:\n",
        "                X_clean[col] = X_clean[col].fillna(fill_value)\n",
        "\n",
        "        # Handle outliers in Weekly_Sales (only for training data)\n",
        "        if 'Weekly_Sales' in X_clean.columns and 'Weekly_Sales' in self.outlier_bounds:\n",
        "            bounds = self.outlier_bounds['Weekly_Sales']\n",
        "            # Cap outliers instead of removing them\n",
        "            X_clean['Weekly_Sales'] = X_clean['Weekly_Sales'].clip(\n",
        "                lower=bounds['lower'], upper=bounds['upper']\n",
        "            )\n",
        "\n",
        "        # Handle negative sales (set to 0)\n",
        "        if 'Weekly_Sales' in X_clean.columns:\n",
        "            X_clean['Weekly_Sales'] = X_clean['Weekly_Sales'].clip(lower=0)\n",
        "\n",
        "        return X_clean\n",
        "\n",
        "# Run data cleaning experiment\n",
        "with mlflow.start_run(run_name=\"PatchTST_Cleaning\"):\n",
        "    print(\"🧹 Starting data cleaning process...\")\n",
        "\n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"cleaning_method\", \"IQR_outlier_detection\")\n",
        "    mlflow.log_param(\"missing_value_strategy\", \"median_fill\")\n",
        "\n",
        "    # Initialize cleaner\n",
        "    cleaner = DataCleaner()\n",
        "\n",
        "    # Drop 'IsHoliday' from features_df before merging\n",
        "    features_df = features_df.drop(columns=['IsHoliday'])\n",
        "\n",
        "    # Safe merge\n",
        "    train_merged = train_df.merge(stores_df, on='Store', how='left')\n",
        "    train_merged = train_merged.merge(features_df, on=['Store', 'Date'], how='left')\n",
        "\n",
        "\n",
        "    # Fit and transform training data\n",
        "    train_cleaned = cleaner.fit_transform(train_merged)\n",
        "\n",
        "    # Log cleaning statistics\n",
        "    original_sales_std = train_df['Weekly_Sales'].std()\n",
        "    cleaned_sales_std = train_cleaned['Weekly_Sales'].std()\n",
        "\n",
        "    mlflow.log_metric(\"original_sales_std\", original_sales_std)\n",
        "    mlflow.log_metric(\"cleaned_sales_std\", cleaned_sales_std)\n",
        "    mlflow.log_metric(\"outlier_reduction\", (original_sales_std - cleaned_sales_std) / original_sales_std)\n",
        "\n",
        "    # Log missing value statistics\n",
        "    missing_before = train_merged.isnull().sum().sum()\n",
        "    missing_after = train_cleaned.isnull().sum().sum()\n",
        "\n",
        "    mlflow.log_metric(\"missing_values_before\", missing_before)\n",
        "    mlflow.log_metric(\"missing_values_after\", missing_after)\n",
        "\n",
        "    print(f\"✅ Data cleaning completed!\")\n",
        "    print(f\"📊 Missing values reduced from {missing_before} to {missing_after}\")\n",
        "    print(f\"📈 Sales volatility reduced by {((original_sales_std - cleaned_sales_std) / original_sales_std) * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "r1ycdjdpeIDx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1ycdjdpeIDx",
        "outputId": "590eafe8-481d-4722-9c1e-b027532f3b7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Type', 'Size',\n",
            "       'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3',\n",
            "       'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "print(train_cleaned.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WTMMRHNvZV0Z",
      "metadata": {
        "id": "WTMMRHNvZV0Z"
      },
      "source": [
        "# 4. Feature Engineering Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "8c12976e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c12976e",
        "outputId": "44e3615a-e3c9-4883-962f-1e2cb6ef144a",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚙️ Starting feature engineering process...\n",
            "✅ Feature engineering completed!\n",
            "📊 Features increased from 16 to 80\n",
            "🎯 Data quality score: 1.000\n"
          ]
        }
      ],
      "source": [
        "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Comprehensive feature engineering for time series forecasting\"\"\"\n",
        "\n",
        "    def __init__(self, lag_periods=[1, 2, 3, 4, 5, 8, 12, 52],\n",
        "                 rolling_windows=[3, 4, 8, 12, 26, 52]):\n",
        "        self.lag_periods = lag_periods\n",
        "        self.rolling_windows = rolling_windows\n",
        "        self.label_encoders = {}\n",
        "        self.fitted = False\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit the feature engineer\"\"\"\n",
        "        X_features = X.copy()\n",
        "\n",
        "        # Fit label encoders\n",
        "        for col in ['Type']:\n",
        "            if col in X_features.columns:\n",
        "                self.label_encoders[col] = LabelEncoder()\n",
        "                self.label_encoders[col].fit(X_features[col].astype(str))\n",
        "\n",
        "        self.fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform data with comprehensive feature engineering\"\"\"\n",
        "        X_features = X.copy()\n",
        "\n",
        "        # 1. Time-based features\n",
        "        X_features = self._create_time_features(X_features)\n",
        "\n",
        "        # 2. Lag features (only for training data with Weekly_Sales)\n",
        "        if 'Weekly_Sales' in X_features.columns:\n",
        "            X_features = self._create_lag_features(X_features)\n",
        "            X_features = self._create_rolling_features(X_features)\n",
        "\n",
        "        # 3. Store and department features\n",
        "        X_features = self._create_store_features(X_features)\n",
        "\n",
        "        # 4. Economic and promotional features\n",
        "        X_features = self._create_economic_features(X_features)\n",
        "\n",
        "        # 5. Holiday and seasonal features\n",
        "        X_features = self._create_holiday_features(X_features)\n",
        "\n",
        "        # 6. Interaction features\n",
        "        X_features = self._create_interaction_features(X_features)\n",
        "\n",
        "        # 7. Encode categorical variables\n",
        "        X_features = self._encode_categorical(X_features)\n",
        "\n",
        "        # Remove rows with too many NaN values (mainly from lag features)\n",
        "        if 'Weekly_Sales' in X_features.columns:\n",
        "            # For training data, we can afford to lose some rows\n",
        "            X_features = X_features.dropna(subset=['Weekly_Sales'])\n",
        "            # Fill remaining NaN values with forward fill then backward fill\n",
        "            X_features = X_features.fillna(method='ffill').fillna(method='bfill')\n",
        "        else:\n",
        "            # For test data, we need to keep all rows\n",
        "            X_features = X_features.fillna(method='ffill').fillna(method='bfill')\n",
        "            # Fill any remaining NaN with median\n",
        "            numeric_cols = X_features.select_dtypes(include=[np.number]).columns\n",
        "            X_features[numeric_cols] = X_features[numeric_cols].fillna(X_features[numeric_cols].median())\n",
        "\n",
        "        return X_features\n",
        "\n",
        "    def _create_time_features(self, df):\n",
        "        \"\"\"Create time-based features\"\"\"\n",
        "        df['Year'] = df['Date'].dt.year\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Week'] = df['Date'].dt.isocalendar().week\n",
        "        df['Day'] = df['Date'].dt.day\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        df['Quarter'] = df['Date'].dt.quarter\n",
        "        df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(int)\n",
        "        df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "\n",
        "        # Cyclical features\n",
        "        df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
        "        df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
        "        df['Week_sin'] = np.sin(2 * np.pi * df['Week'] / 52)\n",
        "        df['Week_cos'] = np.cos(2 * np.pi * df['Week'] / 52)\n",
        "        df['DayOfWeek_sin'] = np.sin(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "        df['DayOfWeek_cos'] = np.cos(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "        df['Quarter_sin'] = np.sin(2 * np.pi * df['Quarter'] / 4)\n",
        "        df['Quarter_cos'] = np.cos(2 * np.pi * df['Quarter'] / 4)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_lag_features(self, df):\n",
        "        \"\"\"Create lag features for time series\"\"\"\n",
        "        df = df.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "        for lag in self.lag_periods:\n",
        "            df[f'Weekly_Sales_lag_{lag}'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(lag)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_rolling_features(self, df):\n",
        "        \"\"\"Create rolling statistical features\"\"\"\n",
        "        df = df.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "        for window in self.rolling_windows:\n",
        "          # Rolling statistics using transform to preserve index alignment\n",
        "          df[f'Weekly_Sales_rolling_mean_{window}'] = (\n",
        "              df.groupby(['Store', 'Dept'])['Weekly_Sales']\n",
        "              .transform(lambda x: x.rolling(window=window, min_periods=1).mean())\n",
        "          )\n",
        "\n",
        "          df[f'Weekly_Sales_rolling_std_{window}'] = (\n",
        "              df.groupby(['Store', 'Dept'])['Weekly_Sales']\n",
        "              .transform(lambda x: x.rolling(window=window, min_periods=1).std())\n",
        "          )\n",
        "\n",
        "          df[f'Weekly_Sales_rolling_min_{window}'] = (\n",
        "              df.groupby(['Store', 'Dept'])['Weekly_Sales']\n",
        "              .transform(lambda x: x.rolling(window=window, min_periods=1).min())\n",
        "          )\n",
        "\n",
        "          df[f'Weekly_Sales_rolling_max_{window}'] = (\n",
        "              df.groupby(['Store', 'Dept'])['Weekly_Sales']\n",
        "              .transform(lambda x: x.rolling(window=window, min_periods=1).max())\n",
        "          )\n",
        "\n",
        "        if 'Temperature' in df.columns:\n",
        "          df[f'Temperature_rolling_mean_{window}'] = (\n",
        "              df.groupby(['Store'])['Temperature']\n",
        "              .transform(lambda x: x.rolling(window=window, min_periods=1).mean())\n",
        "          )\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_store_features(self, df):\n",
        "        \"\"\"Create store-specific features\"\"\"\n",
        "        if 'Size' in df.columns:\n",
        "            # Store size categories\n",
        "            df['Size_category'] = pd.cut(df['Size'], bins=3, labels=['Small', 'Medium', 'Large'])\n",
        "\n",
        "            # Store size relative to average\n",
        "            df['Size_relative'] = df['Size'] / df['Size'].mean()\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_economic_features(self, df):\n",
        "        \"\"\"Create economic and promotional features\"\"\"\n",
        "        # Total markdowns\n",
        "        markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "        available_markdowns = [col for col in markdown_cols if col in df.columns]\n",
        "\n",
        "        if available_markdowns:\n",
        "            df['Total_MarkDown'] = df[available_markdowns].sum(axis=1)\n",
        "            df['Has_MarkDown'] = (df['Total_MarkDown'] > 0).astype(int)\n",
        "            df['MarkDown_Count'] = (df[available_markdowns] > 0).sum(axis=1)\n",
        "\n",
        "        # Economic indicators\n",
        "        if 'CPI' in df.columns and 'Unemployment' in df.columns:\n",
        "            df['Economic_Index'] = df['CPI'] / (df['Unemployment'] + 1)  # Add 1 to avoid division by zero\n",
        "\n",
        "        # Fuel price impact\n",
        "        if 'Fuel_Price' in df.columns:\n",
        "            df['Fuel_Price_High'] = (df['Fuel_Price'] > df['Fuel_Price'].quantile(0.75)).astype(int)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_holiday_features(self, df):\n",
        "        \"\"\"Create holiday and seasonal features\"\"\"\n",
        "        # Holiday impact\n",
        "        df['IsHoliday_int'] = df['IsHoliday'].astype(int)\n",
        "\n",
        "        # Seasonal patterns\n",
        "        df['Is_BackToSchool'] = ((df['Month'] == 8) | (df['Month'] == 9)).astype(int)\n",
        "        df['Is_Christmas'] = (df['Month'] == 12).astype(int)\n",
        "        df['Is_Thanksgiving'] = ((df['Month'] == 11) & (df['Day'] >= 22)).astype(int)\n",
        "        df['Is_Summer'] = ((df['Month'] >= 6) & (df['Month'] <= 8)).astype(int)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_interaction_features(self, df):\n",
        "        \"\"\"Create interaction features\"\"\"\n",
        "        # Store type and size interaction\n",
        "        if 'Type' in df.columns and 'Size' in df.columns:\n",
        "            df['Type_Size_interaction'] = df['Type'].astype(str) + '_' + df['Size'].astype(str)\n",
        "\n",
        "        # Holiday and markdown interaction\n",
        "        if 'IsHoliday' in df.columns and 'Total_MarkDown' in df.columns:\n",
        "            df['Holiday_MarkDown'] = df['IsHoliday'].astype(int) * df['Total_MarkDown']\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _encode_categorical(self, df):\n",
        "        \"\"\"Encode categorical variables\"\"\"\n",
        "        for col, encoder in self.label_encoders.items():\n",
        "            if col in df.columns:\n",
        "                df[f'{col}_encoded'] = encoder.transform(df[col].astype(str))\n",
        "\n",
        "        return df\n",
        "\n",
        "# Run feature engineering experiment\n",
        "with mlflow.start_run(run_name=\"PatchTST_Feature_Engineering\"):\n",
        "    print(\"⚙️ Starting feature engineering process...\")\n",
        "\n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"lag_periods\", [1, 2, 3, 4, 5, 8, 12, 52])\n",
        "    mlflow.log_param(\"rolling_windows\", [3, 4, 8, 12, 26, 52])\n",
        "    mlflow.log_param(\"feature_types\", [\"time\", \"lag\", \"rolling\", \"store\", \"economic\", \"holiday\", \"interaction\"])\n",
        "\n",
        "    # Initialize feature engineer\n",
        "    feature_engineer = FeatureEngineer()\n",
        "\n",
        "    # Fit and transform training data\n",
        "    train_features = feature_engineer.fit_transform(train_cleaned)\n",
        "\n",
        "    # Log feature engineering statistics\n",
        "    original_features = train_cleaned.shape[1]\n",
        "    new_features = train_features.shape[1]\n",
        "\n",
        "    mlflow.log_metric(\"original_features\", original_features)\n",
        "    mlflow.log_metric(\"engineered_features\", new_features)\n",
        "    mlflow.log_metric(\"features_added\", new_features - original_features)\n",
        "\n",
        "    # Log data quality metrics\n",
        "    data_quality_score = (train_features.notna().sum().sum()) / (train_features.shape[0] * train_features.shape[1])\n",
        "    mlflow.log_metric(\"data_quality_score\", data_quality_score)\n",
        "\n",
        "    print(f\"✅ Feature engineering completed!\")\n",
        "    print(f\"📊 Features increased from {original_features} to {new_features}\")\n",
        "    print(f\"🎯 Data quality score: {data_quality_score:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F3cJ3_e-ZYbO",
      "metadata": {
        "id": "F3cJ3_e-ZYbO"
      },
      "source": [
        "# 5. Feature Selection and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "73ca3bab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73ca3bab",
        "outputId": "5a4ef3cf-b774-430b-adf1-6a81fec14cf5",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 Starting feature selection process...\n",
            "✅ Feature selection completed!\n",
            "📊 Selected 54 features from 80\n",
            "🏆 Top 10 features: ['Weekly_Sales_rolling_mean_3', 'Weekly_Sales_rolling_mean_4', 'Weekly_Sales_rolling_min_3', 'Weekly_Sales_rolling_min_4', 'Weekly_Sales_rolling_max_3', 'Weekly_Sales_rolling_mean_8', 'Weekly_Sales_lag_1', 'Weekly_Sales_rolling_mean_12', 'Weekly_Sales_rolling_max_4', 'Weekly_Sales_rolling_min_8']\n"
          ]
        }
      ],
      "source": [
        "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Feature selection and preprocessing for PatchTST\"\"\"\n",
        "\n",
        "    def __init__(self, max_features=50):\n",
        "        self.max_features = max_features\n",
        "        self.selected_features = None\n",
        "        self.scaler = RobustScaler()\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Select features based on importance and correlation\"\"\"\n",
        "        X_select = X.copy()\n",
        "\n",
        "        # Remove non-numeric columns and ID columns\n",
        "        exclude_cols = ['Date', 'Store', 'Dept', 'Type', 'Size_category', 'Type_Size_interaction']\n",
        "        numeric_cols = X_select.select_dtypes(include=[np.number]).columns\n",
        "        feature_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
        "\n",
        "        if y is not None:\n",
        "            # Calculate correlation with target\n",
        "            correlations = {}\n",
        "            for col in feature_cols:\n",
        "                if col != 'Weekly_Sales':\n",
        "                    corr = abs(X_select[col].corr(y))\n",
        "                    if not np.isnan(corr):\n",
        "                        correlations[col] = corr\n",
        "\n",
        "            # Sort by correlation and select top features\n",
        "            sorted_features = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
        "            self.selected_features = [feat[0] for feat in sorted_features[:self.max_features]]\n",
        "        else:\n",
        "            # For test data, use all available numeric features\n",
        "            self.selected_features = feature_cols[:self.max_features]\n",
        "\n",
        "        # Always include basic features\n",
        "        basic_features = ['Store', 'Dept', 'IsHoliday_int', 'Month', 'Week', 'DayOfWeek']\n",
        "        for feat in basic_features:\n",
        "            if feat in X_select.columns and feat not in self.selected_features:\n",
        "                self.selected_features.append(feat)\n",
        "\n",
        "        # Fit scaler on selected features\n",
        "        if self.selected_features:\n",
        "            self.scaler.fit(X_select[self.selected_features])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform data with selected features\"\"\"\n",
        "        X_transformed = X[self.selected_features].copy()\n",
        "        X_scaled = self.scaler.transform(X_transformed)\n",
        "\n",
        "        # Return as DataFrame to maintain feature names\n",
        "        return pd.DataFrame(X_scaled, columns=self.selected_features, index=X.index)\n",
        "\n",
        "# Run feature selection experiment\n",
        "with mlflow.start_run(run_name=\"PatchTST_Feature_Selection\"):\n",
        "    print(\"🎯 Starting feature selection process...\")\n",
        "\n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"selection_method\", \"correlation_based\")\n",
        "    mlflow.log_param(\"max_features\", 50)\n",
        "    mlflow.log_param(\"scaler_type\", \"RobustScaler\")\n",
        "\n",
        "    # Initialize feature selector\n",
        "    feature_selector = FeatureSelector(max_features=50)\n",
        "\n",
        "    # Fit and transform\n",
        "    X_selected = feature_selector.fit_transform(train_features, train_features['Weekly_Sales'])\n",
        "\n",
        "    # Log feature selection statistics\n",
        "    mlflow.log_metric(\"selected_features_count\", len(feature_selector.selected_features))\n",
        "    mlflow.log_metric(\"feature_reduction_ratio\", len(feature_selector.selected_features) / train_features.shape[1])\n",
        "\n",
        "    # Log top features\n",
        "    top_features = feature_selector.selected_features[:10]\n",
        "    mlflow.log_param(\"top_10_features\", top_features)\n",
        "\n",
        "    print(f\"✅ Feature selection completed!\")\n",
        "    print(f\"📊 Selected {len(feature_selector.selected_features)} features from {train_features.shape[1]}\")\n",
        "    print(f\"🏆 Top 10 features: {top_features}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ulW-cgCZay6",
      "metadata": {
        "id": "7ulW-cgCZay6"
      },
      "source": [
        "# 6. PatchTST Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "c23b351d",
      "metadata": {
        "id": "c23b351d",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "class PatchTSTDataset(Dataset):\n",
        "    \"\"\"Dataset class for PatchTST model\"\"\"\n",
        "\n",
        "    def __init__(self, X, y, seq_len=52, pred_len=1, stride=1):\n",
        "        self.X = X.values if hasattr(X, 'values') else X\n",
        "        self.y = y.values if hasattr(y, 'values') else y\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.stride = stride\n",
        "\n",
        "        # Create sequences\n",
        "        self.sequences = []\n",
        "        self.targets = []\n",
        "\n",
        "        for i in range(0, len(self.X) - seq_len - pred_len + 1, stride):\n",
        "            self.sequences.append(self.X[i:i + seq_len])\n",
        "            self.targets.append(self.y[i + seq_len:i + seq_len + pred_len])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.FloatTensor(self.sequences[idx]), torch.FloatTensor(self.targets[idx])\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Patch embedding layer for time series\"\"\"\n",
        "\n",
        "    def __init__(self, patch_len, stride, padding, in_channels, embed_dim):\n",
        "        super().__init__()\n",
        "        self.patch_len = patch_len\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "        self.conv = nn.Conv1d(in_channels, embed_dim, kernel_size=patch_len, stride=stride, padding=padding)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, n_features)\n",
        "        x = x.transpose(1, 2)  # (batch_size, n_features, seq_len)\n",
        "        x = self.conv(x)  # (batch_size, embed_dim, n_patches)\n",
        "        x = x.transpose(1, 2)  # (batch_size, n_patches, embed_dim)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "class PatchTST(nn.Module):\n",
        "    \"\"\"PatchTST model for time series forecasting\"\"\"\n",
        "\n",
        "    def __init__(self, seq_len, pred_len, patch_len, stride, n_features,\n",
        "                 embed_dim=128, num_heads=8, num_layers=3, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.patch_len = patch_len\n",
        "        self.stride = stride\n",
        "        self.n_features = n_features\n",
        "\n",
        "        # Calculate number of patches\n",
        "        self.n_patches = (seq_len - patch_len) // stride + 1\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embedding = PatchEmbedding(patch_len, stride, 0, n_features, embed_dim)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, self.n_patches, embed_dim))\n",
        "\n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=embed_dim * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            activation='gelu'\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Prediction head\n",
        "        self.prediction_head = nn.Sequential(\n",
        "            nn.Linear(embed_dim * self.n_patches, embed_dim * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim * 2, embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim, pred_len)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize weights\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Conv1d):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, n_features)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Patch embedding\n",
        "        x = self.patch_embedding(x)  # (batch_size, n_patches, embed_dim)\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = x + self.positional_encoding\n",
        "\n",
        "        # Transformer encoder\n",
        "        x = self.transformer(x)  # (batch_size, n_patches, embed_dim)\n",
        "\n",
        "        # Flatten and predict\n",
        "        x = x.view(batch_size, -1)  # (batch_size, n_patches * embed_dim)\n",
        "        output = self.prediction_head(x)  # (batch_size, pred_len)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jQQAw69PZcrf",
      "metadata": {
        "id": "jQQAw69PZcrf"
      },
      "source": [
        "# 7. Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "b72963d1",
      "metadata": {
        "id": "b72963d1",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def train_patchtst(model, train_loader, val_loader, epochs=100, lr=0.001, device='cpu'):\n",
        "    \"\"\"Train PatchTST model with advanced training techniques\"\"\"\n",
        "\n",
        "    model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_loader), epochs=epochs)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 15\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_batches = 0\n",
        "\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch_x)\n",
        "            loss = criterion(output, batch_y)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_batches += 1\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "                output = model(batch_x)\n",
        "                loss = criterion(output, batch_y)\n",
        "                val_loss += loss.item()\n",
        "                val_batches += 1\n",
        "\n",
        "        train_loss /= train_batches\n",
        "        val_loss /= val_batches\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), 'best_patchtst_model.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load('best_patchtst_model.pth'))\n",
        "\n",
        "    return model, train_losses, val_losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mvvq8Rq1Zere",
      "metadata": {
        "id": "mvvq8Rq1Zere"
      },
      "source": [
        "# 8. Complete Pipeline Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "e043cb93",
      "metadata": {
        "id": "e043cb93",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "class PatchTSTCompletePipeline:\n",
        "    \"\"\"Complete end-to-end pipeline for PatchTST model\"\"\"\n",
        "\n",
        "    def __init__(self, seq_len=52, pred_len=1, patch_len=8, stride=1,\n",
        "                 embed_dim=128, num_heads=8, num_layers=3, dropout=0.1,\n",
        "                 max_features=50, device=None):\n",
        "        self.seq_len = seq_len\n",
        "        self.pred_len = pred_len\n",
        "        self.patch_len = patch_len\n",
        "        self.stride = stride\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.max_features = max_features\n",
        "\n",
        "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # Initialize all pipeline steps as None, will set later\n",
        "        self.cleaner = DataCleaner()\n",
        "        self.feature_engineer = FeatureEngineer()\n",
        "        self.feature_selector = FeatureSelector(max_features=self.max_features)\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "\n",
        "    def fit(self, train_df, stores_df, features_df, epochs=100, batch_size=64, lr=1e-3):\n",
        "        # Merge data for training\n",
        "        df = train_df.merge(stores_df, on='Store', how='left')\n",
        "        df = df.merge(features_df, on=['Store', 'Date'], how='left')\n",
        "\n",
        "        # Clean data\n",
        "        df_clean = self.cleaner.fit_transform(df)\n",
        "\n",
        "        # Feature engineering\n",
        "        df_feat = self.feature_engineer.fit_transform(df_clean)\n",
        "\n",
        "        # Separate target and features\n",
        "        y = df_feat['Weekly_Sales']\n",
        "        X = df_feat.drop(columns=['Weekly_Sales', 'Date'])  # Drop Date as non-numeric\n",
        "\n",
        "        # Feature selection and scaling\n",
        "        X_selected = self.feature_selector.fit_transform(X, y)\n",
        "\n",
        "        # Save scaler for later use\n",
        "        self.scaler = self.feature_selector.scaler\n",
        "\n",
        "        # Create dataset and dataloaders\n",
        "        dataset = PatchTSTDataset(X_selected, y, seq_len=self.seq_len, pred_len=self.pred_len)\n",
        "        train_size = int(0.8 * len(dataset))\n",
        "        val_size = len(dataset) - train_size\n",
        "        train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Initialize model\n",
        "        n_features = X_selected.shape[1]\n",
        "        self.model = PatchTST(seq_len=self.seq_len, pred_len=self.pred_len,\n",
        "                              patch_len=self.patch_len, stride=self.stride,\n",
        "                              n_features=n_features,\n",
        "                              embed_dim=self.embed_dim,\n",
        "                              num_heads=self.num_heads,\n",
        "                              num_layers=self.num_layers,\n",
        "                              dropout=self.dropout)\n",
        "\n",
        "        # Train model\n",
        "        self.model, train_losses, val_losses = train_patchtst(\n",
        "            self.model, train_loader, val_loader, epochs=epochs, lr=lr, device=self.device\n",
        "        )\n",
        "\n",
        "    def predict(self, test_df, stores_df, features_df):\n",
        "        # Merge test data\n",
        "        df = test_df.merge(stores_df, on='Store', how='left')\n",
        "        df = df.merge(features_df, on=['Store', 'Date'], how='left')\n",
        "\n",
        "        # Clean and engineer features\n",
        "        df_clean = self.cleaner.transform(df)\n",
        "        df_feat = self.feature_engineer.transform(df_clean)\n",
        "\n",
        "        # Select features and scale\n",
        "        X = df_feat.drop(columns=['Date'])\n",
        "        X_selected = X[self.feature_selector.selected_features]\n",
        "        X_scaled = self.scaler.transform(X_selected)\n",
        "        X_scaled_df = pd.DataFrame(X_scaled, columns=self.feature_selector.selected_features)\n",
        "\n",
        "        # Create dataset and dataloader for inference\n",
        "        dataset = PatchTSTDataset(X_scaled_df, y=pd.Series([0]*len(X_scaled_df)), seq_len=self.seq_len, pred_len=self.pred_len)\n",
        "        loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "        self.model.eval()\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for batch_x, _ in loader:\n",
        "                batch_x = batch_x.to(self.device)\n",
        "                output = self.model(batch_x)\n",
        "                preds.append(output.cpu().numpy())\n",
        "\n",
        "        preds = np.concatenate(preds, axis=0)\n",
        "        return preds\n",
        "\n",
        "    def save(self, path=\"patchtst_complete_pipeline\"):\n",
        "        import os\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "        # Save model weights\n",
        "        torch.save(self.model.state_dict(), f\"{path}/model.pth\")\n",
        "        # Save scaler and pipeline components\n",
        "        joblib.dump(self.scaler, f\"{path}/scaler.pkl\")\n",
        "        joblib.dump(self.cleaner, f\"{path}/cleaner.pkl\")\n",
        "        joblib.dump(self.feature_engineer, f\"{path}/feature_engineer.pkl\")\n",
        "        joblib.dump(self.feature_selector, f\"{path}/feature_selector.pkl\")\n",
        "\n",
        "    def load(self, path=\"patchtst_complete_pipeline\"):\n",
        "        # Load scaler and pipeline components\n",
        "        self.scaler = joblib.load(f\"{path}/scaler.pkl\")\n",
        "        self.cleaner = joblib.load(f\"{path}/cleaner.pkl\")\n",
        "        self.feature_engineer = joblib.load(f\"{path}/feature_engineer.pkl\")\n",
        "        self.feature_selector = joblib.load(f\"{path}/feature_selector.pkl\")\n",
        "\n",
        "        # Recreate model and load weights\n",
        "        n_features = len(self.feature_selector.selected_features)\n",
        "        self.model = PatchTST(seq_len=self.seq_len, pred_len=self.pred_len,\n",
        "                              patch_len=self.patch_len, stride=self.stride,\n",
        "                              n_features=n_features,\n",
        "                              embed_dim=self.embed_dim,\n",
        "                              num_heads=self.num_heads,\n",
        "                              num_layers=self.num_layers,\n",
        "                              dropout=self.dropout)\n",
        "        self.model.load_state_dict(torch.load(f\"{path}/model.pth\", map_location=self.device))\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_ufxf5rmgksf",
      "metadata": {
        "id": "_ufxf5rmgksf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
