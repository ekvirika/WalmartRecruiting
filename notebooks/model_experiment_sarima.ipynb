{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb32f68",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e14487",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn mlflow\n",
    "\n",
    "# Set up Kaggle API\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930027ae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Upload your kaggle.json to Colab and run:\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14933596",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
    "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b30e6f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!unzip -q train.csv.zip\n",
    "!unzip -q stores.csv.zip\n",
    "!unzip -q test.csv.zip\n",
    "!unzip -q features.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328e5c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# SARIMA Pipeline for Walmart Sales Forecasting\n",
    "# =====================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Time series analysis libraries\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Other utilities\n",
    "import joblib\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SARIMAWalmartPipeline:\n",
    "    \"\"\"\n",
    "    Complete SARIMA pipeline for Walmart sales forecasting\n",
    "    \n",
    "    This pipeline handles:\n",
    "    1. Data loading and preprocessing\n",
    "    2. Time series stationarity analysis\n",
    "    3. Feature engineering for time series\n",
    "    4. SARIMA model selection and training\n",
    "    5. Model evaluation and validation\n",
    "    6. Forecasting and submission generation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_name: str = \"SARIMA_Training\"):\n",
    "        \"\"\"\n",
    "        Initialize the SARIMA pipeline\n",
    "        \n",
    "        Args:\n",
    "            experiment_name: Name for MLflow experiment\n",
    "        \"\"\"\n",
    "        self.experiment_name = experiment_name\n",
    "        self.models = {}\n",
    "        self.best_model = None\n",
    "        self.best_params = None\n",
    "        self.best_score = np.inf\n",
    "        self.scalers = {}\n",
    "        \n",
    "        # Set up MLflow experiment\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        \n",
    "    def load_data(self, train_path: str, test_path: str, features_path: str, \n",
    "                  stores_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Load and merge all datasets\n",
    "        \n",
    "        Args:\n",
    "            train_path: Path to training data\n",
    "            test_path: Path to test data\n",
    "            features_path: Path to features data\n",
    "            stores_path: Path to stores data\n",
    "        \"\"\"\n",
    "        print(\"Loading datasets...\")\n",
    "        \n",
    "        # Load main datasets\n",
    "        self.train_df = pd.read_csv(train_path)\n",
    "        self.test_df = pd.read_csv(test_path)\n",
    "        self.features_df = pd.read_csv(features_path)\n",
    "        self.stores_df = pd.read_csv(stores_path)\n",
    "        \n",
    "        print(f\"Train data shape: {self.train_df.shape}\")\n",
    "        print(f\"Test data shape: {self.test_df.shape}\")\n",
    "        print(f\"Features data shape: {self.features_df.shape}\")\n",
    "        print(f\"Stores data shape: {self.stores_df.shape}\")\n",
    "        \n",
    "    def preprocess_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Preprocess the data for time series analysis\n",
    "        \n",
    "        This includes:\n",
    "        - Converting date columns to datetime\n",
    "        - Merging datasets\n",
    "        - Handling missing values\n",
    "        - Creating time-based features\n",
    "        \"\"\"\n",
    "        with mlflow.start_run(run_name=\"SARIMA_Preprocessing\"):\n",
    "            print(\"Preprocessing data...\")\n",
    "            \n",
    "            # Convert date columns\n",
    "            self.train_df['Date'] = pd.to_datetime(self.train_df['Date'])\n",
    "            self.test_df['Date'] = pd.to_datetime(self.test_df['Date'])\n",
    "            self.features_df['Date'] = pd.to_datetime(self.features_df['Date'])\n",
    "            \n",
    "            # Merge training data with features and stores\n",
    "            self.train_merged = self.train_df.merge(self.features_df, on=['Store', 'Date'], how='left')\n",
    "            self.train_merged = self.train_merged.merge(self.stores_df, on='Store', how='left')\n",
    "            \n",
    "            # Merge test data with features and stores\n",
    "            self.test_merged = self.test_df.merge(self.features_df, on=['Store', 'Date'], how='left')\n",
    "            self.test_merged = self.test_merged.merge(self.stores_df, on='Store', how='left')\n",
    "            \n",
    "            # Handle missing values in features\n",
    "            numeric_columns = self.train_merged.select_dtypes(include=[np.number]).columns\n",
    "            self.train_merged[numeric_columns] = self.train_merged[numeric_columns].fillna(method='ffill')\n",
    "            self.test_merged[numeric_columns] = self.test_merged[numeric_columns].fillna(method='ffill')\n",
    "            \n",
    "            # Create time-based features\n",
    "            self._create_time_features()\n",
    "            \n",
    "            # Log preprocessing metrics\n",
    "            mlflow.log_metric(\"train_samples\", len(self.train_merged))\n",
    "            mlflow.log_metric(\"test_samples\", len(self.test_merged))\n",
    "            mlflow.log_metric(\"missing_values_train\", self.train_merged.isnull().sum().sum())\n",
    "            \n",
    "            print(\"Data preprocessing completed!\")\n",
    "            \n",
    "    def _create_time_features(self) -> None:\n",
    "        \"\"\"\n",
    "        Create time-based features for both train and test datasets\n",
    "        \"\"\"\n",
    "        for df in [self.train_merged, self.test_merged]:\n",
    "            df['Year'] = df['Date'].dt.year\n",
    "            df['Month'] = df['Date'].dt.month\n",
    "            df['Week'] = df['Date'].dt.isocalendar().week\n",
    "            df['DayOfYear'] = df['Date'].dt.dayofyear\n",
    "            df['Quarter'] = df['Date'].dt.quarter\n",
    "            df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
    "            \n",
    "            # Cyclical encoding for seasonal features\n",
    "            df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "            df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "            df['Week_sin'] = np.sin(2 * np.pi * df['Week'] / 52)\n",
    "            df['Week_cos'] = np.cos(2 * np.pi * df['Week'] / 52)\n",
    "            \n",
    "    def analyze_stationarity(self, store_id: int, dept_id: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze stationarity of time series using ADF and KPSS tests\n",
    "        \n",
    "        Args:\n",
    "            store_id: Store identifier\n",
    "            dept_id: Department identifier\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with stationarity test results\n",
    "        \"\"\"\n",
    "        # Get time series for specific store-department combination\n",
    "        ts_data = self.train_merged[\n",
    "            (self.train_merged['Store'] == store_id) & \n",
    "            (self.train_merged['Dept'] == dept_id)\n",
    "        ].set_index('Date')['Weekly_Sales'].sort_index()\n",
    "        \n",
    "        # ADF Test (Null hypothesis: series has unit root - non-stationary)\n",
    "        adf_result = adfuller(ts_data.dropna())\n",
    "        \n",
    "        # KPSS Test (Null hypothesis: series is stationary)\n",
    "        kpss_result = kpss(ts_data.dropna())\n",
    "        \n",
    "        results = {\n",
    "            'adf_statistic': adf_result[0],\n",
    "            'adf_pvalue': adf_result[1],\n",
    "            'adf_is_stationary': adf_result[1] < 0.05,\n",
    "            'kpss_statistic': kpss_result[0],\n",
    "            'kpss_pvalue': kpss_result[1],\n",
    "            'kpss_is_stationary': kpss_result[1] > 0.05,\n",
    "            'series_length': len(ts_data)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def seasonal_decomposition(self, store_id: int, dept_id: int, \n",
    "                             period: int = 52) -> None:\n",
    "        \"\"\"\n",
    "        Perform seasonal decomposition of time series\n",
    "        \n",
    "        Args:\n",
    "            store_id: Store identifier\n",
    "            dept_id: Department identifier\n",
    "            period: Seasonal period (52 for weekly data)\n",
    "        \"\"\"\n",
    "        ts_data = self.train_merged[\n",
    "            (self.train_merged['Store'] == store_id) & \n",
    "            (self.train_merged['Dept'] == dept_id)\n",
    "        ].set_index('Date')['Weekly_Sales'].sort_index()\n",
    "        \n",
    "        if len(ts_data) < 2 * period:\n",
    "            print(f\"Not enough data for decomposition. Need at least {2*period} points, got {len(ts_data)}\")\n",
    "            return\n",
    "        \n",
    "        decomposition = seasonal_decompose(ts_data, model='additive', period=period)\n",
    "        \n",
    "        fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "        decomposition.observed.plot(ax=axes[0], title='Original')\n",
    "        decomposition.trend.plot(ax=axes[1], title='Trend')\n",
    "        decomposition.seasonal.plot(ax=axes[2], title='Seasonal')\n",
    "        decomposition.resid.plot(ax=axes[3], title='Residual')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return decomposition\n",
    "    \n",
    "    def find_optimal_sarima_params(self, store_id: int, dept_id: int,\n",
    "                                  max_p: int = 3, max_d: int = 2, max_q: int = 3,\n",
    "                                  max_P: int = 2, max_D: int = 1, max_Q: int = 2,\n",
    "                                  seasonal_period: int = 52) -> Dict:\n",
    "        \"\"\"\n",
    "        Find optimal SARIMA parameters using grid search with AIC criterion\n",
    "        \n",
    "        Args:\n",
    "            store_id: Store identifier\n",
    "            dept_id: Department identifier\n",
    "            max_p, max_d, max_q: Maximum values for non-seasonal parameters\n",
    "            max_P, max_D, max_Q: Maximum values for seasonal parameters\n",
    "            seasonal_period: Seasonal period\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with best parameters and model performance\n",
    "        \"\"\"\n",
    "        with mlflow.start_run(run_name=f\"SARIMA_GridSearch_Store{store_id}_Dept{dept_id}\"):\n",
    "            \n",
    "            # Get time series data\n",
    "            ts_data = self.train_merged[\n",
    "                (self.train_merged['Store'] == store_id) & \n",
    "                (self.train_merged['Dept'] == dept_id)\n",
    "            ].set_index('Date')['Weekly_Sales'].sort_index()\n",
    "            \n",
    "            if len(ts_data) < 100:  # Need sufficient data for SARIMA\n",
    "                print(f\"Insufficient data for Store {store_id}, Dept {dept_id}\")\n",
    "                return None\n",
    "            \n",
    "            # Generate parameter combinations\n",
    "            p_values = range(0, max_p + 1)\n",
    "            d_values = range(0, max_d + 1)\n",
    "            q_values = range(0, max_q + 1)\n",
    "            P_values = range(0, max_P + 1)\n",
    "            D_values = range(0, max_D + 1)\n",
    "            Q_values = range(0, max_Q + 1)\n",
    "            \n",
    "            param_combinations = list(itertools.product(\n",
    "                p_values, d_values, q_values, P_values, D_values, Q_values\n",
    "            ))\n",
    "            \n",
    "            best_aic = np.inf\n",
    "            best_params = None\n",
    "            results = []\n",
    "            \n",
    "            print(f\"Testing {len(param_combinations)} parameter combinations...\")\n",
    "            \n",
    "            for params in tqdm(param_combinations):\n",
    "                try:\n",
    "                    p, d, q, P, D, Q = params\n",
    "                    \n",
    "                    # Fit SARIMA model\n",
    "                    model = SARIMAX(ts_data, \n",
    "                                   order=(p, d, q),\n",
    "                                   seasonal_order=(P, D, Q, seasonal_period),\n",
    "                                   enforce_stationarity=False,\n",
    "                                   enforce_invertibility=False)\n",
    "                    \n",
    "                    fitted_model = model.fit(disp=False)\n",
    "                    \n",
    "                    aic = fitted_model.aic\n",
    "                    bic = fitted_model.bic\n",
    "                    \n",
    "                    results.append({\n",
    "                        'params': params,\n",
    "                        'aic': aic,\n",
    "                        'bic': bic,\n",
    "                        'converged': fitted_model.mle_retvals['converged']\n",
    "                    })\n",
    "                    \n",
    "                    if aic < best_aic and fitted_model.mle_retvals['converged']:\n",
    "                        best_aic = aic\n",
    "                        best_params = params\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            # Log best parameters\n",
    "            if best_params:\n",
    "                p, d, q, P, D, Q = best_params\n",
    "                mlflow.log_params({\n",
    "                    'best_p': p, 'best_d': d, 'best_q': q,\n",
    "                    'best_P': P, 'best_D': D, 'best_Q': Q,\n",
    "                    'seasonal_period': seasonal_period\n",
    "                })\n",
    "                mlflow.log_metric('best_aic', best_aic)\n",
    "                mlflow.log_metric('store_id', store_id)\n",
    "                mlflow.log_metric('dept_id', dept_id)\n",
    "                \n",
    "            return {\n",
    "                'best_params': best_params,\n",
    "                'best_aic': best_aic,\n",
    "                'all_results': results,\n",
    "                'store_id': store_id,\n",
    "                'dept_id': dept_id\n",
    "            }\n",
    "    \n",
    "    def train_sarima_model(self, store_id: int, dept_id: int,\n",
    "                          params: Optional[Tuple] = None) -> None:\n",
    "        \"\"\"\n",
    "        Train SARIMA model for specific store-department combination\n",
    "        \n",
    "        Args:\n",
    "            store_id: Store identifier\n",
    "            dept_id: Department identifier\n",
    "            params: SARIMA parameters (p,d,q,P,D,Q). If None, will use grid search\n",
    "        \"\"\"\n",
    "        with mlflow.start_run(run_name=f\"SARIMA_Training_Store{store_id}_Dept{dept_id}\"):\n",
    "            \n",
    "            # Get time series data\n",
    "            ts_data = self.train_merged[\n",
    "                (self.train_merged['Store'] == store_id) & \n",
    "                (self.train_merged['Dept'] == dept_id)\n",
    "            ].set_index('Date')['Weekly_Sales'].sort_index()\n",
    "            \n",
    "            if len(ts_data) < 100:\n",
    "                print(f\"Insufficient data for Store {store_id}, Dept {dept_id}\")\n",
    "                return\n",
    "            \n",
    "            # Find optimal parameters if not provided\n",
    "            if params is None:\n",
    "                param_search = self.find_optimal_sarima_params(store_id, dept_id)\n",
    "                if param_search and param_search['best_params']:\n",
    "                    params = param_search['best_params']\n",
    "                else:\n",
    "                    print(f\"Could not find optimal parameters for Store {store_id}, Dept {dept_id}\")\n",
    "                    return\n",
    "            \n",
    "            p, d, q, P, D, Q = params\n",
    "            \n",
    "            # Split data for validation\n",
    "            train_size = int(len(ts_data) * 0.8)\n",
    "            train_data = ts_data[:train_size]\n",
    "            val_data = ts_data[train_size:]\n",
    "            \n",
    "            # Fit SARIMA model\n",
    "            model = SARIMAX(train_data,\n",
    "                           order=(p, d, q),\n",
    "                           seasonal_order=(P, D, Q, 52),\n",
    "                           enforce_stationarity=False,\n",
    "                           enforce_invertibility=False)\n",
    "            \n",
    "            fitted_model = model.fit(disp=False)\n",
    "            \n",
    "            # Make predictions on validation set\n",
    "            val_predictions = fitted_model.forecast(steps=len(val_data))\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = mean_absolute_error(val_data, val_predictions)\n",
    "            mse = mean_squared_error(val_data, val_predictions)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            # Log metrics\n",
    "            mlflow.log_params({\n",
    "                'p': p, 'd': d, 'q': q, 'P': P, 'D': D, 'Q': Q,\n",
    "                'seasonal_period': 52, 'store_id': store_id, 'dept_id': dept_id\n",
    "            })\n",
    "            mlflow.log_metrics({\n",
    "                'mae': mae, 'mse': mse, 'rmse': rmse,\n",
    "                'aic': fitted_model.aic, 'bic': fitted_model.bic\n",
    "            })\n",
    "            \n",
    "            # Store model\n",
    "            model_key = f\"store_{store_id}_dept_{dept_id}\"\n",
    "            self.models[model_key] = {\n",
    "                'model': fitted_model,\n",
    "                'params': params,\n",
    "                'metrics': {'mae': mae, 'mse': mse, 'rmse': rmse},\n",
    "                'store_id': store_id,\n",
    "                'dept_id': dept_id\n",
    "            }\n",
    "            \n",
    "            # Save model artifact\n",
    "            model_path = f\"sarima_model_store_{store_id}_dept_{dept_id}\"\n",
    "            mlflow.statsmodels.log_model(fitted_model, model_path)\n",
    "            \n",
    "            print(f\"Model trained for Store {store_id}, Dept {dept_id}\")\n",
    "            print(f\"Parameters: {params}\")\n",
    "            print(f\"Validation RMSE: {rmse:.2f}\")\n",
    "            \n",
    "    def train_all_models(self, sample_stores: Optional[List[int]] = None) -> None:\n",
    "        \"\"\"\n",
    "        Train SARIMA models for all store-department combinations\n",
    "        \n",
    "        Args:\n",
    "            sample_stores: List of store IDs to train on. If None, trains on all stores\n",
    "        \"\"\"\n",
    "        print(\"Training SARIMA models for all store-department combinations...\")\n",
    "        \n",
    "        # Get unique store-department combinations\n",
    "        combinations = self.train_merged[['Store', 'Dept']].drop_duplicates()\n",
    "        \n",
    "        if sample_stores:\n",
    "            combinations = combinations[combinations['Store'].isin(sample_stores)]\n",
    "        \n",
    "        print(f\"Training models for {len(combinations)} combinations...\")\n",
    "        \n",
    "        for _, row in tqdm(combinations.iterrows(), total=len(combinations)):\n",
    "            store_id, dept_id = row['Store'], row['Dept']\n",
    "            \n",
    "            # Check if we have enough data\n",
    "            store_dept_data = self.train_merged[\n",
    "                (self.train_merged['Store'] == store_id) & \n",
    "                (self.train_merged['Dept'] == dept_id)\n",
    "            ]\n",
    "            \n",
    "            if len(store_dept_data) >= 100:  # Minimum data requirement\n",
    "                self.train_sarima_model(store_id, dept_id)\n",
    "            else:\n",
    "                print(f\"Skipping Store {store_id}, Dept {dept_id} - insufficient data\")\n",
    "    \n",
    "    def generate_forecasts(self, forecast_horizon: int = 39) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate forecasts for test period using trained models\n",
    "        \n",
    "        Args:\n",
    "            forecast_horizon: Number of periods to forecast\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with forecasts\n",
    "        \"\"\"\n",
    "        print(\"Generating forecasts...\")\n",
    "        \n",
    "        forecasts = []\n",
    "        \n",
    "        for model_key, model_info in self.models.items():\n",
    "            store_id = model_info['store_id']\n",
    "            dept_id = model_info['dept_id']\n",
    "            fitted_model = model_info['model']\n",
    "            \n",
    "            # Get test data for this store-department combination\n",
    "            test_data = self.test_merged[\n",
    "                (self.test_merged['Store'] == store_id) & \n",
    "                (self.test_merged['Dept'] == dept_id)\n",
    "            ].copy()\n",
    "            \n",
    "            if len(test_data) > 0:\n",
    "                # Generate forecasts\n",
    "                forecast = fitted_model.forecast(steps=len(test_data))\n",
    "                \n",
    "                # Create forecast dataframe\n",
    "                forecast_df = test_data[['Store', 'Dept', 'Date']].copy()\n",
    "                forecast_df['Weekly_Sales'] = forecast\n",
    "                \n",
    "                forecasts.append(forecast_df)\n",
    "        \n",
    "        # Combine all forecasts\n",
    "        if forecasts:\n",
    "            final_forecasts = pd.concat(forecasts, ignore_index=True)\n",
    "            return final_forecasts\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def create_submission(self, forecasts_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create submission file for Kaggle\n",
    "        \n",
    "        Args:\n",
    "            forecasts_df: DataFrame with forecasts\n",
    "            \n",
    "        Returns:\n",
    "            Submission DataFrame\n",
    "        \"\"\"\n",
    "        # Merge with test data to get the Id column\n",
    "        submission = self.test_merged.merge(\n",
    "            forecasts_df, \n",
    "            on=['Store', 'Dept', 'Date'], \n",
    "            how='left'\n",
    "        )[['Id', 'Weekly_Sales']]\n",
    "        \n",
    "        # Handle missing predictions with median\n",
    "        submission['Weekly_Sales'] = submission['Weekly_Sales'].fillna(\n",
    "            submission['Weekly_Sales'].median()\n",
    "        )\n",
    "        \n",
    "        return submission\n",
    "    \n",
    "    def evaluate_model_performance(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate overall model performance using cross-validation\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with performance metrics\n",
    "        \"\"\"\n",
    "        print(\"Evaluating model performance...\")\n",
    "        \n",
    "        all_metrics = []\n",
    "        \n",
    "        for model_key, model_info in self.models.items():\n",
    "            store_id = model_info['store_id']\n",
    "            dept_id = model_info['dept_id']\n",
    "            \n",
    "            # Get time series data\n",
    "            ts_data = self.train_merged[\n",
    "                (self.train_merged['Store'] == store_id) & \n",
    "                (self.train_merged['Dept'] == dept_id)\n",
    "            ].set_index('Date')['Weekly_Sales'].sort_index()\n",
    "            \n",
    "            if len(ts_data) >= 120:  # Need enough data for cross-validation\n",
    "                # Time series cross-validation\n",
    "                tscv = TimeSeriesSplit(n_splits=3)\n",
    "                fold_metrics = []\n",
    "                \n",
    "                for train_idx, val_idx in tscv.split(ts_data):\n",
    "                    train_fold = ts_data.iloc[train_idx]\n",
    "                    val_fold = ts_data.iloc[val_idx]\n",
    "                    \n",
    "                    try:\n",
    "                        # Fit model on fold\n",
    "                        params = model_info['params']\n",
    "                        p, d, q, P, D, Q = params\n",
    "                        \n",
    "                        model = SARIMAX(train_fold,\n",
    "                                       order=(p, d, q),\n",
    "                                       seasonal_order=(P, D, Q, 52),\n",
    "                                       enforce_stationarity=False,\n",
    "                                       enforce_invertibility=False)\n",
    "                        \n",
    "                        fitted_model = model.fit(disp=False)\n",
    "                        \n",
    "                        # Forecast\n",
    "                        forecast = fitted_model.forecast(steps=len(val_fold))\n",
    "                        \n",
    "                        # Calculate metrics\n",
    "                        mae = mean_absolute_error(val_fold, forecast)\n",
    "                        mse = mean_squared_error(val_fold, forecast)\n",
    "                        rmse = np.sqrt(mse)\n",
    "                        \n",
    "                        fold_metrics.append({'mae': mae, 'mse': mse, 'rmse': rmse})\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                \n",
    "                if fold_metrics:\n",
    "                    # Average metrics across folds\n",
    "                    avg_metrics = {\n",
    "                        'mae': np.mean([m['mae'] for m in fold_metrics]),\n",
    "                        'mse': np.mean([m['mse'] for m in fold_metrics]),\n",
    "                        'rmse': np.mean([m['rmse'] for m in fold_metrics]),\n",
    "                        'store_id': store_id,\n",
    "                        'dept_id': dept_id\n",
    "                    }\n",
    "                    all_metrics.append(avg_metrics)\n",
    "        \n",
    "        if all_metrics:\n",
    "            # Calculate overall performance\n",
    "            overall_performance = {\n",
    "                'mean_mae': np.mean([m['mae'] for m in all_metrics]),\n",
    "                'mean_mse': np.mean([m['mse'] for m in all_metrics]),\n",
    "                'mean_rmse': np.mean([m['rmse'] for m in all_metrics]),\n",
    "                'median_mae': np.median([m['mae'] for m in all_metrics]),\n",
    "                'median_mse': np.median([m['mse'] for m in all_metrics]),\n",
    "                'median_rmse': np.median([m['rmse'] for m in all_metrics]),\n",
    "                'num_models': len(all_metrics)\n",
    "            }\n",
    "            \n",
    "            return overall_performance\n",
    "        else:\n",
    "            return {}\n",
    "    \n",
    "    def save_pipeline(self, filepath: str) -> None:\n",
    "        \"\"\"\n",
    "        Save the complete pipeline\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to save the pipeline\n",
    "        \"\"\"\n",
    "        pipeline_data = {\n",
    "            'models': self.models,\n",
    "            'best_model': self.best_model,\n",
    "            'best_params': self.best_params,\n",
    "            'best_score': self.best_score,\n",
    "            'experiment_name': self.experiment_name\n",
    "        }\n",
    "        \n",
    "        joblib.dump(pipeline_data, filepath)\n",
    "        print(f\"Pipeline saved to {filepath}\")\n",
    "    \n",
    "    def load_pipeline(self, filepath: str) -> None:\n",
    "        \"\"\"\n",
    "        Load a saved pipeline\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to load the pipeline from\n",
    "        \"\"\"\n",
    "        pipeline_data = joblib.load(filepath)\n",
    "        \n",
    "        self.models = pipeline_data['models']\n",
    "        self.best_model = pipeline_data['best_model']\n",
    "        self.best_params = pipeline_data['best_params']\n",
    "        self.best_score = pipeline_data['best_score']\n",
    "        self.experiment_name = pipeline_data['experiment_name']\n",
    "        \n",
    "        print(f\"Pipeline loaded from {filepath}\")\n",
    "\n",
    "# Example usage and demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize pipeline\n",
    "    pipeline = SARIMAWalmartPipeline(\"SARIMA_Walmart_Experiment\")\n",
    "    \n",
    "    # Load data (you need to provide the actual file paths)\n",
    "    # pipeline.load_data('train.csv', 'test.csv', 'features.csv', 'stores.csv')\n",
    "    \n",
    "    # Preprocess data\n",
    "    # pipeline.preprocess_data()\n",
    "    \n",
    "    # Train models (start with a sample of stores for testing)\n",
    "    # pipeline.train_all_models(sample_stores=[1, 2, 3, 4, 5])\n",
    "    \n",
    "    # Generate forecasts\n",
    "    # forecasts = pipeline.generate_forecasts()\n",
    "    \n",
    "    # Create submission\n",
    "    # submission = pipeline.create_submission(forecasts)\n",
    "    # submission.to_csv('sarima_submission.csv', index=False)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    # performance = pipeline.evaluate_model_performance()\n",
    "    # print(\"Overall Performance:\", performance)\n",
    "    \n",
    "    # Save pipeline\n",
    "    # pipeline.save_pipeline('sarima_pipeline.pkl')\n",
    "    \n",
    "    print(\"SARIMA pipeline ready for use!\") "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
