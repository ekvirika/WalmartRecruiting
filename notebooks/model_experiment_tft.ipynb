{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekvirika/WalmartRecruiting/blob/main/notebooks/model_experiment_tft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "08129789",
      "metadata": {
        "id": "08129789",
        "outputId": "197544b0-8a82-44ba-c171-8e7d3e8b1ee5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "0f0a10fe",
      "metadata": {
        "id": "0f0a10fe"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn mlflow pytorch_lightning pytorch_forecasting mlflow\n",
        "\n",
        "# Set up Kaggle API\n",
        "!pip install -q kaggle pytorch_forecasting pytorch_lightning dagshub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "03d80336",
      "metadata": {
        "id": "03d80336"
      },
      "outputs": [],
      "source": [
        "# Upload your kaggle.json to Colab and run:\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7173af2a",
      "metadata": {
        "id": "7173af2a",
        "outputId": "c6bd16a9-877c-4daa-b427-4a10e3add271",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "walmart-recruiting-store-sales-forecasting.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "replace features.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17d2db62",
      "metadata": {
        "id": "17d2db62"
      },
      "outputs": [],
      "source": [
        "!unzip -q train.csv.zip\n",
        "!unzip -q stores.csv.zip\n",
        "!unzip -q test.csv.zip\n",
        "!unzip -q features.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40b2609c",
      "metadata": {
        "id": "40b2609c"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep Learning Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from pytorch_lightning.loggers import MLFlowLogger\n",
        "\n",
        "# Time Series Libraries\n",
        "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "from pytorch_forecasting.metrics import SMAPE, MAE, RMSE\n",
        "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
        "\n",
        "# MLflow for experiment tracking\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import mlflow.sklearn\n",
        "from mlflow.models.signature import infer_signature\n",
        "import joblib\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74e4035c",
      "metadata": {
        "id": "74e4035c"
      },
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "pl.seed_everything(42)\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "2a37ab68",
      "metadata": {
        "id": "2a37ab68",
        "outputId": "d1708060-08d0-4b0c-9bec-3f4e31d4f840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373,
          "referenced_widgets": [
            "ca5c5cdde5e94c2db1c90afd8910ead0",
            "c47b38613ef94d30ad4ae2672fd8ca40"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca5c5cdde5e94c2db1c90afd8910ead0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Open the following link in your browser to authorize the client:\n",
            "https://dagshub.com/login/oauth/authorize?state=ac1c4832-add4-4427-b17b-846470eff286&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=ecd594167c26daf06717456f0bb77b7591f12da433156efb8098526d3108a4c8\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Accessing as ekvirika\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as ekvirika\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"ekvirika/WalmartRecruiting\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"ekvirika/WalmartRecruiting\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository ekvirika/WalmartRecruiting initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository ekvirika/WalmartRecruiting initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/07 20:22:16 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
            "2025/07/07 20:22:17 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
            "2025/07/07 20:22:18 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
            "2025/07/07 20:22:18 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n",
            "2025/07/07 20:22:18 WARNING mlflow.utils.autologging_utils: MLflow transformers autologging is known to be compatible with 4.35.2 <= transformers <= 4.52.4, but the installed version is 4.53.0. If you encounter errors during autologging, try upgrading / downgrading transformers to a compatible version, or try upgrading MLflow.\n",
            "2025/07/07 20:22:21 INFO mlflow.tracking.fluent: Autologging successfully enabled for transformers.\n",
            "2025/07/07 20:22:21 WARNING mlflow.spark: With Pyspark >= 3.2, PYSPARK_PIN_THREAD environment variable must be set to false for Spark datasource autologging to work.\n",
            "2025/07/07 20:22:21 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLflow experiment 'TFT_Training' is ready!\n"
          ]
        }
      ],
      "source": [
        "# MLflow Experiment Setup\n",
        "import dagshub, mlflow\n",
        "dagshub.init(repo_owner='ekvirika', repo_name='WalmartRecruiting', mlflow=True)\n",
        "mlflow.autolog()\n",
        "\n",
        "experiment_name = \"TFT_Training\"\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "print(f\"MLflow experiment '{experiment_name}' is ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a57ad43",
      "metadata": {
        "id": "2a57ad43"
      },
      "outputs": [],
      "source": [
        "# Data Loading and Initial Exploration\n",
        "def load_data():\n",
        "    \"\"\"Load and explore the Walmart dataset\"\"\"\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    stores_df = pd.read_csv('stores.csv')\n",
        "    features_df = pd.read_csv('features.csv')\n",
        "\n",
        "    print(\"Dataset shapes:\")\n",
        "    print(f\"Train: {train_df.shape}\")\n",
        "    print(f\"Test: {test_df.shape}\")\n",
        "    print(f\"Stores: {stores_df.shape}\")\n",
        "    print(f\"Features: {features_df.shape}\")\n",
        "\n",
        "    return train_df, test_df, stores_df, features_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "794c698b",
      "metadata": {
        "id": "794c698b"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81b62c42",
      "metadata": {
        "id": "81b62c42"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "train_df, test_df, stores_df, features_df = load_data()\n",
        "# Display basic info about the datasets\n",
        "print(\"\\nTrain dataset info:\")\n",
        "print(train_df.info())\n",
        "print(f\"\\nTrain dataset head:\\n{train_df.head()}\")\n",
        "\n",
        "print(\"\\nTest dataset info:\")\n",
        "print(test_df.info())\n",
        "print(f\"\\nTest dataset head:\\n{test_df.head()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3dae799",
      "metadata": {
        "id": "e3dae799"
      },
      "source": [
        "# MLflow Run: Data Cleaning and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feede334",
      "metadata": {
        "id": "feede334"
      },
      "outputs": [],
      "source": [
        "with mlflow.start_run(run_name=\"TFT_Data_Cleaning\"):\n",
        "    print(\"Starting data cleaning and preprocessing...\")\n",
        "\n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"train_shape\", train_df.shape)\n",
        "    mlflow.log_param(\"test_shape\", test_df.shape)\n",
        "\n",
        "    # Data cleaning function\n",
        "    def clean_data(df):\n",
        "        \"\"\"Clean the dataset\"\"\"\n",
        "        # Convert Date to datetime\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        # Handle missing values\n",
        "        missing_before = df.isnull().sum().sum()\n",
        "\n",
        "        # Fill missing values with appropriate methods\n",
        "        if 'Weekly_Sales' in df.columns:\n",
        "            # For training data\n",
        "            df['Weekly_Sales'].fillna(df['Weekly_Sales'].median(), inplace=True)\n",
        "\n",
        "        missing_after = df.isnull().sum().sum()\n",
        "\n",
        "        print(f\"Missing values before cleaning: {missing_before}\")\n",
        "        print(f\"Missing values after cleaning: {missing_after}\")\n",
        "\n",
        "        return df, missing_before, missing_after\n",
        "\n",
        "    # Clean training data\n",
        "    train_df, missing_before_train, missing_after_train = clean_data(train_df)\n",
        "\n",
        "    # Clean test data\n",
        "    test_df, missing_before_test, missing_after_test = clean_data(test_df)\n",
        "\n",
        "    # Log cleaning metrics\n",
        "    mlflow.log_metric(\"missing_before_train\", missing_before_train)\n",
        "    mlflow.log_metric(\"missing_after_train\", missing_after_train)\n",
        "    mlflow.log_metric(\"missing_before_test\", missing_before_test)\n",
        "    mlflow.log_metric(\"missing_after_test\", missing_after_test)\n",
        "\n",
        "    print(\"Data cleaning completed!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca7761ab",
      "metadata": {
        "id": "ca7761ab"
      },
      "source": [
        "# Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35494272",
      "metadata": {
        "id": "35494272"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mlflow\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "with mlflow.start_run(run_name=\"TFT_Feature_Engineering\"):\n",
        "    print(\"Starting feature engineering...\")\n",
        "\n",
        "    # --- Ensure Date columns are datetime ---\n",
        "    train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "    test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
        "    features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
        "    stores_df = stores_df.copy()  # in case you want to modify safely\n",
        "\n",
        "    features_df = features_df.drop(columns=['IsHoliday'], errors='ignore')\n",
        "\n",
        "    # --- Feature engineering function ---\n",
        "    def engineer_features(df):\n",
        "        df['Year'] = df['Date'].dt.year\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Week'] = df['Date'].dt.isocalendar().week\n",
        "        df['Day'] = df['Date'].dt.day\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        df['Quarter'] = df['Date'].dt.quarter\n",
        "        df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
        "        df = df.sort_values(['Store', 'Date']).reset_index(drop=True)\n",
        "        return df\n",
        "\n",
        "    # --- Apply feature engineering ---\n",
        "    train_df = engineer_features(train_df)\n",
        "    test_df = engineer_features(test_df)\n",
        "\n",
        "    # --- Handle overlapping columns more carefully ---\n",
        "    # First, identify what columns exist in each dataframe\n",
        "    print(\"Available columns:\")\n",
        "    print(f\"  train_df: {list(train_df.columns)}\")\n",
        "    print(f\"  stores_df: {list(stores_df.columns)}\")\n",
        "    print(f\"  features_df: {list(features_df.columns)}\")\n",
        "\n",
        "    # Check what columns overlap between train_df and stores_df\n",
        "    stores_overlap = [col for col in stores_df.columns if col in train_df.columns and col != 'Store']\n",
        "    print(f\"Overlapping columns with stores_df: {stores_overlap}\")\n",
        "\n",
        "    # Only drop columns that actually exist in both and cause conflicts\n",
        "    # Keep Type and Size from stores_df by dropping them from train_df if they exist\n",
        "    if 'Type' in train_df.columns and 'Type' in stores_df.columns:\n",
        "        train_df = train_df.drop(columns=['Type'], errors='ignore')\n",
        "        test_df = test_df.drop(columns=['Type'], errors='ignore')\n",
        "    if 'Size' in train_df.columns and 'Size' in stores_df.columns:\n",
        "        train_df = train_df.drop(columns=['Size'], errors='ignore')\n",
        "        test_df = test_df.drop(columns=['Size'], errors='ignore')\n",
        "\n",
        "    # --- Merge with stores data ---\n",
        "    train_df = train_df.merge(stores_df, on='Store', how='left')\n",
        "    test_df = test_df.merge(stores_df, on='Store', how='left')\n",
        "\n",
        "    # Check for overlapping columns with features_df and drop them from train/test\n",
        "    features_overlap = [col for col in features_df.columns if col in train_df.columns and col not in ['Store', 'Date']]\n",
        "    if features_overlap:\n",
        "        print(f\"Dropping overlapping columns before features merge: {features_overlap}\")\n",
        "        train_df = train_df.drop(columns=features_overlap, errors='ignore')\n",
        "        test_df = test_df.drop(columns=features_overlap, errors='ignore')\n",
        "\n",
        "    # --- Merge with features data ---\n",
        "    train_df = train_df.merge(features_df, on=['Store', 'Date'], how='left')\n",
        "    test_df = test_df.merge(features_df, on=['Store', 'Date'], how='left')\n",
        "\n",
        "    # --- Encode categorical variables ---\n",
        "    # Check if Type column exists before encoding\n",
        "    if 'Type' in train_df.columns:\n",
        "        le_type = LabelEncoder()\n",
        "        train_df['Type_encoded'] = le_type.fit_transform(train_df['Type'])\n",
        "        test_df['Type_encoded'] = le_type.transform(test_df['Type'])\n",
        "    else:\n",
        "        print(\"Warning: 'Type' column not found in dataframes after merging\")\n",
        "\n",
        "    # --- Fill missing values in numerical columns (handle train and test separately) ---\n",
        "    # Get numeric columns for each dataset separately\n",
        "    train_numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
        "    test_numeric_cols = test_df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    # Fill missing values using medians from training data\n",
        "    train_df[train_numeric_cols] = train_df[train_numeric_cols].fillna(train_df[train_numeric_cols].median())\n",
        "\n",
        "    # For test data, use training data medians for common columns, test data medians for test-only columns\n",
        "    for col in test_numeric_cols:\n",
        "        if col in train_numeric_cols:\n",
        "            # Use training data median for consistency\n",
        "            test_df[col] = test_df[col].fillna(train_df[col].median())\n",
        "        else:\n",
        "            # Use test data median for columns not in training data\n",
        "            test_df[col] = test_df[col].fillna(test_df[col].median())\n",
        "\n",
        "    # --- Log to MLflow ---\n",
        "    mlflow.log_param(\"features_after_engineering\", len(train_df.columns))\n",
        "    mlflow.log_param(\"time_features_added\", 6)\n",
        "    mlflow.log_param(\"train_numeric_cols\", len(train_numeric_cols))\n",
        "    mlflow.log_param(\"test_numeric_cols\", len(test_numeric_cols))\n",
        "\n",
        "    print(f\"Feature engineering completed!\")\n",
        "    print(f\"Train shape: {train_df.shape}\")\n",
        "    print(f\"Test shape: {test_df.shape}\")\n",
        "    print(f\"Train columns: {list(train_df.columns)}\")\n",
        "    print(f\"Test columns: {list(test_df.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d594729e",
      "metadata": {
        "id": "d594729e"
      },
      "outputs": [],
      "source": [
        "# Prepare data for TFT\n",
        "with mlflow.start_run(run_name=\"TFT_Data_Preparation\"):\n",
        "    print(\"Preparing data for TFT...\")\n",
        "\n",
        "    # Create time index\n",
        "    train_df['time_idx'] = (train_df['Date'] - train_df['Date'].min()).dt.days\n",
        "    test_df['time_idx'] = (test_df['Date'] - train_df['Date'].min()).dt.days\n",
        "\n",
        "    # Define the features for TFT\n",
        "    static_categoricals = ['Store', 'Type_encoded']\n",
        "    static_reals = ['Size']\n",
        "    time_varying_known_categoricals = ['IsHoliday', 'Month', 'Quarter', 'DayOfWeek']\n",
        "    time_varying_known_reals = ['time_idx']\n",
        "    time_varying_unknown_reals = ['Weekly_Sales']\n",
        "\n",
        "    # Add external features if available\n",
        "    external_features = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "    available_external = [col for col in external_features if col in train_df.columns]\n",
        "    time_varying_known_reals.extend(available_external)\n",
        "\n",
        "    # Create target variable\n",
        "    target = 'Weekly_Sales'\n",
        "\n",
        "    # Split data for validation\n",
        "    max_prediction_length = 12  # 12 weeks ahead\n",
        "    max_encoder_length = 52     # Use 52 weeks of history\n",
        "\n",
        "    # Calculate cutoff for validation\n",
        "    cutoff = train_df['time_idx'].max() - max_prediction_length\n",
        "\n",
        "    # Create training and validation sets\n",
        "    training_data = train_df[train_df['time_idx'] <= cutoff]\n",
        "    validation_data = train_df[train_df['time_idx'] > cutoff]\n",
        "\n",
        "    print(f\"Training data shape: {training_data.shape}\")\n",
        "    print(f\"Validation data shape: {validation_data.shape}\")\n",
        "\n",
        "    # Log data preparation parameters\n",
        "    mlflow.log_param(\"max_prediction_length\", max_prediction_length)\n",
        "    mlflow.log_param(\"max_encoder_length\", max_encoder_length)\n",
        "    mlflow.log_param(\"training_samples\", len(training_data))\n",
        "    mlflow.log_param(\"validation_samples\", len(validation_data))\n",
        "\n",
        "    print(\"Data preparation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47bcc5e6",
      "metadata": {
        "id": "47bcc5e6"
      },
      "source": [
        "# Create TFT Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac2d88cd",
      "metadata": {
        "id": "ac2d88cd"
      },
      "outputs": [],
      "source": [
        "with mlflow.start_run(run_name=\"TFT_Dataset_Creation\"):\n",
        "    print(\"Creating TFT dataset...\")\n",
        "\n",
        "    # Convert Store to string type for categorical handling\n",
        "    train_df['Store'] = train_df['Store'].astype(str)\n",
        "\n",
        "    # Also convert any other categorical columns that might be numeric\n",
        "    for col in static_categoricals + time_varying_known_categoricals:\n",
        "        if col in train_df.columns:\n",
        "            train_df[col] = train_df[col].astype(str)\n",
        "\n",
        "    # Handle missing values in target variable\n",
        "    print(f\"Missing values in {target} before handling: {train_df[target].isna().sum()}\")\n",
        "\n",
        "    # Option 1: Fill missing target values with forward fill then backward fill\n",
        "    train_df[target] = train_df.groupby('Store')[target].fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "    # Option 2: If still missing, fill with store-specific median\n",
        "    train_df[target] = train_df.groupby('Store')[target].fillna(train_df.groupby('Store')[target].transform('median'))\n",
        "\n",
        "    # Option 3: If still missing, fill with overall median\n",
        "    train_df[target] = train_df[target].fillna(train_df[target].median())\n",
        "\n",
        "    print(f\"Missing values in {target} after handling: {train_df[target].isna().sum()}\")\n",
        "\n",
        "    # Check for infinite values and handle them\n",
        "    inf_mask = np.isinf(train_df[target])\n",
        "    if inf_mask.any():\n",
        "        print(f\"Found {inf_mask.sum()} infinite values in {target}, replacing with median\")\n",
        "        train_df.loc[inf_mask, target] = train_df[target].median()\n",
        "\n",
        "    # Final check for any remaining problematic values\n",
        "    print(f\"Final check - NaN: {train_df[target].isna().sum()}, Inf: {np.isinf(train_df[target]).sum()}\")\n",
        "\n",
        "    # Create the filtered dataset for training\n",
        "    train_subset = train_df[train_df['time_idx'] <= cutoff].copy()\n",
        "    print(f\"Training subset shape: {train_subset.shape}\")\n",
        "    print(f\"Missing values in {target} in training subset: {train_subset[target].isna().sum()}\")\n",
        "\n",
        "    # Handle missing values in the training subset\n",
        "    if train_subset[target].isna().sum() > 0:\n",
        "        print(\"Handling missing values in training subset...\")\n",
        "        # Fill missing values in the training subset\n",
        "        train_subset[target] = train_subset.groupby('Store')[target].fillna(method='ffill').fillna(method='bfill')\n",
        "        train_subset[target] = train_subset.groupby('Store')[target].fillna(train_subset.groupby('Store')[target].transform('median'))\n",
        "        train_subset[target] = train_subset[target].fillna(train_subset[target].median())\n",
        "\n",
        "        # Handle infinite values\n",
        "        inf_mask = np.isinf(train_subset[target])\n",
        "        if inf_mask.any():\n",
        "            print(f\"Found {inf_mask.sum()} infinite values in training subset, replacing with median\")\n",
        "            train_subset.loc[inf_mask, target] = train_subset[target].median()\n",
        "\n",
        "    print(f\"Final training subset check - NaN: {train_subset[target].isna().sum()}, Inf: {np.isinf(train_subset[target]).sum()}\")\n",
        "\n",
        "    # Additional debugging - check all columns for missing values\n",
        "    print(\"Checking all columns for missing values:\")\n",
        "    for col in train_subset.columns:\n",
        "        missing_count = train_subset[col].isna().sum()\n",
        "        if missing_count > 0:\n",
        "            print(f\"  {col}: {missing_count} missing values\")\n",
        "\n",
        "    # Check for any problematic values in all numeric columns\n",
        "    numeric_cols = train_subset.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        inf_count = np.isinf(train_subset[col]).sum()\n",
        "        if inf_count > 0:\n",
        "            print(f\"  {col}: {inf_count} infinite values\")\n",
        "            train_subset[col] = train_subset[col].replace([np.inf, -np.inf], train_subset[col].median())\n",
        "\n",
        "    # Fill any remaining missing values in all columns\n",
        "    print(\"Filling any remaining missing values in all columns...\")\n",
        "    for col in train_subset.columns:\n",
        "        if train_subset[col].isna().sum() > 0:\n",
        "            if train_subset[col].dtype == 'object':\n",
        "                # For categorical columns, fill with mode\n",
        "                train_subset[col] = train_subset[col].fillna(train_subset[col].mode()[0] if len(train_subset[col].mode()) > 0 else 'Unknown')\n",
        "            else:\n",
        "                # For numeric columns, fill with median\n",
        "                train_subset[col] = train_subset[col].fillna(train_subset[col].median())\n",
        "\n",
        "    print(\"Final check of all columns after comprehensive cleaning:\")\n",
        "    total_missing = train_subset.isna().sum().sum()\n",
        "    print(f\"Total missing values across all columns: {total_missing}\")\n",
        "\n",
        "    # Debug: Check the actual values in Weekly_Sales\n",
        "    print(f\"Weekly_Sales statistics:\")\n",
        "    print(f\"  Min: {train_subset[target].min()}\")\n",
        "    print(f\"  Max: {train_subset[target].max()}\")\n",
        "    print(f\"  Mean: {train_subset[target].mean()}\")\n",
        "    print(f\"  Unique values with potential issues: {train_subset[target][train_subset[target] <= 0].count()}\")\n",
        "\n",
        "    # Handle edge cases that might cause issues with GroupNormalizer\n",
        "    if (train_subset[target] <= 0).any():\n",
        "        print(\"Found non-positive values in Weekly_Sales, adjusting for GroupNormalizer...\")\n",
        "        # Add a small constant to ensure all values are positive for softplus transformation\n",
        "        min_val = train_subset[target].min()\n",
        "        if min_val <= 0:\n",
        "            train_subset[target] = train_subset[target] + abs(min_val) + 1\n",
        "\n",
        "    # Try with a simpler normalizer first\n",
        "    from pytorch_forecasting.data.encoders import EncoderNormalizer\n",
        "\n",
        "    # Create the dataset with a simpler normalizer\n",
        "    training_dataset = TimeSeriesDataSet(\n",
        "        train_subset,\n",
        "        time_idx='time_idx',\n",
        "        target=target,\n",
        "        group_ids=['Store'],\n",
        "        min_encoder_length=max_encoder_length // 2,\n",
        "        max_encoder_length=max_encoder_length,\n",
        "        min_prediction_length=1,\n",
        "        max_prediction_length=max_prediction_length,\n",
        "        static_categoricals=static_categoricals,\n",
        "        static_reals=static_reals,\n",
        "        time_varying_known_categoricals=time_varying_known_categoricals,\n",
        "        time_varying_known_reals=time_varying_known_reals,\n",
        "        time_varying_unknown_reals=time_varying_unknown_reals,\n",
        "        target_normalizer=EncoderNormalizer(),  # Use simpler normalizer\n",
        "        add_relative_time_idx=True,\n",
        "        add_target_scales=True,\n",
        "        add_encoder_length=True,\n",
        "        allow_missing_timesteps=True,\n",
        "    )\n",
        "\n",
        "    # Create validation dataset\n",
        "    validation_dataset = TimeSeriesDataSet.from_dataset(\n",
        "        training_dataset,\n",
        "        train_df,\n",
        "        predict=True,\n",
        "        stop_randomization=True\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    batch_size = 128\n",
        "    train_dataloader = training_dataset.to_dataloader(\n",
        "        train=True,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=0\n",
        "    )\n",
        "    val_dataloader = validation_dataset.to_dataloader(\n",
        "        train=False,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    print(f\"Training dataset size: {len(training_dataset)}\")\n",
        "    print(f\"Validation dataset size: {len(validation_dataset)}\")\n",
        "\n",
        "    # Log dataset parameters\n",
        "    mlflow.log_param(\"batch_size\", batch_size)\n",
        "    mlflow.log_param(\"train_dataset_size\", len(training_dataset))\n",
        "    mlflow.log_param(\"val_dataset_size\", len(validation_dataset))\n",
        "\n",
        "    print(\"Dataset creation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff339cbd",
      "metadata": {
        "id": "ff339cbd"
      },
      "source": [
        "# Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5606b68f",
      "metadata": {
        "id": "5606b68f"
      },
      "outputs": [],
      "source": [
        "import lightning.pytorch as pl  # Fixed import\n",
        "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from lightning.pytorch.loggers import MLFlowLogger\n",
        "\n",
        "with mlflow.start_run(run_name=\"TFT_Model_Training\"):\n",
        "    print(\"Starting TFT model training...\")\n",
        "\n",
        "    # Enable MLflow auto-logging for PyTorch Lightning\n",
        "    mlflow.pytorch.autolog()\n",
        "\n",
        "    # Create MLflow logger\n",
        "    mlflow_logger = MLFlowLogger(\n",
        "        experiment_name=experiment_name,\n",
        "        tracking_uri=mlflow.get_tracking_uri()\n",
        "    )\n",
        "\n",
        "    # Model configuration\n",
        "    model_config = {\n",
        "        \"hidden_size\": 64,\n",
        "        \"lstm_layers\": 2,\n",
        "        \"dropout\": 0.1,\n",
        "        \"attention_head_size\": 4,\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"reduce_on_plateau_patience\": 3,\n",
        "        \"optimizer\": \"Adam\"\n",
        "    }\n",
        "\n",
        "    # Create the model\n",
        "    tft = TemporalFusionTransformer.from_dataset(\n",
        "        training_dataset,\n",
        "        hidden_size=model_config[\"hidden_size\"],\n",
        "        lstm_layers=model_config[\"lstm_layers\"],\n",
        "        dropout=model_config[\"dropout\"],\n",
        "        attention_head_size=model_config[\"attention_head_size\"],\n",
        "        output_size=1,  # Fixed for SMAPE loss\n",
        "        loss=SMAPE(),\n",
        "        learning_rate=model_config[\"learning_rate\"],\n",
        "        reduce_on_plateau_patience=model_config[\"reduce_on_plateau_patience\"],\n",
        "        optimizer=model_config[\"optimizer\"],\n",
        "    )\n",
        "\n",
        "    # Log model configuration\n",
        "    for key, value in model_config.items():\n",
        "        mlflow.log_param(key, value)\n",
        "\n",
        "    # Setup callbacks\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        verbose=True,\n",
        "        mode='min'\n",
        "    )\n",
        "\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        save_top_k=1,\n",
        "        filename='best_tft_model'\n",
        "    )\n",
        "\n",
        "    # Create trainer - FIXED: Removed deterministic=True\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=50,\n",
        "        accelerator='gpu',\n",
        "        devices=1,\n",
        "        callbacks=[early_stopping, checkpoint_callback],\n",
        "        logger=mlflow_logger,\n",
        "        enable_progress_bar=True,\n",
        "        # Removed: deterministic=True\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.fit(\n",
        "        tft,\n",
        "        train_dataloaders=train_dataloader,\n",
        "        val_dataloaders=val_dataloader\n",
        "    )\n",
        "\n",
        "    # Load best model\n",
        "    best_model = TemporalFusionTransformer.load_from_checkpoint(\n",
        "        checkpoint_callback.best_model_path\n",
        "    )\n",
        "\n",
        "    print(\"Model training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model type: {type(tft)}\")\n",
        "print(f\"Is LightningModule: {isinstance(tft, pl.LightningModule)}\")\n",
        "print(f\"Model MRO: {type(tft).__mro__}\")"
      ],
      "metadata": {
        "id": "QM_yRHTQciMk"
      },
      "id": "QM_yRHTQciMk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "43b535b5",
      "metadata": {
        "id": "43b535b5"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fad2cba",
      "metadata": {
        "id": "1fad2cba"
      },
      "outputs": [],
      "source": [
        "with mlflow.start_run(run_name=\"TFT_Model_Evaluation\"):\n",
        "    print(\"Starting model evaluation...\")\n",
        "\n",
        "    # Make predictions on validation set\n",
        "    predictions = best_model.predict(val_dataloader, return_y=True)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae = MAE()(predictions.output, predictions.y).item()\n",
        "    smape = SMAPE()(predictions.output, predictions.y).item()\n",
        "    rmse = RMSE()(predictions.output, predictions.y).item()\n",
        "\n",
        "    # Log evaluation metrics\n",
        "    mlflow.log_metric(\"val_mae\", mae)\n",
        "    mlflow.log_metric(\"val_smape\", smape)\n",
        "    mlflow.log_metric(\"val_rmse\", rmse)\n",
        "\n",
        "    print(f\"Validation MAE: {mae:.4f}\")\n",
        "    print(f\"Validation SMAPE: {smape:.4f}\")\n",
        "    print(f\"Validation RMSE: {rmse:.4f}\")\n",
        "\n",
        "    # Create prediction plots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Plot 1: Actual vs Predicted\n",
        "    actual = predictions.y.cpu().numpy().flatten()\n",
        "    predicted = predictions.output.cpu().numpy().flatten()\n",
        "\n",
        "    axes[0, 0].scatter(actual, predicted, alpha=0.5)\n",
        "    axes[0, 0].plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'r--', lw=2)\n",
        "    axes[0, 0].set_xlabel('Actual')\n",
        "    axes[0, 0].set_ylabel('Predicted')\n",
        "    axes[0, 0].set_title('Actual vs Predicted')\n",
        "\n",
        "    # Plot 2: Residuals\n",
        "    residuals = actual - predicted\n",
        "    axes[0, 1].scatter(predicted, residuals, alpha=0.5)\n",
        "    axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
        "    axes[0, 1].set_xlabel('Predicted')\n",
        "    axes[0, 1].set_ylabel('Residuals')\n",
        "    axes[0, 1].set_title('Residual Plot')\n",
        "\n",
        "    # Plot 3: Residuals histogram\n",
        "    axes[1, 0].hist(residuals, bins=50, alpha=0.7)\n",
        "    axes[1, 0].set_xlabel('Residuals')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    axes[1, 0].set_title('Residuals Distribution')\n",
        "\n",
        "    # Plot 4: Time series example\n",
        "    example_idx = 0\n",
        "    example_prediction = predictions.output[example_idx].cpu().numpy()\n",
        "    example_actual = predictions.y[example_idx].cpu().numpy()\n",
        "\n",
        "    axes[1, 1].plot(range(len(example_actual)), example_actual, 'b-', label='Actual', linewidth=2)\n",
        "    axes[1, 1].plot(range(len(example_prediction)), example_prediction, 'r--', label='Predicted', linewidth=2)\n",
        "    axes[1, 1].set_xlabel('Time Steps')\n",
        "    axes[1, 1].set_ylabel('Weekly Sales')\n",
        "    axes[1, 1].set_title('Example Prediction')\n",
        "    axes[1, 1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('tft_evaluation_plots.png', dpi=300, bbox_inches='tight')\n",
        "    mlflow.log_artifact('tft_evaluation_plots.png')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Model evaluation completed!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytorch-lightning==1.9.5 pytorch-forecasting==1.0.0\n"
      ],
      "metadata": {
        "id": "k1g9I7rYYOxM"
      },
      "id": "k1g9I7rYYOxM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2b58316",
      "metadata": {
        "id": "d2b58316"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter Tuning (Optional)\n",
        "with mlflow.start_run(run_name=\"TFT_Hyperparameter_Tuning\"):\n",
        "    print(\"Starting hyperparameter tuning...\")\n",
        "\n",
        "    # Define hyperparameter ranges\n",
        "    study = optimize_hyperparameters(\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        model_path=\"optuna_test\",\n",
        "        n_trials=10,  # Reduce for faster execution\n",
        "        max_epochs=20,\n",
        "        gradient_clip_val_range=(0.01, 1.0),\n",
        "        hidden_size_range=(32, 128),\n",
        "        lstm_layers_range=(1, 4),\n",
        "        dropout_range=(0.1, 0.3),\n",
        "        attention_head_size_range=(1, 8),\n",
        "        learning_rate_range=(0.001, 0.1),\n",
        "        use_learning_rate_finder=False,\n",
        "    )\n",
        "\n",
        "    # Log best parameters\n",
        "    best_params = study.best_params\n",
        "    for key, value in best_params.items():\n",
        "        mlflow.log_param(f\"best_{key}\", value)\n",
        "\n",
        "    mlflow.log_metric(\"best_trial_value\", study.best_value)\n",
        "\n",
        "    print(f\"Best trial value: {study.best_value}\")\n",
        "    print(f\"Best parameters: {best_params}\")\n",
        "\n",
        "# Final Model Training with Best Parameters\n",
        "with mlflow.start_run(run_name=\"TFT_Final_Model_Training\"):\n",
        "    print(\"Training final model with best parameters...\")\n",
        "\n",
        "    # Create final model with best parameters (use default if tuning was skipped)\n",
        "    final_tft = TemporalFusionTransformer.from_dataset(\n",
        "        training_dataset,\n",
        "        hidden_size=64,  # Use best params if available\n",
        "        lstm_layers=2,\n",
        "        dropout=0.1,\n",
        "        attention_head_size=4,\n",
        "        output_size=7,\n",
        "        loss=SMAPE(),\n",
        "        learning_rate=0.001,\n",
        "        reduce_on_plateau_patience=3,\n",
        "        optimizer=\"Adam\",\n",
        "    )\n",
        "\n",
        "    # Create final trainer\n",
        "    final_trainer = pl.Trainer(\n",
        "        max_epochs=100,\n",
        "        accelerator='cpu',\n",
        "        callbacks=[early_stopping, checkpoint_callback],\n",
        "        logger=mlflow_logger,\n",
        "        enable_progress_bar=True,\n",
        "        deterministic=True\n",
        "    )\n",
        "\n",
        "    # Train final model\n",
        "    final_trainer.fit(\n",
        "        final_tft,\n",
        "        train_dataloaders=train_dataloader,\n",
        "        val_dataloaders=val_dataloader\n",
        "    )\n",
        "\n",
        "    # Load best final model\n",
        "    final_best_model = TemporalFusionTransformer.load_from_checkpoint(\n",
        "        checkpoint_callback.best_model_path\n",
        "    )\n",
        "\n",
        "    print(\"Final model training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e689e65",
      "metadata": {
        "id": "2e689e65"
      },
      "source": [
        "# Create Pipeline and Save Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c7a95e7",
      "metadata": {
        "id": "9c7a95e7"
      },
      "outputs": [],
      "source": [
        "with mlflow.start_run(run_name=\"TFT_Pipeline_Creation\"):\n",
        "    print(\"Creating TFT pipeline...\")\n",
        "\n",
        "    # Create a pipeline class for TFT\n",
        "    class TFTPipeline:\n",
        "        def __init__(self, model, dataset_config, preprocessing_params):\n",
        "            self.model = model\n",
        "            self.dataset_config = dataset_config\n",
        "            self.preprocessing_params = preprocessing_params\n",
        "            self.label_encoders = {}\n",
        "\n",
        "        def preprocess(self, data):\n",
        "            \"\"\"Preprocess raw data for TFT\"\"\"\n",
        "            # Apply the same preprocessing as training\n",
        "            data = data.copy()\n",
        "\n",
        "            # Convert Date to datetime\n",
        "            data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "            # Engineer features\n",
        "            data['Year'] = data['Date'].dt.year\n",
        "            data['Month'] = data['Date'].dt.month\n",
        "            data['Week'] = data['Date'].dt.week\n",
        "            data['Day'] = data['Date'].dt.day\n",
        "            data['DayOfWeek'] = data['Date'].dt.dayofweek\n",
        "            data['Quarter'] = data['Date'].dt.quarter\n",
        "            data['IsHoliday'] = data['IsHoliday'].astype(int)\n",
        "\n",
        "            # Create time index\n",
        "            data['time_idx'] = (data['Date'] - self.preprocessing_params['min_date']).dt.days\n",
        "\n",
        "            # Handle categorical encoding\n",
        "            if 'Type' in data.columns:\n",
        "                data['Type_encoded'] = le_type.transform(data['Type'])\n",
        "\n",
        "            # Fill missing values\n",
        "            numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "            data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].median())\n",
        "\n",
        "            return data\n",
        "\n",
        "        def predict(self, data):\n",
        "            \"\"\"Make predictions on new data\"\"\"\n",
        "            # Preprocess data\n",
        "            processed_data = self.preprocess(data)\n",
        "\n",
        "            # Create dataset for prediction\n",
        "            prediction_dataset = TimeSeriesDataSet.from_dataset(\n",
        "                self.dataset_config,\n",
        "                processed_data,\n",
        "                predict=True,\n",
        "                stop_randomization=True\n",
        "            )\n",
        "\n",
        "            # Create dataloader\n",
        "            prediction_dataloader = prediction_dataset.to_dataloader(\n",
        "                train=False,\n",
        "                batch_size=128,\n",
        "                num_workers=0\n",
        "            )\n",
        "\n",
        "            # Make predictions\n",
        "            predictions = self.model.predict(prediction_dataloader)\n",
        "\n",
        "            return predictions\n",
        "\n",
        "    # Create pipeline\n",
        "    preprocessing_params = {\n",
        "        'min_date': train_df['Date'].min(),\n",
        "        'max_date': train_df['Date'].max(),\n",
        "        'features': list(train_df.columns)\n",
        "    }\n",
        "\n",
        "    tft_pipeline = TFTPipeline(\n",
        "        model=final_best_model,\n",
        "        dataset_config=training_dataset,\n",
        "        preprocessing_params=preprocessing_params\n",
        "    )\n",
        "\n",
        "    # Save pipeline\n",
        "    pipeline_path = \"tft_pipeline.pkl\"\n",
        "    joblib.dump(tft_pipeline, pipeline_path)\n",
        "\n",
        "    # Log pipeline\n",
        "    mlflow.log_artifact(pipeline_path)\n",
        "\n",
        "    # Save additional components\n",
        "    joblib.dump(le_type, \"label_encoder_type.pkl\")\n",
        "    mlflow.log_artifact(\"label_encoder_type.pkl\")\n",
        "\n",
        "    print(\"Pipeline creation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b707dc3a",
      "metadata": {
        "id": "b707dc3a"
      },
      "source": [
        "# Model Registration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6ae809c",
      "metadata": {
        "id": "c6ae809c"
      },
      "outputs": [],
      "source": [
        "with mlflow.start_run(run_name=\"TFT_Model_Registration\"):\n",
        "    print(\"Registering model...\")\n",
        "\n",
        "    # Create model signature\n",
        "    sample_input = train_df.head(100)\n",
        "    sample_output = np.random.randn(100, max_prediction_length)\n",
        "    signature = infer_signature(sample_input, sample_output)\n",
        "\n",
        "    # Register model\n",
        "    model_name = \"TFT_Walmart_Sales_Forecast\"\n",
        "\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=tft_pipeline,\n",
        "        artifact_path=\"tft_model\",\n",
        "        signature=signature,\n",
        "        registered_model_name=model_name\n",
        "    )\n",
        "\n",
        "    print(f\"Model registered as '{model_name}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3c0a3dc",
      "metadata": {
        "id": "b3c0a3dc"
      },
      "outputs": [],
      "source": [
        "print(\"TFT experiment completed successfully!\")\n",
        "print(\"All artifacts and models have been logged to MLflow\")\n",
        "print(\"Check your MLflow UI to view the experiments and model registry\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ca5c5cdde5e94c2db1c90afd8910ead0": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_c47b38613ef94d30ad4ae2672fd8ca40",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m⠦\u001b[0m Waiting for authorization\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠦</span> Waiting for authorization\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "c47b38613ef94d30ad4ae2672fd8ca40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}