{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08129789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a10fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn mlflow\n",
    "\n",
    "# Set up Kaggle API\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d80336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your kaggle.json to Colab and run:\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
    "! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7173af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
    "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d2db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q train.csv.zip\n",
    "!unzip -q stores.csv.zip\n",
    "!unzip -q test.csv.zip\n",
    "!unzip -q features.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b2609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "\n",
    "# Time Series Libraries\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, MAE, RMSE\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "pl.seed_everything(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37ab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow Experiment Setup\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "experiment_name = \"TFT_Training\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"MLflow experiment '{experiment_name}' is ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a57ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Initial Exploration\n",
    "def load_data():\n",
    "    \"\"\"Load and explore the Walmart dataset\"\"\"\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "    stores_df = pd.read_csv('stores.csv')\n",
    "    features_df = pd.read_csv('features.csv')\n",
    "    \n",
    "    print(\"Dataset shapes:\")\n",
    "    print(f\"Train: {train_df.shape}\")\n",
    "    print(f\"Test: {test_df.shape}\")\n",
    "    print(f\"Stores: {stores_df.shape}\")\n",
    "    print(f\"Features: {features_df.shape}\")\n",
    "    \n",
    "    return train_df, test_df, stores_df, features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794c698b",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b62c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df, test_df, stores_df, features_df = load_data()\n",
    "# Display basic info about the datasets\n",
    "print(\"\\nTrain dataset info:\")\n",
    "print(train_df.info())\n",
    "print(f\"\\nTrain dataset head:\\n{train_df.head()}\")\n",
    "\n",
    "print(\"\\nTest dataset info:\")\n",
    "print(test_df.info())\n",
    "print(f\"\\nTest dataset head:\\n{test_df.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dae799",
   "metadata": {},
   "source": [
    "# MLflow Run: Data Cleaning and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feede334",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"TFT_Data_Cleaning\"):\n",
    "    print(\"Starting data cleaning and preprocessing...\")\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"train_shape\", train_df.shape)\n",
    "    mlflow.log_param(\"test_shape\", test_df.shape)\n",
    "    \n",
    "    # Data cleaning function\n",
    "    def clean_data(df):\n",
    "        \"\"\"Clean the dataset\"\"\"\n",
    "        # Convert Date to datetime\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        \n",
    "        # Handle missing values\n",
    "        missing_before = df.isnull().sum().sum()\n",
    "        \n",
    "        # Fill missing values with appropriate methods\n",
    "        if 'Weekly_Sales' in df.columns:\n",
    "            # For training data\n",
    "            df['Weekly_Sales'].fillna(df['Weekly_Sales'].median(), inplace=True)\n",
    "        \n",
    "        missing_after = df.isnull().sum().sum()\n",
    "        \n",
    "        print(f\"Missing values before cleaning: {missing_before}\")\n",
    "        print(f\"Missing values after cleaning: {missing_after}\")\n",
    "        \n",
    "        return df, missing_before, missing_after\n",
    "    \n",
    "    # Clean training data\n",
    "    train_df, missing_before_train, missing_after_train = clean_data(train_df)\n",
    "    \n",
    "    # Clean test data\n",
    "    test_df, missing_before_test, missing_after_test = clean_data(test_df)\n",
    "    \n",
    "    # Log cleaning metrics\n",
    "    mlflow.log_metric(\"missing_before_train\", missing_before_train)\n",
    "    mlflow.log_metric(\"missing_after_train\", missing_after_train)\n",
    "    mlflow.log_metric(\"missing_before_test\", missing_before_test)\n",
    "    mlflow.log_metric(\"missing_after_test\", missing_after_test)\n",
    "    \n",
    "    print(\"Data cleaning completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7761ab",
   "metadata": {},
   "source": [
    "# Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35494272",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"TFT_Feature_Engineering\"):\n",
    "    print(\"Starting feature engineering...\")\n",
    "    \n",
    "    def engineer_features(df):\n",
    "        \"\"\"Engineer time-based and other features\"\"\"\n",
    "        # Time-based features\n",
    "        df['Year'] = df['Date'].dt.year\n",
    "        df['Month'] = df['Date'].dt.month\n",
    "        df['Week'] = df['Date'].dt.week\n",
    "        df['Day'] = df['Date'].dt.day\n",
    "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "        df['Quarter'] = df['Date'].dt.quarter\n",
    "        \n",
    "        # Holiday indicator (simple approach)\n",
    "        df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
    "        \n",
    "        # Sort by Store and Date\n",
    "        df = df.sort_values(['Store', 'Date']).reset_index(drop=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # Engineer features\n",
    "    train_df = engineer_features(train_df)\n",
    "    test_df = engineer_features(test_df)\n",
    "    \n",
    "    # Merge with stores and features data\n",
    "    train_df = train_df.merge(stores_df, on='Store', how='left')\n",
    "    test_df = test_df.merge(stores_df, on='Store', how='left')\n",
    "    \n",
    "    train_df = train_df.merge(features_df, on=['Store', 'Date'], how='left')\n",
    "    test_df = test_df.merge(features_df, on=['Store', 'Date'], how='left')\n",
    "    \n",
    "    # Handle categorical variables\n",
    "    le_type = LabelEncoder()\n",
    "    train_df['Type_encoded'] = le_type.fit_transform(train_df['Type'])\n",
    "    test_df['Type_encoded'] = le_type.transform(test_df['Type'])\n",
    "    \n",
    "    # Fill remaining missing values\n",
    "    numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "    train_df[numeric_cols] = train_df[numeric_cols].fillna(train_df[numeric_cols].median())\n",
    "    test_df[numeric_cols] = test_df[numeric_cols].fillna(test_df[numeric_cols].median())\n",
    "    \n",
    "    # Log feature engineering metrics\n",
    "    mlflow.log_param(\"features_after_engineering\", len(train_df.columns))\n",
    "    mlflow.log_param(\"time_features_added\", 6)\n",
    "    \n",
    "    print(f\"Feature engineering completed! New shape: {train_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d594729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for TFT\n",
    "with mlflow.start_run(run_name=\"TFT_Data_Preparation\"):\n",
    "    print(\"Preparing data for TFT...\")\n",
    "    \n",
    "    # Create time index\n",
    "    train_df['time_idx'] = (train_df['Date'] - train_df['Date'].min()).dt.days\n",
    "    test_df['time_idx'] = (test_df['Date'] - train_df['Date'].min()).dt.days\n",
    "    \n",
    "    # Define the features for TFT\n",
    "    static_categoricals = ['Store', 'Type_encoded']\n",
    "    static_reals = ['Size']\n",
    "    time_varying_known_categoricals = ['IsHoliday', 'Month', 'Quarter', 'DayOfWeek']\n",
    "    time_varying_known_reals = ['time_idx']\n",
    "    time_varying_unknown_reals = ['Weekly_Sales']\n",
    "    \n",
    "    # Add external features if available\n",
    "    external_features = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
    "    available_external = [col for col in external_features if col in train_df.columns]\n",
    "    time_varying_known_reals.extend(available_external)\n",
    "    \n",
    "    # Create target variable\n",
    "    target = 'Weekly_Sales'\n",
    "    \n",
    "    # Split data for validation\n",
    "    max_prediction_length = 12  # 12 weeks ahead\n",
    "    max_encoder_length = 52     # Use 52 weeks of history\n",
    "    \n",
    "    # Calculate cutoff for validation\n",
    "    cutoff = train_df['time_idx'].max() - max_prediction_length\n",
    "    \n",
    "    # Create training and validation sets\n",
    "    training_data = train_df[train_df['time_idx'] <= cutoff]\n",
    "    validation_data = train_df[train_df['time_idx'] > cutoff]\n",
    "    \n",
    "    print(f\"Training data shape: {training_data.shape}\")\n",
    "    print(f\"Validation data shape: {validation_data.shape}\")\n",
    "    \n",
    "    # Log data preparation parameters\n",
    "    mlflow.log_param(\"max_prediction_length\", max_prediction_length)\n",
    "    mlflow.log_param(\"max_encoder_length\", max_encoder_length)\n",
    "    mlflow.log_param(\"training_samples\", len(training_data))\n",
    "    mlflow.log_param(\"validation_samples\", len(validation_data))\n",
    "    \n",
    "    print(\"Data preparation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bcc5e6",
   "metadata": {},
   "source": [
    "# Create TFT Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d88cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"TFT_Dataset_Creation\"):\n",
    "    print(\"Creating TFT dataset...\")\n",
    "    \n",
    "    # Create the dataset\n",
    "    training_dataset = TimeSeriesDataSet(\n",
    "        train_df[train_df['time_idx'] <= cutoff],\n",
    "        time_idx='time_idx',\n",
    "        target=target,\n",
    "        group_ids=['Store'],\n",
    "        min_encoder_length=max_encoder_length // 2,\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=1,\n",
    "        max_prediction_length=max_prediction_length,\n",
    "        static_categoricals=static_categoricals,\n",
    "        static_reals=static_reals,\n",
    "        time_varying_known_categoricals=time_varying_known_categoricals,\n",
    "        time_varying_known_reals=time_varying_known_reals,\n",
    "        time_varying_unknown_reals=time_varying_unknown_reals,\n",
    "        target_normalizer=GroupNormalizer(groups=['Store'], transformation='softplus'),\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "        allow_missing_timesteps=True,\n",
    "    )\n",
    "    \n",
    "    # Create validation dataset\n",
    "    validation_dataset = TimeSeriesDataSet.from_dataset(\n",
    "        training_dataset, \n",
    "        train_df, \n",
    "        predict=True, \n",
    "        stop_randomization=True\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    batch_size = 128\n",
    "    train_dataloader = training_dataset.to_dataloader(\n",
    "        train=True, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=0\n",
    "    )\n",
    "    val_dataloader = validation_dataset.to_dataloader(\n",
    "        train=False, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    print(f\"Training dataset size: {len(training_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(validation_dataset)}\")\n",
    "    \n",
    "    # Log dataset parameters\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"train_dataset_size\", len(training_dataset))\n",
    "    mlflow.log_param(\"val_dataset_size\", len(validation_dataset))\n",
    "    \n",
    "    print(\"Dataset creation completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff339cbd",
   "metadata": {},
   "source": [
    "# Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606b68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with mlflow.start_run(run_name=\"TFT_Model_Training\"):\n",
    "    print(\"Starting TFT model training...\")\n",
    "    \n",
    "    # Enable MLflow auto-logging for PyTorch Lightning\n",
    "    mlflow.pytorch.autolog()\n",
    "    \n",
    "    # Create MLflow logger\n",
    "    mlflow_logger = MLFlowLogger(\n",
    "        experiment_name=experiment_name,\n",
    "        tracking_uri=mlflow.get_tracking_uri()\n",
    "    )\n",
    "    \n",
    "    # Model configuration\n",
    "    model_config = {\n",
    "        \"hidden_size\": 64,\n",
    "        \"lstm_layers\": 2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"attention_head_size\": 4,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"reduce_on_plateau_patience\": 3,\n",
    "        \"optimizer\": \"Adam\"\n",
    "    }\n",
    "    \n",
    "    # Create the model\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        training_dataset,\n",
    "        hidden_size=model_config[\"hidden_size\"],\n",
    "        lstm_layers=model_config[\"lstm_layers\"],\n",
    "        dropout=model_config[\"dropout\"],\n",
    "        attention_head_size=model_config[\"attention_head_size\"],\n",
    "        output_size=7,  # 7 quantiles by default\n",
    "        loss=SMAPE(),\n",
    "        learning_rate=model_config[\"learning_rate\"],\n",
    "        reduce_on_plateau_patience=model_config[\"reduce_on_plateau_patience\"],\n",
    "        optimizer=model_config[\"optimizer\"],\n",
    "    )\n",
    "    \n",
    "    # Log model configuration\n",
    "    for key, value in model_config.items():\n",
    "        mlflow.log_param(key, value)\n",
    "    \n",
    "    # Setup callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        verbose=True,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_top_k=1,\n",
    "        filename='best_tft_model'\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        accelerator='cpu',  # Change to 'gpu' if available\n",
    "        callbacks=[early_stopping, checkpoint_callback],\n",
    "        logger=mlflow_logger,\n",
    "        enable_progress_bar=True,\n",
    "        deterministic=True\n",
    "    )\n",
    "    \n",
    "     # Train the model\n",
    "    trainer.fit(\n",
    "        tft, \n",
    "        train_dataloaders=train_dataloader, \n",
    "        val_dataloaders=val_dataloader\n",
    "    )\n",
    "    \n",
    "    # Load best model\n",
    "    best_model = TemporalFusionTransformer.load_from_checkpoint(\n",
    "        checkpoint_callback.best_model_path\n",
    "    )\n",
    "    \n",
    "    print(\"Model training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b535b5",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fad2cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"TFT_Model_Evaluation\"):\n",
    "    print(\"Starting model evaluation...\")\n",
    "    \n",
    "    # Make predictions on validation set\n",
    "    predictions = best_model.predict(val_dataloader, return_y=True)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = MAE()(predictions.output, predictions.y).item()\n",
    "    smape = SMAPE()(predictions.output, predictions.y).item()\n",
    "    rmse = RMSE()(predictions.output, predictions.y).item()\n",
    "    \n",
    "    # Log evaluation metrics\n",
    "    mlflow.log_metric(\"val_mae\", mae)\n",
    "    mlflow.log_metric(\"val_smape\", smape)\n",
    "    mlflow.log_metric(\"val_rmse\", rmse)\n",
    "    \n",
    "    print(f\"Validation MAE: {mae:.4f}\")\n",
    "    print(f\"Validation SMAPE: {smape:.4f}\")\n",
    "    print(f\"Validation RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    # Create prediction plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Actual vs Predicted\n",
    "    actual = predictions.y.cpu().numpy().flatten()\n",
    "    predicted = predictions.output.cpu().numpy().flatten()\n",
    "    \n",
    "    axes[0, 0].scatter(actual, predicted, alpha=0.5)\n",
    "    axes[0, 0].plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'r--', lw=2)\n",
    "    axes[0, 0].set_xlabel('Actual')\n",
    "    axes[0, 0].set_ylabel('Predicted')\n",
    "    axes[0, 0].set_title('Actual vs Predicted')\n",
    "    \n",
    "    # Plot 2: Residuals\n",
    "    residuals = actual - predicted\n",
    "    axes[0, 1].scatter(predicted, residuals, alpha=0.5)\n",
    "    axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0, 1].set_xlabel('Predicted')\n",
    "    axes[0, 1].set_ylabel('Residuals')\n",
    "    axes[0, 1].set_title('Residual Plot')\n",
    "    \n",
    "    # Plot 3: Residuals histogram\n",
    "    axes[1, 0].hist(residuals, bins=50, alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Residuals')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Residuals Distribution')\n",
    "    \n",
    "    # Plot 4: Time series example\n",
    "    example_idx = 0\n",
    "    example_prediction = predictions.output[example_idx].cpu().numpy()\n",
    "    example_actual = predictions.y[example_idx].cpu().numpy()\n",
    "    \n",
    "    axes[1, 1].plot(range(len(example_actual)), example_actual, 'b-', label='Actual', linewidth=2)\n",
    "    axes[1, 1].plot(range(len(example_prediction)), example_prediction, 'r--', label='Predicted', linewidth=2)\n",
    "    axes[1, 1].set_xlabel('Time Steps')\n",
    "    axes[1, 1].set_ylabel('Weekly Sales')\n",
    "    axes[1, 1].set_title('Example Prediction')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tft_evaluation_plots.png', dpi=300, bbox_inches='tight')\n",
    "    mlflow.log_artifact('tft_evaluation_plots.png')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Model evaluation completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b58316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning (Optional)\n",
    "with mlflow.start_run(run_name=\"TFT_Hyperparameter_Tuning\"):\n",
    "    print(\"Starting hyperparameter tuning...\")\n",
    "    \n",
    "    # Define hyperparameter ranges\n",
    "    study = optimize_hyperparameters(\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        model_path=\"optuna_test\",\n",
    "        n_trials=10,  # Reduce for faster execution\n",
    "        max_epochs=20,\n",
    "        gradient_clip_val_range=(0.01, 1.0),\n",
    "        hidden_size_range=(32, 128),\n",
    "        lstm_layers_range=(1, 4),\n",
    "        dropout_range=(0.1, 0.3),\n",
    "        attention_head_size_range=(1, 8),\n",
    "        learning_rate_range=(0.001, 0.1),\n",
    "        use_learning_rate_finder=False,\n",
    "    )\n",
    "    \n",
    "    # Log best parameters\n",
    "    best_params = study.best_params\n",
    "    for key, value in best_params.items():\n",
    "        mlflow.log_param(f\"best_{key}\", value)\n",
    "    \n",
    "    mlflow.log_metric(\"best_trial_value\", study.best_value)\n",
    "    \n",
    "    print(f\"Best trial value: {study.best_value}\")\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Final Model Training with Best Parameters\n",
    "with mlflow.start_run(run_name=\"TFT_Final_Model_Training\"):\n",
    "    print(\"Training final model with best parameters...\")\n",
    "    \n",
    "    # Create final model with best parameters (use default if tuning was skipped)\n",
    "    final_tft = TemporalFusionTransformer.from_dataset(\n",
    "        training_dataset,\n",
    "        hidden_size=64,  # Use best params if available\n",
    "        lstm_layers=2,\n",
    "        dropout=0.1,\n",
    "        attention_head_size=4,\n",
    "        output_size=7,\n",
    "        loss=SMAPE(),\n",
    "        learning_rate=0.001,\n",
    "        reduce_on_plateau_patience=3,\n",
    "        optimizer=\"Adam\",\n",
    "    )\n",
    "    \n",
    "    # Create final trainer\n",
    "    final_trainer = pl.Trainer(\n",
    "        max_epochs=100,\n",
    "        accelerator='cpu',\n",
    "        callbacks=[early_stopping, checkpoint_callback],\n",
    "        logger=mlflow_logger,\n",
    "        enable_progress_bar=True,\n",
    "        deterministic=True\n",
    "    )\n",
    "    \n",
    "    # Train final model\n",
    "    final_trainer.fit(\n",
    "        final_tft,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader\n",
    "    )\n",
    "    \n",
    "    # Load best final model\n",
    "    final_best_model = TemporalFusionTransformer.load_from_checkpoint(\n",
    "        checkpoint_callback.best_model_path\n",
    "    )\n",
    "    \n",
    "    print(\"Final model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e689e65",
   "metadata": {},
   "source": [
    "# Create Pipeline and Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a95e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"TFT_Pipeline_Creation\"):\n",
    "    print(\"Creating TFT pipeline...\")\n",
    "    \n",
    "    # Create a pipeline class for TFT\n",
    "    class TFTPipeline:\n",
    "        def __init__(self, model, dataset_config, preprocessing_params):\n",
    "            self.model = model\n",
    "            self.dataset_config = dataset_config\n",
    "            self.preprocessing_params = preprocessing_params\n",
    "            self.label_encoders = {}\n",
    "        \n",
    "        def preprocess(self, data):\n",
    "            \"\"\"Preprocess raw data for TFT\"\"\"\n",
    "            # Apply the same preprocessing as training\n",
    "            data = data.copy()\n",
    "            \n",
    "            # Convert Date to datetime\n",
    "            data['Date'] = pd.to_datetime(data['Date'])\n",
    "            \n",
    "            # Engineer features\n",
    "            data['Year'] = data['Date'].dt.year\n",
    "            data['Month'] = data['Date'].dt.month\n",
    "            data['Week'] = data['Date'].dt.week\n",
    "            data['Day'] = data['Date'].dt.day\n",
    "            data['DayOfWeek'] = data['Date'].dt.dayofweek\n",
    "            data['Quarter'] = data['Date'].dt.quarter\n",
    "            data['IsHoliday'] = data['IsHoliday'].astype(int)\n",
    "            \n",
    "            # Create time index\n",
    "            data['time_idx'] = (data['Date'] - self.preprocessing_params['min_date']).dt.days\n",
    "            \n",
    "            # Handle categorical encoding\n",
    "            if 'Type' in data.columns:\n",
    "                data['Type_encoded'] = le_type.transform(data['Type'])\n",
    "            \n",
    "            # Fill missing values\n",
    "            numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "            data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].median())\n",
    "            \n",
    "            return data\n",
    "        \n",
    "        def predict(self, data):\n",
    "            \"\"\"Make predictions on new data\"\"\"\n",
    "            # Preprocess data\n",
    "            processed_data = self.preprocess(data)\n",
    "            \n",
    "            # Create dataset for prediction\n",
    "            prediction_dataset = TimeSeriesDataSet.from_dataset(\n",
    "                self.dataset_config,\n",
    "                processed_data,\n",
    "                predict=True,\n",
    "                stop_randomization=True\n",
    "            )\n",
    "            \n",
    "            # Create dataloader\n",
    "            prediction_dataloader = prediction_dataset.to_dataloader(\n",
    "                train=False,\n",
    "                batch_size=128,\n",
    "                num_workers=0\n",
    "            )\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = self.model.predict(prediction_dataloader)\n",
    "            \n",
    "            return predictions\n",
    "    \n",
    "    # Create pipeline\n",
    "    preprocessing_params = {\n",
    "        'min_date': train_df['Date'].min(),\n",
    "        'max_date': train_df['Date'].max(),\n",
    "        'features': list(train_df.columns)\n",
    "    }\n",
    "    \n",
    "    tft_pipeline = TFTPipeline(\n",
    "        model=final_best_model,\n",
    "        dataset_config=training_dataset,\n",
    "        preprocessing_params=preprocessing_params\n",
    "    )\n",
    "    \n",
    "    # Save pipeline\n",
    "    pipeline_path = \"tft_pipeline.pkl\"\n",
    "    joblib.dump(tft_pipeline, pipeline_path)\n",
    "    \n",
    "    # Log pipeline\n",
    "    mlflow.log_artifact(pipeline_path)\n",
    "    \n",
    "    # Save additional components\n",
    "    joblib.dump(le_type, \"label_encoder_type.pkl\")\n",
    "    mlflow.log_artifact(\"label_encoder_type.pkl\")\n",
    "    \n",
    "    print(\"Pipeline creation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b707dc3a",
   "metadata": {},
   "source": [
    "# Model Registration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"TFT_Model_Registration\"):\n",
    "    print(\"Registering model...\")\n",
    "    \n",
    "    # Create model signature\n",
    "    sample_input = train_df.head(100)\n",
    "    sample_output = np.random.randn(100, max_prediction_length)\n",
    "    signature = infer_signature(sample_input, sample_output)\n",
    "    \n",
    "    # Register model\n",
    "    model_name = \"TFT_Walmart_Sales_Forecast\"\n",
    "    \n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=tft_pipeline,\n",
    "        artifact_path=\"tft_model\",\n",
    "        signature=signature,\n",
    "        registered_model_name=model_name\n",
    "    )\n",
    "    \n",
    "    print(f\"Model registered as '{model_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c0a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TFT experiment completed successfully!\")\n",
    "print(\"All artifacts and models have been logged to MLflow\")\n",
    "print(\"Check your MLflow UI to view the experiments and model registry\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
