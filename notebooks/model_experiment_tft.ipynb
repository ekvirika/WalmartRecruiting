{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekvirika/WalmartRecruiting/blob/main/notebooks/model_experiment_tft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "08129789",
      "metadata": {
        "id": "08129789",
        "outputId": "1c385a2d-91be-4f1a-9a5a-b520db11c10c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0f0a10fe",
      "metadata": {
        "id": "0f0a10fe",
        "outputId": "a2ce46a0-6dc8-4709-fcaf-83317ec357a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting mlflow\n",
            "  Downloading mlflow-3.1.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pytorch_forecasting\n",
            "  Downloading pytorch_forecasting-1.4.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (24.2)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting mlflow-skinny==3.1.1 (from mlflow)\n",
            "  Downloading mlflow_skinny-3.1.1-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.41)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.1.1->mlflow)\n",
            "  Downloading databricks_sdk-0.57.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.115.14)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (8.7.0)\n",
            "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.1.1->mlflow)\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.1.1->mlflow)\n",
            "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.5.3)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.1->mlflow) (0.35.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (4.67.1)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.7.4-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting lightning<3.0.0,>=2.0.0 (from pytorch_forecasting)\n",
            "  Downloading lightning-2.5.2-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.4.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.11.15)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.20.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==3.1.1->mlflow) (0.46.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.1->mlflow) (3.23.0)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow)\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1->mlflow-skinny==3.1.1->mlflow) (0.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.6.1)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow-3.1.1-py3-none-any.whl (24.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-3.1.1-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.2-py3-none-any.whl (825 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_forecasting-1.4.0-py3-none-any.whl (260 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.2-py3-none-any.whl (821 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading torchmetrics-1.7.4-py3-none-any.whl (963 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.57.0-py3-none-any.whl (733 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m733.8/733.8 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, gunicorn, graphql-core, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, graphql-relay, docker, alembic, opentelemetry-semantic-conventions, nvidia-cusolver-cu12, graphene, databricks-sdk, opentelemetry-sdk, torchmetrics, mlflow-skinny, pytorch_lightning, mlflow, lightning, pytorch_forecasting\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed alembic-1.16.2 databricks-sdk-0.57.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 lightning-2.5.2 lightning-utilities-0.14.3 mlflow-3.1.1 mlflow-skinny-3.1.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 opentelemetry-api-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 pytorch_forecasting-1.4.0 pytorch_lightning-2.5.2 torchmetrics-1.7.4\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn mlflow pytorch_lightning pytorch_forecasting\n",
        "\n",
        "# Set up Kaggle API\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "03d80336",
      "metadata": {
        "id": "03d80336"
      },
      "outputs": [],
      "source": [
        "# Upload your kaggle.json to Colab and run:\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7173af2a",
      "metadata": {
        "id": "7173af2a",
        "outputId": "06fac68d-a9e6-4b19-9736-5260a48755fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 635MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "17d2db62",
      "metadata": {
        "id": "17d2db62",
        "outputId": "296b974b-14c8-4a95-bb4e-c881c64c3703",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open stores.csv.zip, stores.csv.zip.zip or stores.csv.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "!unzip -q train.csv.zip\n",
        "!unzip -q stores.csv.zip\n",
        "!unzip -q test.csv.zip\n",
        "!unzip -q features.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "40b2609c",
      "metadata": {
        "id": "40b2609c"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep Learning Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from pytorch_lightning.loggers import MLFlowLogger\n",
        "\n",
        "# Time Series Libraries\n",
        "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "from pytorch_forecasting.metrics import SMAPE, MAE, RMSE\n",
        "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
        "\n",
        "# MLflow for experiment tracking\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import mlflow.sklearn\n",
        "from mlflow.models.signature import infer_signature\n",
        "import joblib\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "74e4035c",
      "metadata": {
        "id": "74e4035c",
        "outputId": "a429f1c3-0b68-402c-f621-9b59dc74d06c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "pl.seed_everything(42)\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2a37ab68",
      "metadata": {
        "id": "2a37ab68",
        "outputId": "3356f0d7-8c82-405e-a916-006c497d10e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 15:32:47 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
            "2025/07/06 15:32:47 INFO mlflow.store.db.utils: Updating database tables\n",
            "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
            "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
            "INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step\n",
            "INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
            "INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
            "INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
            "INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
            "INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
            "INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!\n",
            "INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
            "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.\n",
            "INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!\n",
            "INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
            "INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
            "INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
            "INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
            "INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
            "INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
            "INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
            "INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
            "INFO  [alembic.runtime.migration] Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
            "INFO  [alembic.runtime.migration] Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
            "INFO  [alembic.runtime.migration] Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
            "INFO  [alembic.runtime.migration] Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
            "INFO  [alembic.runtime.migration] Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
            "INFO  [alembic.runtime.migration] Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
            "INFO  [alembic.runtime.migration] Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
            "INFO  [alembic.runtime.migration] Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
            "INFO  [alembic.runtime.migration] Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
            "INFO  [alembic.runtime.migration] Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
            "INFO  [alembic.runtime.migration] Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
            "INFO  [alembic.runtime.migration] Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
            "INFO  [alembic.runtime.migration] Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
            "INFO  [alembic.runtime.migration] Running upgrade 0584bdc529eb -> 400f98739977, add logged model tables\n",
            "INFO  [alembic.runtime.migration] Running upgrade 400f98739977 -> 6953534de441, add step to inputs table\n",
            "INFO  [alembic.runtime.migration] Running upgrade 6953534de441 -> bda7b8c39065, increase_model_version_tag_value_limit\n",
            "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
            "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
            "2025/07/06 15:32:48 INFO mlflow.tracking.fluent: Experiment with name 'TFT_Training' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLflow experiment 'TFT_Training' is ready!\n"
          ]
        }
      ],
      "source": [
        "# MLflow Experiment Setup\n",
        "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
        "experiment_name = \"TFT_Training\"\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "print(f\"MLflow experiment '{experiment_name}' is ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2a57ad43",
      "metadata": {
        "id": "2a57ad43"
      },
      "outputs": [],
      "source": [
        "# Data Loading and Initial Exploration\n",
        "def load_data():\n",
        "    \"\"\"Load and explore the Walmart dataset\"\"\"\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    stores_df = pd.read_csv('stores.csv')\n",
        "    features_df = pd.read_csv('features.csv')\n",
        "\n",
        "    print(\"Dataset shapes:\")\n",
        "    print(f\"Train: {train_df.shape}\")\n",
        "    print(f\"Test: {test_df.shape}\")\n",
        "    print(f\"Stores: {stores_df.shape}\")\n",
        "    print(f\"Features: {features_df.shape}\")\n",
        "\n",
        "    return train_df, test_df, stores_df, features_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "794c698b",
      "metadata": {
        "id": "794c698b"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "81b62c42",
      "metadata": {
        "id": "81b62c42",
        "outputId": "0084cf66-fdc3-4aea-92e2-249aba965cbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shapes:\n",
            "Train: (421570, 5)\n",
            "Test: (115064, 4)\n",
            "Stores: (45, 3)\n",
            "Features: (8190, 12)\n",
            "\n",
            "Train dataset info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 421570 entries, 0 to 421569\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count   Dtype  \n",
            "---  ------        --------------   -----  \n",
            " 0   Store         421570 non-null  int64  \n",
            " 1   Dept          421570 non-null  int64  \n",
            " 2   Date          421570 non-null  object \n",
            " 3   Weekly_Sales  421570 non-null  float64\n",
            " 4   IsHoliday     421570 non-null  bool   \n",
            "dtypes: bool(1), float64(1), int64(2), object(1)\n",
            "memory usage: 13.3+ MB\n",
            "None\n",
            "\n",
            "Train dataset head:\n",
            "   Store  Dept        Date  Weekly_Sales  IsHoliday\n",
            "0      1     1  2010-02-05      24924.50      False\n",
            "1      1     1  2010-02-12      46039.49       True\n",
            "2      1     1  2010-02-19      41595.55      False\n",
            "3      1     1  2010-02-26      19403.54      False\n",
            "4      1     1  2010-03-05      21827.90      False\n",
            "\n",
            "Test dataset info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 115064 entries, 0 to 115063\n",
            "Data columns (total 4 columns):\n",
            " #   Column     Non-Null Count   Dtype \n",
            "---  ------     --------------   ----- \n",
            " 0   Store      115064 non-null  int64 \n",
            " 1   Dept       115064 non-null  int64 \n",
            " 2   Date       115064 non-null  object\n",
            " 3   IsHoliday  115064 non-null  bool  \n",
            "dtypes: bool(1), int64(2), object(1)\n",
            "memory usage: 2.7+ MB\n",
            "None\n",
            "\n",
            "Test dataset head:\n",
            "   Store  Dept        Date  IsHoliday\n",
            "0      1     1  2012-11-02      False\n",
            "1      1     1  2012-11-09      False\n",
            "2      1     1  2012-11-16      False\n",
            "3      1     1  2012-11-23       True\n",
            "4      1     1  2012-11-30      False\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "train_df, test_df, stores_df, features_df = load_data()\n",
        "# Display basic info about the datasets\n",
        "print(\"\\nTrain dataset info:\")\n",
        "print(train_df.info())\n",
        "print(f\"\\nTrain dataset head:\\n{train_df.head()}\")\n",
        "\n",
        "print(\"\\nTest dataset info:\")\n",
        "print(test_df.info())\n",
        "print(f\"\\nTest dataset head:\\n{test_df.head()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3dae799",
      "metadata": {
        "id": "e3dae799"
      },
      "source": [
        "# MLflow Run: Data Cleaning and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "feede334",
      "metadata": {
        "id": "feede334",
        "outputId": "eb0df033-2f19-47a8-c801-e9ef394d96bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data cleaning and preprocessing...\n",
            "Missing values before cleaning: 0\n",
            "Missing values after cleaning: 0\n",
            "Missing values before cleaning: 0\n",
            "Missing values after cleaning: 0\n",
            "Data cleaning completed!\n"
          ]
        }
      ],
      "source": [
        "with mlflow.start_run(run_name=\"TFT_Data_Cleaning\"):\n",
        "    print(\"Starting data cleaning and preprocessing...\")\n",
        "\n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"train_shape\", train_df.shape)\n",
        "    mlflow.log_param(\"test_shape\", test_df.shape)\n",
        "\n",
        "    # Data cleaning function\n",
        "    def clean_data(df):\n",
        "        \"\"\"Clean the dataset\"\"\"\n",
        "        # Convert Date to datetime\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        # Handle missing values\n",
        "        missing_before = df.isnull().sum().sum()\n",
        "\n",
        "        # Fill missing values with appropriate methods\n",
        "        if 'Weekly_Sales' in df.columns:\n",
        "            # For training data\n",
        "            df['Weekly_Sales'].fillna(df['Weekly_Sales'].median(), inplace=True)\n",
        "\n",
        "        missing_after = df.isnull().sum().sum()\n",
        "\n",
        "        print(f\"Missing values before cleaning: {missing_before}\")\n",
        "        print(f\"Missing values after cleaning: {missing_after}\")\n",
        "\n",
        "        return df, missing_before, missing_after\n",
        "\n",
        "    # Clean training data\n",
        "    train_df, missing_before_train, missing_after_train = clean_data(train_df)\n",
        "\n",
        "    # Clean test data\n",
        "    test_df, missing_before_test, missing_after_test = clean_data(test_df)\n",
        "\n",
        "    # Log cleaning metrics\n",
        "    mlflow.log_metric(\"missing_before_train\", missing_before_train)\n",
        "    mlflow.log_metric(\"missing_after_train\", missing_after_train)\n",
        "    mlflow.log_metric(\"missing_before_test\", missing_before_test)\n",
        "    mlflow.log_metric(\"missing_after_test\", missing_after_test)\n",
        "\n",
        "    print(\"Data cleaning completed!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca7761ab",
      "metadata": {
        "id": "ca7761ab"
      },
      "source": [
        "# Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "35494272",
      "metadata": {
        "id": "35494272",
        "outputId": "1b02e281-2a6a-4210-fd73-eab3b685da3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting feature engineering...\n",
            "Available columns:\n",
            "  train_df: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Year', 'Month', 'Week', 'Day', 'DayOfWeek', 'Quarter', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment']\n",
            "  stores_df: ['Store']\n",
            "  features_df: ['Store', 'Date', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment']\n",
            "Overlapping columns with stores_df: []\n",
            "Dropping overlapping columns before features merge: ['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment']\n",
            "Warning: 'Type' column not found in dataframes after merging\n",
            "Feature engineering completed!\n",
            "Train shape: (421570, 20)\n",
            "Test shape: (115064, 19)\n",
            "Train columns: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Year', 'Month', 'Week', 'Day', 'DayOfWeek', 'Quarter', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment']\n",
            "Test columns: ['Store', 'Dept', 'Date', 'IsHoliday', 'Year', 'Month', 'Week', 'Day', 'DayOfWeek', 'Quarter', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mlflow\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "with mlflow.start_run(run_name=\"TFT_Feature_Engineering\"):\n",
        "    print(\"Starting feature engineering...\")\n",
        "\n",
        "    # --- Ensure Date columns are datetime ---\n",
        "    train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "    test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
        "    features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
        "    stores_df = stores_df.copy()  # in case you want to modify safely\n",
        "\n",
        "    features_df = features_df.drop(columns=['IsHoliday'], errors='ignore')\n",
        "\n",
        "    # --- Feature engineering function ---\n",
        "    def engineer_features(df):\n",
        "        df['Year'] = df['Date'].dt.year\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Week'] = df['Date'].dt.isocalendar().week\n",
        "        df['Day'] = df['Date'].dt.day\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        df['Quarter'] = df['Date'].dt.quarter\n",
        "        df['IsHoliday'] = df['IsHoliday'].astype(int)\n",
        "        df = df.sort_values(['Store', 'Date']).reset_index(drop=True)\n",
        "        return df\n",
        "\n",
        "    # --- Apply feature engineering ---\n",
        "    train_df = engineer_features(train_df)\n",
        "    test_df = engineer_features(test_df)\n",
        "\n",
        "    # --- Handle overlapping columns more carefully ---\n",
        "    # First, identify what columns exist in each dataframe\n",
        "    print(\"Available columns:\")\n",
        "    print(f\"  train_df: {list(train_df.columns)}\")\n",
        "    print(f\"  stores_df: {list(stores_df.columns)}\")\n",
        "    print(f\"  features_df: {list(features_df.columns)}\")\n",
        "\n",
        "    # Check what columns overlap between train_df and stores_df\n",
        "    stores_overlap = [col for col in stores_df.columns if col in train_df.columns and col != 'Store']\n",
        "    print(f\"Overlapping columns with stores_df: {stores_overlap}\")\n",
        "\n",
        "    # Only drop columns that actually exist in both and cause conflicts\n",
        "    # Keep Type and Size from stores_df by dropping them from train_df if they exist\n",
        "    if 'Type' in train_df.columns and 'Type' in stores_df.columns:\n",
        "        train_df = train_df.drop(columns=['Type'], errors='ignore')\n",
        "        test_df = test_df.drop(columns=['Type'], errors='ignore')\n",
        "    if 'Size' in train_df.columns and 'Size' in stores_df.columns:\n",
        "        train_df = train_df.drop(columns=['Size'], errors='ignore')\n",
        "        test_df = test_df.drop(columns=['Size'], errors='ignore')\n",
        "\n",
        "    # --- Merge with stores data ---\n",
        "    train_df = train_df.merge(stores_df, on='Store', how='left')\n",
        "    test_df = test_df.merge(stores_df, on='Store', how='left')\n",
        "\n",
        "    # Check for overlapping columns with features_df and drop them from train/test\n",
        "    features_overlap = [col for col in features_df.columns if col in train_df.columns and col not in ['Store', 'Date']]\n",
        "    if features_overlap:\n",
        "        print(f\"Dropping overlapping columns before features merge: {features_overlap}\")\n",
        "        train_df = train_df.drop(columns=features_overlap, errors='ignore')\n",
        "        test_df = test_df.drop(columns=features_overlap, errors='ignore')\n",
        "\n",
        "    # --- Merge with features data ---\n",
        "    train_df = train_df.merge(features_df, on=['Store', 'Date'], how='left')\n",
        "    test_df = test_df.merge(features_df, on=['Store', 'Date'], how='left')\n",
        "\n",
        "    # --- Encode categorical variables ---\n",
        "    # Check if Type column exists before encoding\n",
        "    if 'Type' in train_df.columns:\n",
        "        le_type = LabelEncoder()\n",
        "        train_df['Type_encoded'] = le_type.fit_transform(train_df['Type'])\n",
        "        test_df['Type_encoded'] = le_type.transform(test_df['Type'])\n",
        "    else:\n",
        "        print(\"Warning: 'Type' column not found in dataframes after merging\")\n",
        "\n",
        "    # --- Fill missing values in numerical columns (handle train and test separately) ---\n",
        "    # Get numeric columns for each dataset separately\n",
        "    train_numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
        "    test_numeric_cols = test_df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    # Fill missing values using medians from training data\n",
        "    train_df[train_numeric_cols] = train_df[train_numeric_cols].fillna(train_df[train_numeric_cols].median())\n",
        "\n",
        "    # For test data, use training data medians for common columns, test data medians for test-only columns\n",
        "    for col in test_numeric_cols:\n",
        "        if col in train_numeric_cols:\n",
        "            # Use training data median for consistency\n",
        "            test_df[col] = test_df[col].fillna(train_df[col].median())\n",
        "        else:\n",
        "            # Use test data median for columns not in training data\n",
        "            test_df[col] = test_df[col].fillna(test_df[col].median())\n",
        "\n",
        "    # --- Log to MLflow ---\n",
        "    mlflow.log_param(\"features_after_engineering\", len(train_df.columns))\n",
        "    mlflow.log_param(\"time_features_added\", 6)\n",
        "    mlflow.log_param(\"train_numeric_cols\", len(train_numeric_cols))\n",
        "    mlflow.log_param(\"test_numeric_cols\", len(test_numeric_cols))\n",
        "\n",
        "    print(f\"Feature engineering completed!\")\n",
        "    print(f\"Train shape: {train_df.shape}\")\n",
        "    print(f\"Test shape: {test_df.shape}\")\n",
        "    print(f\"Train columns: {list(train_df.columns)}\")\n",
        "    print(f\"Test columns: {list(test_df.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d594729e",
      "metadata": {
        "id": "d594729e",
        "outputId": "1f144a2d-5f37-402a-d9d2-4515fa6c124e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing data for TFT...\n",
            "Training data shape: (415661, 21)\n",
            "Validation data shape: (5909, 21)\n",
            "Data preparation completed!\n"
          ]
        }
      ],
      "source": [
        "# Prepare data for TFT\n",
        "with mlflow.start_run(run_name=\"TFT_Data_Preparation\"):\n",
        "    print(\"Preparing data for TFT...\")\n",
        "\n",
        "    # Create time index\n",
        "    train_df['time_idx'] = (train_df['Date'] - train_df['Date'].min()).dt.days\n",
        "    test_df['time_idx'] = (test_df['Date'] - train_df['Date'].min()).dt.days\n",
        "\n",
        "    # Define the features for TFT\n",
        "    static_categoricals = ['Store', 'Type_encoded']\n",
        "    static_reals = ['Size']\n",
        "    time_varying_known_categoricals = ['IsHoliday', 'Month', 'Quarter', 'DayOfWeek']\n",
        "    time_varying_known_reals = ['time_idx']\n",
        "    time_varying_unknown_reals = ['Weekly_Sales']\n",
        "\n",
        "    # Add external features if available\n",
        "    external_features = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
        "    available_external = [col for col in external_features if col in train_df.columns]\n",
        "    time_varying_known_reals.extend(available_external)\n",
        "\n",
        "    # Create target variable\n",
        "    target = 'Weekly_Sales'\n",
        "\n",
        "    # Split data for validation\n",
        "    max_prediction_length = 12  # 12 weeks ahead\n",
        "    max_encoder_length = 52     # Use 52 weeks of history\n",
        "\n",
        "    # Calculate cutoff for validation\n",
        "    cutoff = train_df['time_idx'].max() - max_prediction_length\n",
        "\n",
        "    # Create training and validation sets\n",
        "    training_data = train_df[train_df['time_idx'] <= cutoff]\n",
        "    validation_data = train_df[train_df['time_idx'] > cutoff]\n",
        "\n",
        "    print(f\"Training data shape: {training_data.shape}\")\n",
        "    print(f\"Validation data shape: {validation_data.shape}\")\n",
        "\n",
        "    # Log data preparation parameters\n",
        "    mlflow.log_param(\"max_prediction_length\", max_prediction_length)\n",
        "    mlflow.log_param(\"max_encoder_length\", max_encoder_length)\n",
        "    mlflow.log_param(\"training_samples\", len(training_data))\n",
        "    mlflow.log_param(\"validation_samples\", len(validation_data))\n",
        "\n",
        "    print(\"Data preparation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47bcc5e6",
      "metadata": {
        "id": "47bcc5e6"
      },
      "source": [
        "# Create TFT Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ac2d88cd",
      "metadata": {
        "id": "ac2d88cd",
        "outputId": "df91a7ed-1196-42ff-ab60-6e8d9dea7668",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating TFT dataset...\n",
            "Missing values in Weekly_Sales before handling: 0\n",
            "Missing values in Weekly_Sales after handling: 0\n",
            "Final check - NaN: 0, Inf: 0\n",
            "Training subset shape: (415661, 21)\n",
            "Missing values in Weekly_Sales in training subset: 0\n",
            "Final training subset check - NaN: 0, Inf: 0\n",
            "Checking all columns for missing values:\n",
            "Filling any remaining missing values in all columns...\n",
            "Final check of all columns after comprehensive cleaning:\n",
            "Total missing values across all columns: 0\n",
            "Weekly_Sales statistics:\n",
            "  Min: -4988.94\n",
            "  Max: 693099.36\n",
            "  Mean: 15990.320141293021\n",
            "  Unique values with potential issues: 1347\n",
            "Found non-positive values in Weekly_Sales, adjusting for GroupNormalizer...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'variable Type_encoded specified but not found in data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-17-54120140.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;31m# Create the dataset with a simpler normalizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     training_dataset = TimeSeriesDataSet(\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mtrain_subset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mtime_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'time_idx'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/data/timeseries/_timeseries.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, time_idx, target, group_ids, weight, max_encoder_length, min_encoder_length, min_prediction_idx, min_prediction_length, max_prediction_length, static_categoricals, static_reals, time_varying_known_categoricals, time_varying_known_reals, time_varying_unknown_categoricals, time_varying_unknown_reals, variable_groups, constant_fill_strategy, allow_missing_timesteps, lags, add_relative_time_idx, add_target_scales, add_encoder_length, target_normalizer, categorical_encoders, scalers, randomize_length, predict_mode)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;31m# validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0;31m# add lags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_forecasting/data/timeseries/_timeseries.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_categoricals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"variable {name} specified but not found in data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m             if not (\n\u001b[1;32m   1020\u001b[0m                 \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobject_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'variable Type_encoded specified but not found in data'"
          ]
        }
      ],
      "source": [
        "with mlflow.start_run(run_name=\"TFT_Dataset_Creation\"):\n",
        "    print(\"Creating TFT dataset...\")\n",
        "\n",
        "    # Convert Store to string type for categorical handling\n",
        "    train_df['Store'] = train_df['Store'].astype(str)\n",
        "\n",
        "    # Also convert any other categorical columns that might be numeric\n",
        "    for col in static_categoricals + time_varying_known_categoricals:\n",
        "        if col in train_df.columns:\n",
        "            train_df[col] = train_df[col].astype(str)\n",
        "\n",
        "    # Handle missing values in target variable\n",
        "    print(f\"Missing values in {target} before handling: {train_df[target].isna().sum()}\")\n",
        "\n",
        "    # Option 1: Fill missing target values with forward fill then backward fill\n",
        "    train_df[target] = train_df.groupby('Store')[target].fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "    # Option 2: If still missing, fill with store-specific median\n",
        "    train_df[target] = train_df.groupby('Store')[target].fillna(train_df.groupby('Store')[target].transform('median'))\n",
        "\n",
        "    # Option 3: If still missing, fill with overall median\n",
        "    train_df[target] = train_df[target].fillna(train_df[target].median())\n",
        "\n",
        "    print(f\"Missing values in {target} after handling: {train_df[target].isna().sum()}\")\n",
        "\n",
        "    # Check for infinite values and handle them\n",
        "    inf_mask = np.isinf(train_df[target])\n",
        "    if inf_mask.any():\n",
        "        print(f\"Found {inf_mask.sum()} infinite values in {target}, replacing with median\")\n",
        "        train_df.loc[inf_mask, target] = train_df[target].median()\n",
        "\n",
        "    # Final check for any remaining problematic values\n",
        "    print(f\"Final check - NaN: {train_df[target].isna().sum()}, Inf: {np.isinf(train_df[target]).sum()}\")\n",
        "\n",
        "    # Create the filtered dataset for training\n",
        "    train_subset = train_df[train_df['time_idx'] <= cutoff].copy()\n",
        "    print(f\"Training subset shape: {train_subset.shape}\")\n",
        "    print(f\"Missing values in {target} in training subset: {train_subset[target].isna().sum()}\")\n",
        "\n",
        "    # Handle missing values in the training subset\n",
        "    if train_subset[target].isna().sum() > 0:\n",
        "        print(\"Handling missing values in training subset...\")\n",
        "        # Fill missing values in the training subset\n",
        "        train_subset[target] = train_subset.groupby('Store')[target].fillna(method='ffill').fillna(method='bfill')\n",
        "        train_subset[target] = train_subset.groupby('Store')[target].fillna(train_subset.groupby('Store')[target].transform('median'))\n",
        "        train_subset[target] = train_subset[target].fillna(train_subset[target].median())\n",
        "\n",
        "        # Handle infinite values\n",
        "        inf_mask = np.isinf(train_subset[target])\n",
        "        if inf_mask.any():\n",
        "            print(f\"Found {inf_mask.sum()} infinite values in training subset, replacing with median\")\n",
        "            train_subset.loc[inf_mask, target] = train_subset[target].median()\n",
        "\n",
        "    print(f\"Final training subset check - NaN: {train_subset[target].isna().sum()}, Inf: {np.isinf(train_subset[target]).sum()}\")\n",
        "\n",
        "    # Additional debugging - check all columns for missing values\n",
        "    print(\"Checking all columns for missing values:\")\n",
        "    for col in train_subset.columns:\n",
        "        missing_count = train_subset[col].isna().sum()\n",
        "        if missing_count > 0:\n",
        "            print(f\"  {col}: {missing_count} missing values\")\n",
        "\n",
        "    # Check for any problematic values in all numeric columns\n",
        "    numeric_cols = train_subset.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        inf_count = np.isinf(train_subset[col]).sum()\n",
        "        if inf_count > 0:\n",
        "            print(f\"  {col}: {inf_count} infinite values\")\n",
        "            train_subset[col] = train_subset[col].replace([np.inf, -np.inf], train_subset[col].median())\n",
        "\n",
        "    # Fill any remaining missing values in all columns\n",
        "    print(\"Filling any remaining missing values in all columns...\")\n",
        "    for col in train_subset.columns:\n",
        "        if train_subset[col].isna().sum() > 0:\n",
        "            if train_subset[col].dtype == 'object':\n",
        "                # For categorical columns, fill with mode\n",
        "                train_subset[col] = train_subset[col].fillna(train_subset[col].mode()[0] if len(train_subset[col].mode()) > 0 else 'Unknown')\n",
        "            else:\n",
        "                # For numeric columns, fill with median\n",
        "                train_subset[col] = train_subset[col].fillna(train_subset[col].median())\n",
        "\n",
        "    print(\"Final check of all columns after comprehensive cleaning:\")\n",
        "    total_missing = train_subset.isna().sum().sum()\n",
        "    print(f\"Total missing values across all columns: {total_missing}\")\n",
        "\n",
        "    # Debug: Check the actual values in Weekly_Sales\n",
        "    print(f\"Weekly_Sales statistics:\")\n",
        "    print(f\"  Min: {train_subset[target].min()}\")\n",
        "    print(f\"  Max: {train_subset[target].max()}\")\n",
        "    print(f\"  Mean: {train_subset[target].mean()}\")\n",
        "    print(f\"  Unique values with potential issues: {train_subset[target][train_subset[target] <= 0].count()}\")\n",
        "\n",
        "    # Handle edge cases that might cause issues with GroupNormalizer\n",
        "    if (train_subset[target] <= 0).any():\n",
        "        print(\"Found non-positive values in Weekly_Sales, adjusting for GroupNormalizer...\")\n",
        "        # Add a small constant to ensure all values are positive for softplus transformation\n",
        "        min_val = train_subset[target].min()\n",
        "        if min_val <= 0:\n",
        "            train_subset[target] = train_subset[target] + abs(min_val) + 1\n",
        "\n",
        "    # Try with a simpler normalizer first\n",
        "    from pytorch_forecasting.data.encoders import EncoderNormalizer\n",
        "\n",
        "    # Create the dataset with a simpler normalizer\n",
        "    training_dataset = TimeSeriesDataSet(\n",
        "        train_subset,\n",
        "        time_idx='time_idx',\n",
        "        target=target,\n",
        "        group_ids=['Store'],\n",
        "        min_encoder_length=max_encoder_length // 2,\n",
        "        max_encoder_length=max_encoder_length,\n",
        "        min_prediction_length=1,\n",
        "        max_prediction_length=max_prediction_length,\n",
        "        static_categoricals=static_categoricals,\n",
        "        static_reals=static_reals,\n",
        "        time_varying_known_categoricals=time_varying_known_categoricals,\n",
        "        time_varying_known_reals=time_varying_known_reals,\n",
        "        time_varying_unknown_reals=time_varying_unknown_reals,\n",
        "        target_normalizer=EncoderNormalizer(),  # Use simpler normalizer\n",
        "        add_relative_time_idx=True,\n",
        "        add_target_scales=True,\n",
        "        add_encoder_length=True,\n",
        "        allow_missing_timesteps=True,\n",
        "    )\n",
        "\n",
        "    # Create validation dataset\n",
        "    validation_dataset = TimeSeriesDataSet.from_dataset(\n",
        "        training_dataset,\n",
        "        train_df,\n",
        "        predict=True,\n",
        "        stop_randomization=True\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    batch_size = 128\n",
        "    train_dataloader = training_dataset.to_dataloader(\n",
        "        train=True,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=0\n",
        "    )\n",
        "    val_dataloader = validation_dataset.to_dataloader(\n",
        "        train=False,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    print(f\"Training dataset size: {len(training_dataset)}\")\n",
        "    print(f\"Validation dataset size: {len(validation_dataset)}\")\n",
        "\n",
        "    # Log dataset parameters\n",
        "    mlflow.log_param(\"batch_size\", batch_size)\n",
        "    mlflow.log_param(\"train_dataset_size\", len(training_dataset))\n",
        "    mlflow.log_param(\"val_dataset_size\", len(validation_dataset))\n",
        "\n",
        "    print(\"Dataset creation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff339cbd",
      "metadata": {
        "id": "ff339cbd"
      },
      "source": [
        "# Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5606b68f",
      "metadata": {
        "id": "5606b68f"
      },
      "outputs": [],
      "source": [
        "\n",
        "with mlflow.start_run(run_name=\"TFT_Model_Training\"):\n",
        "    print(\"Starting TFT model training...\")\n",
        "\n",
        "    # Enable MLflow auto-logging for PyTorch Lightning\n",
        "    mlflow.pytorch.autolog()\n",
        "\n",
        "    # Create MLflow logger\n",
        "    mlflow_logger = MLFlowLogger(\n",
        "        experiment_name=experiment_name,\n",
        "        tracking_uri=mlflow.get_tracking_uri()\n",
        "    )\n",
        "\n",
        "    # Model configuration\n",
        "    model_config = {\n",
        "        \"hidden_size\": 64,\n",
        "        \"lstm_layers\": 2,\n",
        "        \"dropout\": 0.1,\n",
        "        \"attention_head_size\": 4,\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"reduce_on_plateau_patience\": 3,\n",
        "        \"optimizer\": \"Adam\"\n",
        "    }\n",
        "\n",
        "    # Create the model\n",
        "    tft = TemporalFusionTransformer.from_dataset(\n",
        "        training_dataset,\n",
        "        hidden_size=model_config[\"hidden_size\"],\n",
        "        lstm_layers=model_config[\"lstm_layers\"],\n",
        "        dropout=model_config[\"dropout\"],\n",
        "        attention_head_size=model_config[\"attention_head_size\"],\n",
        "        output_size=7,  # 7 quantiles by default\n",
        "        loss=SMAPE(),\n",
        "        learning_rate=model_config[\"learning_rate\"],\n",
        "        reduce_on_plateau_patience=model_config[\"reduce_on_plateau_patience\"],\n",
        "        optimizer=model_config[\"optimizer\"],\n",
        "    )\n",
        "\n",
        "    # Log model configuration\n",
        "    for key, value in model_config.items():\n",
        "        mlflow.log_param(key, value)\n",
        "\n",
        "    # Setup callbacks\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        verbose=True,\n",
        "        mode='min'\n",
        "    )\n",
        "\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        save_top_k=1,\n",
        "        filename='best_tft_model'\n",
        "    )\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=50,\n",
        "        accelerator='cpu',  # Change to 'gpu' if available\n",
        "        callbacks=[early_stopping, checkpoint_callback],\n",
        "        logger=mlflow_logger,\n",
        "        enable_progress_bar=True,\n",
        "        deterministic=True\n",
        "    )\n",
        "\n",
        "     # Train the model\n",
        "    trainer.fit(\n",
        "        tft,\n",
        "        train_dataloaders=train_dataloader,\n",
        "        val_dataloaders=val_dataloader\n",
        "    )\n",
        "\n",
        "    # Load best model\n",
        "    best_model = TemporalFusionTransformer.load_from_checkpoint(\n",
        "        checkpoint_callback.best_model_path\n",
        "    )\n",
        "\n",
        "    print(\"Model training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43b535b5",
      "metadata": {
        "id": "43b535b5"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fad2cba",
      "metadata": {
        "id": "1fad2cba"
      },
      "outputs": [],
      "source": [
        "with mlflow.start_run(run_name=\"TFT_Model_Evaluation\"):\n",
        "    print(\"Starting model evaluation...\")\n",
        "\n",
        "    # Make predictions on validation set\n",
        "    predictions = best_model.predict(val_dataloader, return_y=True)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae = MAE()(predictions.output, predictions.y).item()\n",
        "    smape = SMAPE()(predictions.output, predictions.y).item()\n",
        "    rmse = RMSE()(predictions.output, predictions.y).item()\n",
        "\n",
        "    # Log evaluation metrics\n",
        "    mlflow.log_metric(\"val_mae\", mae)\n",
        "    mlflow.log_metric(\"val_smape\", smape)\n",
        "    mlflow.log_metric(\"val_rmse\", rmse)\n",
        "\n",
        "    print(f\"Validation MAE: {mae:.4f}\")\n",
        "    print(f\"Validation SMAPE: {smape:.4f}\")\n",
        "    print(f\"Validation RMSE: {rmse:.4f}\")\n",
        "\n",
        "    # Create prediction plots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Plot 1: Actual vs Predicted\n",
        "    actual = predictions.y.cpu().numpy().flatten()\n",
        "    predicted = predictions.output.cpu().numpy().flatten()\n",
        "\n",
        "    axes[0, 0].scatter(actual, predicted, alpha=0.5)\n",
        "    axes[0, 0].plot([actual.min(), actual.max()], [actual.min(), actual.max()], 'r--', lw=2)\n",
        "    axes[0, 0].set_xlabel('Actual')\n",
        "    axes[0, 0].set_ylabel('Predicted')\n",
        "    axes[0, 0].set_title('Actual vs Predicted')\n",
        "\n",
        "    # Plot 2: Residuals\n",
        "    residuals = actual - predicted\n",
        "    axes[0, 1].scatter(predicted, residuals, alpha=0.5)\n",
        "    axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
        "    axes[0, 1].set_xlabel('Predicted')\n",
        "    axes[0, 1].set_ylabel('Residuals')\n",
        "    axes[0, 1].set_title('Residual Plot')\n",
        "\n",
        "    # Plot 3: Residuals histogram\n",
        "    axes[1, 0].hist(residuals, bins=50, alpha=0.7)\n",
        "    axes[1, 0].set_xlabel('Residuals')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    axes[1, 0].set_title('Residuals Distribution')\n",
        "\n",
        "    # Plot 4: Time series example\n",
        "    example_idx = 0\n",
        "    example_prediction = predictions.output[example_idx].cpu().numpy()\n",
        "    example_actual = predictions.y[example_idx].cpu().numpy()\n",
        "\n",
        "    axes[1, 1].plot(range(len(example_actual)), example_actual, 'b-', label='Actual', linewidth=2)\n",
        "    axes[1, 1].plot(range(len(example_prediction)), example_prediction, 'r--', label='Predicted', linewidth=2)\n",
        "    axes[1, 1].set_xlabel('Time Steps')\n",
        "    axes[1, 1].set_ylabel('Weekly Sales')\n",
        "    axes[1, 1].set_title('Example Prediction')\n",
        "    axes[1, 1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('tft_evaluation_plots.png', dpi=300, bbox_inches='tight')\n",
        "    mlflow.log_artifact('tft_evaluation_plots.png')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Model evaluation completed!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2b58316",
      "metadata": {
        "id": "d2b58316"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter Tuning (Optional)\n",
        "with mlflow.start_run(run_name=\"TFT_Hyperparameter_Tuning\"):\n",
        "    print(\"Starting hyperparameter tuning...\")\n",
        "\n",
        "    # Define hyperparameter ranges\n",
        "    study = optimize_hyperparameters(\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        model_path=\"optuna_test\",\n",
        "        n_trials=10,  # Reduce for faster execution\n",
        "        max_epochs=20,\n",
        "        gradient_clip_val_range=(0.01, 1.0),\n",
        "        hidden_size_range=(32, 128),\n",
        "        lstm_layers_range=(1, 4),\n",
        "        dropout_range=(0.1, 0.3),\n",
        "        attention_head_size_range=(1, 8),\n",
        "        learning_rate_range=(0.001, 0.1),\n",
        "        use_learning_rate_finder=False,\n",
        "    )\n",
        "\n",
        "    # Log best parameters\n",
        "    best_params = study.best_params\n",
        "    for key, value in best_params.items():\n",
        "        mlflow.log_param(f\"best_{key}\", value)\n",
        "\n",
        "    mlflow.log_metric(\"best_trial_value\", study.best_value)\n",
        "\n",
        "    print(f\"Best trial value: {study.best_value}\")\n",
        "    print(f\"Best parameters: {best_params}\")\n",
        "\n",
        "# Final Model Training with Best Parameters\n",
        "with mlflow.start_run(run_name=\"TFT_Final_Model_Training\"):\n",
        "    print(\"Training final model with best parameters...\")\n",
        "\n",
        "    # Create final model with best parameters (use default if tuning was skipped)\n",
        "    final_tft = TemporalFusionTransformer.from_dataset(\n",
        "        training_dataset,\n",
        "        hidden_size=64,  # Use best params if available\n",
        "        lstm_layers=2,\n",
        "        dropout=0.1,\n",
        "        attention_head_size=4,\n",
        "        output_size=7,\n",
        "        loss=SMAPE(),\n",
        "        learning_rate=0.001,\n",
        "        reduce_on_plateau_patience=3,\n",
        "        optimizer=\"Adam\",\n",
        "    )\n",
        "\n",
        "    # Create final trainer\n",
        "    final_trainer = pl.Trainer(\n",
        "        max_epochs=100,\n",
        "        accelerator='cpu',\n",
        "        callbacks=[early_stopping, checkpoint_callback],\n",
        "        logger=mlflow_logger,\n",
        "        enable_progress_bar=True,\n",
        "        deterministic=True\n",
        "    )\n",
        "\n",
        "    # Train final model\n",
        "    final_trainer.fit(\n",
        "        final_tft,\n",
        "        train_dataloaders=train_dataloader,\n",
        "        val_dataloaders=val_dataloader\n",
        "    )\n",
        "\n",
        "    # Load best final model\n",
        "    final_best_model = TemporalFusionTransformer.load_from_checkpoint(\n",
        "        checkpoint_callback.best_model_path\n",
        "    )\n",
        "\n",
        "    print(\"Final model training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e689e65",
      "metadata": {
        "id": "2e689e65"
      },
      "source": [
        "# Create Pipeline and Save Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c7a95e7",
      "metadata": {
        "id": "9c7a95e7"
      },
      "outputs": [],
      "source": [
        "with mlflow.start_run(run_name=\"TFT_Pipeline_Creation\"):\n",
        "    print(\"Creating TFT pipeline...\")\n",
        "\n",
        "    # Create a pipeline class for TFT\n",
        "    class TFTPipeline:\n",
        "        def __init__(self, model, dataset_config, preprocessing_params):\n",
        "            self.model = model\n",
        "            self.dataset_config = dataset_config\n",
        "            self.preprocessing_params = preprocessing_params\n",
        "            self.label_encoders = {}\n",
        "\n",
        "        def preprocess(self, data):\n",
        "            \"\"\"Preprocess raw data for TFT\"\"\"\n",
        "            # Apply the same preprocessing as training\n",
        "            data = data.copy()\n",
        "\n",
        "            # Convert Date to datetime\n",
        "            data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "            # Engineer features\n",
        "            data['Year'] = data['Date'].dt.year\n",
        "            data['Month'] = data['Date'].dt.month\n",
        "            data['Week'] = data['Date'].dt.week\n",
        "            data['Day'] = data['Date'].dt.day\n",
        "            data['DayOfWeek'] = data['Date'].dt.dayofweek\n",
        "            data['Quarter'] = data['Date'].dt.quarter\n",
        "            data['IsHoliday'] = data['IsHoliday'].astype(int)\n",
        "\n",
        "            # Create time index\n",
        "            data['time_idx'] = (data['Date'] - self.preprocessing_params['min_date']).dt.days\n",
        "\n",
        "            # Handle categorical encoding\n",
        "            if 'Type' in data.columns:\n",
        "                data['Type_encoded'] = le_type.transform(data['Type'])\n",
        "\n",
        "            # Fill missing values\n",
        "            numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "            data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].median())\n",
        "\n",
        "            return data\n",
        "\n",
        "        def predict(self, data):\n",
        "            \"\"\"Make predictions on new data\"\"\"\n",
        "            # Preprocess data\n",
        "            processed_data = self.preprocess(data)\n",
        "\n",
        "            # Create dataset for prediction\n",
        "            prediction_dataset = TimeSeriesDataSet.from_dataset(\n",
        "                self.dataset_config,\n",
        "                processed_data,\n",
        "                predict=True,\n",
        "                stop_randomization=True\n",
        "            )\n",
        "\n",
        "            # Create dataloader\n",
        "            prediction_dataloader = prediction_dataset.to_dataloader(\n",
        "                train=False,\n",
        "                batch_size=128,\n",
        "                num_workers=0\n",
        "            )\n",
        "\n",
        "            # Make predictions\n",
        "            predictions = self.model.predict(prediction_dataloader)\n",
        "\n",
        "            return predictions\n",
        "\n",
        "    # Create pipeline\n",
        "    preprocessing_params = {\n",
        "        'min_date': train_df['Date'].min(),\n",
        "        'max_date': train_df['Date'].max(),\n",
        "        'features': list(train_df.columns)\n",
        "    }\n",
        "\n",
        "    tft_pipeline = TFTPipeline(\n",
        "        model=final_best_model,\n",
        "        dataset_config=training_dataset,\n",
        "        preprocessing_params=preprocessing_params\n",
        "    )\n",
        "\n",
        "    # Save pipeline\n",
        "    pipeline_path = \"tft_pipeline.pkl\"\n",
        "    joblib.dump(tft_pipeline, pipeline_path)\n",
        "\n",
        "    # Log pipeline\n",
        "    mlflow.log_artifact(pipeline_path)\n",
        "\n",
        "    # Save additional components\n",
        "    joblib.dump(le_type, \"label_encoder_type.pkl\")\n",
        "    mlflow.log_artifact(\"label_encoder_type.pkl\")\n",
        "\n",
        "    print(\"Pipeline creation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b707dc3a",
      "metadata": {
        "id": "b707dc3a"
      },
      "source": [
        "# Model Registration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6ae809c",
      "metadata": {
        "id": "c6ae809c"
      },
      "outputs": [],
      "source": [
        "with mlflow.start_run(run_name=\"TFT_Model_Registration\"):\n",
        "    print(\"Registering model...\")\n",
        "\n",
        "    # Create model signature\n",
        "    sample_input = train_df.head(100)\n",
        "    sample_output = np.random.randn(100, max_prediction_length)\n",
        "    signature = infer_signature(sample_input, sample_output)\n",
        "\n",
        "    # Register model\n",
        "    model_name = \"TFT_Walmart_Sales_Forecast\"\n",
        "\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=tft_pipeline,\n",
        "        artifact_path=\"tft_model\",\n",
        "        signature=signature,\n",
        "        registered_model_name=model_name\n",
        "    )\n",
        "\n",
        "    print(f\"Model registered as '{model_name}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3c0a3dc",
      "metadata": {
        "id": "b3c0a3dc"
      },
      "outputs": [],
      "source": [
        "print(\"TFT experiment completed successfully!\")\n",
        "print(\"All artifacts and models have been logged to MLflow\")\n",
        "print(\"Check your MLflow UI to view the experiments and model registry\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}