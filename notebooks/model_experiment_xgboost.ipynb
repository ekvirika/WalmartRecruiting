{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 3816,
          "databundleVersionId": 32105,
          "sourceType": "competition"
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2685415ee9524bcb904dfc152d0bac10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39687dbee4ae4b1e85cdf150ffd78d84",
              "IPY_MODEL_db87779c504e40e48950d21c0c414b9d",
              "IPY_MODEL_c69364d3a184447e8dfddd2240b90e71"
            ],
            "layout": "IPY_MODEL_8acacaafb0604a54b4237dc41a4d807f"
          }
        },
        "39687dbee4ae4b1e85cdf150ffd78d84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c8dbb12f2a444908cde8e2f0513c2b6",
            "placeholder": "​",
            "style": "IPY_MODEL_0ae6fac694724d1e83dcd811b52e4bc1",
            "value": "Downloading artifacts: 100%"
          }
        },
        "db87779c504e40e48950d21c0c414b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e42bc0940e04566a38f37b249744bb9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a519062cc2d74dc89c3f45498b036c27",
            "value": 1
          }
        },
        "c69364d3a184447e8dfddd2240b90e71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1dbdf809df44f4c9705a7bece29753e",
            "placeholder": "​",
            "style": "IPY_MODEL_ed5d2f00030e4ce5bb2e8431cdcb7901",
            "value": " 1/1 [00:00&lt;00:00,  4.42it/s]"
          }
        },
        "8acacaafb0604a54b4237dc41a4d807f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c8dbb12f2a444908cde8e2f0513c2b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ae6fac694724d1e83dcd811b52e4bc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e42bc0940e04566a38f37b249744bb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a519062cc2d74dc89c3f45498b036c27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1dbdf809df44f4c9705a7bece29753e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed5d2f00030e4ce5bb2e8431cdcb7901": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekvirika/WalmartRecruiting/blob/main/notebooks/model_experiment_xgboost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "TTX7B7wlTO2t",
        "outputId": "2a395345-a900-4118-8168-705f021fb427",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn\n",
        "\n",
        "# Set up Kaggle API\n",
        "!pip install kaggle"
      ],
      "metadata": {
        "trusted": true,
        "id": "bGrqWCxbTO2t",
        "outputId": "a15d03a2-3ed3-4605-a31a-9ddae87681ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.7.14)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Upload your kaggle.json to Colab and run:\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "trusted": true,
        "id": "pOX3Df1jTO2t"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Download the dataset\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
      ],
      "metadata": {
        "trusted": true,
        "id": "SHblSkKnTO2u",
        "outputId": "f528a30a-90d8-40fd-cf9f-7cc2ca7cc869",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 604MB/s]\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q train.csv.zip\n",
        "!unzip -q stores.csv.zip\n",
        "!unzip -q test.csv.zip\n",
        "!unzip -q features.csv.zip"
      ],
      "metadata": {
        "id": "Gt8OnAW9UID4",
        "outputId": "3fd2a7a8-e340-486d-91d3-56764b5d7a25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open stores.csv.zip, stores.csv.zip.zip or stores.csv.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dagshub mlflow --quiet\n"
      ],
      "metadata": {
        "id": "xrt_tcN7UpsI",
        "outputId": "e97d586e-7aa3-43a8-f615-fdb14798957b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.2/261.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.0/677.0 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 1"
      ],
      "metadata": {
        "id": "aQ00zpw0jjtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler, OneHotEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, mean_absolute_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import xgboost as xgb\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import mlflow.xgboost\n",
        "from mlflow.models.signature import infer_signature\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "yXmt7oouAKGe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install optuna"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oWDfcRxjBI5v",
        "outputId": "8090e564-7bcc-40d7-f134-12e5f20c6513",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.4)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/395.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.9.0 optuna-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train = pd.read_csv('train.csv')\n",
        "# test = pd.read_csv('test.csv')\n",
        "# stores = pd.read_csv('stores.csv')\n",
        "# features = pd.read_csv('features.csv')"
      ],
      "metadata": {
        "id": "tSo5MUqKa29M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# fig, axs = plt.subplots(4, 1, figsize=(15, 12), sharex=True)\n",
        "\n",
        "# # Original Weekly Sales\n",
        "# axs[0].plot(train['Weekly_Sales'], label='Weekly_Sales', color='blue')\n",
        "# axs[0].set_title('Original Weekly Sales')\n",
        "# axs[0].legend()\n",
        "\n",
        "# # Rolling Average with window=4\n",
        "# axs[1].plot(train['Weekly_Sales_rolling_4'], label='Rolling Mean (4 weeks)', color='orange')\n",
        "# axs[1].set_title('Rolling Average - Window 4')\n",
        "# axs[1].legend()\n",
        "\n",
        "# # Rolling Average with window=13\n",
        "# axs[2].plot(train['Weekly_Sales_rolling_13'], label='Rolling Mean (13 weeks)', color='green')\n",
        "# axs[2].set_title('Rolling Average - Window 13')\n",
        "# axs[2].legend()\n",
        "\n",
        "# # Rolling Average with window=52\n",
        "# axs[3].plot(train['Weekly_Sales_rolling_52'], label='Rolling Mean (52 weeks)', color='red')\n",
        "# axs[3].set_title('Rolling Average - Window 52')\n",
        "# axs[3].legend()\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "EFTHg0BYwICJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from dagshub import dagshub_logger\n",
        "import os\n",
        "\n",
        "# Set tracking URI manually\n",
        "mlflow.set_tracking_uri(\"https://dagshub.com/ekvirika/WalmartRecruiting.mlflow\")\n",
        "\n",
        "# Use your DagsHub credentials\n",
        "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"ekvirika\"\n",
        "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"0adb1004ddd4221395353efea2d8ead625e26197\"\n",
        "\n",
        "# Optional: set registry if you're using model registry\n",
        "mlflow.set_registry_uri(\"https://dagshub.com/ekvirika/WalmartRecruiting.mlflow\")"
      ],
      "metadata": {
        "id": "WHIvxHEiywEA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import mlflow\n",
        "# import mlflow.sklearn\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from xgboost import XGBRegressor\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "# from sklearn.model_selection import RandomizedSearchCV\n",
        "# import matplotlib.pyplot as plt\n",
        "# import joblib\n",
        "\n",
        "\n",
        "\n",
        "# # ----------------------\n",
        "# # Evaluation Metric (WMAE)\n",
        "# # ----------------------\n",
        "# def weighted_mae(y_true, y_pred, is_holiday):\n",
        "#     weights = np.where(is_holiday, 5, 1)\n",
        "#     return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "# # ----------------------\n",
        "# # Data Loader\n",
        "# # ----------------------\n",
        "# def load_data():\n",
        "#     with mlflow.start_run(run_name=\"XGBoost_Data_Loading\", nested=True):\n",
        "#         train_df = pd.read_csv('train.csv')\n",
        "#         test_df = pd.read_csv('test.csv')\n",
        "#         stores_df = pd.read_csv('stores.csv')\n",
        "#         features_df = pd.read_csv('features.csv')\n",
        "\n",
        "#         df = train_df.merge(features_df, on=['Store', 'Date'], how='inner') \\\n",
        "#                      .merge(stores_df, on='Store', how='inner')\n",
        "\n",
        "#         mlflow.log_params({\n",
        "#             \"train_shape\": train_df.shape,\n",
        "#             \"test_shape\": test_df.shape,\n",
        "#             \"missing_values_train\": train_df.isnull().sum().sum(),\n",
        "#             \"missing_values_test\": test_df.isnull().sum().sum()\n",
        "#         })\n",
        "\n",
        "#         return df\n",
        "\n",
        "# # ----------------------\n",
        "# # Preprocessing\n",
        "# # ----------------------\n",
        "# def preprocess(df):\n",
        "#     with mlflow.start_run(run_name=\"XGBoost_Cleaning\", nested=True):\n",
        "\n",
        "\n",
        "#       df = df.copy()\n",
        "\n",
        "#       # Fix column naming issues\n",
        "#       df[\"IsHoliday\"] = df.pop(\"IsHoliday_x\") if \"IsHoliday_x\" in df else df[\"IsHoliday\"]\n",
        "#       df.drop(columns=[\"IsHoliday_y\"], errors='ignore', inplace=True)\n",
        "\n",
        "#       # Filter and sort\n",
        "#       df = df[df[\"Weekly_Sales\"] > 0]\n",
        "#       df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "#       df = df.sort_values(by=\"Date\")\n",
        "\n",
        "#       # Remove outliers\n",
        "#       q_low = df[\"Weekly_Sales\"].quantile(0.01)\n",
        "#       q_high = df[\"Weekly_Sales\"].quantile(0.99)\n",
        "#       df = df[(df[\"Weekly_Sales\"] >= q_low) & (df[\"Weekly_Sales\"] <= q_high)]\n",
        "\n",
        "#       # Lag features\n",
        "#       for lag in [1, 2, 4, 52]:\n",
        "#           df[f\"lag_{lag}\"] = df.groupby(\"Store\")[\"Weekly_Sales\"].shift(lag)\n",
        "\n",
        "#       # Rolling statistics\n",
        "#       df[\"rolling_mean_4\"] = df.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(window=4).mean()\n",
        "#       df[\"rolling_std_4\"] = df.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(window=4).std()\n",
        "\n",
        "#       # Fill missing MarkDowns\n",
        "#       markdown_cols = [col for col in df.columns if \"MarkDown\" in col]\n",
        "#       df[markdown_cols] = df[markdown_cols].fillna(0)\n",
        "\n",
        "#       # Date features\n",
        "#       df[\"Year\"] = df[\"Date\"].dt.year\n",
        "#       df[\"Month\"] = df[\"Date\"].dt.month\n",
        "#       df[\"Week\"] = df[\"Date\"].dt.isocalendar().week\n",
        "#       df[\"DayOfWeek\"] = df[\"Date\"].dt.dayofweek\n",
        "#       df[\"IsMonthStart\"] = df[\"Date\"].dt.is_month_start.astype(int)\n",
        "#       df[\"IsMonthEnd\"] = df[\"Date\"].dt.is_month_end.astype(int)\n",
        "#       df[\"Quarter\"] = df[\"Date\"].dt.quarter\n",
        "#       df.drop(columns=[\"Date\"], inplace=True)\n",
        "\n",
        "#       mlflow.log_params({\"droppped_cols\": \"IsHoliday_x, IsHoliday_y, Date\", \"fill_markdown_NaNs\": \"0\"})\n",
        "\n",
        "#       # Drop rows with missing lag/rolling values\n",
        "#       df = df.dropna()\n",
        "\n",
        "#       return df\n",
        "\n",
        "# # ----------------------\n",
        "# # Dynamic Pipeline Builder\n",
        "# # ----------------------\n",
        "# def build_pipeline(X, model=None):\n",
        "#     with mlflow.start_run(run_name=\"XGBoost_Feature_Engineering\", nested=True):\n",
        "\n",
        "#       numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "#       categorical_cols = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "\n",
        "#       preprocessor = ColumnTransformer(transformers=[\n",
        "#           (\"num\", SimpleImputer(strategy='mean'), numeric_cols),\n",
        "#           (\"cat\", OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)\n",
        "#       ])\n",
        "\n",
        "#       if model is None:\n",
        "#           model = XGBRegressor(\n",
        "#               n_estimators=100,\n",
        "#               max_depth=6,\n",
        "#               learning_rate=0.1,\n",
        "#               random_state=42,\n",
        "#               n_jobs=-1\n",
        "#           )\n",
        "\n",
        "#       pipeline = Pipeline([\n",
        "#           (\"preprocessor\", preprocessor),\n",
        "#           (\"model\", model)\n",
        "#       ])\n",
        "\n",
        "#       mlflow.log_params({\"num_cols\": numeric_cols, \"cat_cols\": categorical_cols,\n",
        "#                         \"simple_imputer\": \"mean\", \"cat_encoder\": \"one_hot\", \"pipeline\": pipeline})\n",
        "\n",
        "#       return pipeline\n",
        "\n",
        "# # ----------------------\n",
        "# # Runner\n",
        "# # ----------------------\n",
        "# def run():\n",
        "#     # with mlflow.start_run(run_name=\"XGBoost_Training\"):\n",
        "\n",
        "#       df = load_data()\n",
        "#       df = preprocess(df)\n",
        "\n",
        "#       # Define features and target\n",
        "#       X = df.drop(columns=[\"Weekly_Sales\"])\n",
        "#       y = df[\"Weekly_Sales\"]\n",
        "\n",
        "#       # Split\n",
        "#       X_train, X_test, y_train, y_test = train_test_split(\n",
        "#           X, y, test_size=0.2, random_state=42\n",
        "#       )\n",
        "\n",
        "#       # Store IsHoliday for WMAE\n",
        "#       is_holiday_test = X_test[\"IsHoliday\"].astype(bool).values if \"IsHoliday\" in X_test.columns else np.zeros_like(y_test)\n",
        "\n",
        "#       # Train\n",
        "#       pipeline = build_pipeline(X_train)\n",
        "\n",
        "#       # Hyperparameter search space\n",
        "#       param_dist = {\n",
        "#           \"model__n_estimators\": [100, 200, 300],\n",
        "#           \"model__max_depth\": [3, 6, 9],\n",
        "#           \"model__learning_rate\": [0.05, 0.1, 0.2],\n",
        "#           \"model__subsample\": [0.5, 0.7, 1.0],\n",
        "#           \"model__colsample_bytree\": [0.5, 0.7, 1.0]\n",
        "#       }\n",
        "\n",
        "#       search = RandomizedSearchCV(\n",
        "#           pipeline,\n",
        "#           param_distributions=param_dist,\n",
        "#           n_iter=20,\n",
        "#           cv=3,\n",
        "#           scoring=\"neg_mean_absolute_error\",  # you could also define custom scorer using WMAE if needed\n",
        "#           verbose=2,\n",
        "#           n_jobs=-1,\n",
        "#           random_state=42\n",
        "#       )\n",
        "\n",
        "#       search.fit(X_train, y_train)\n",
        "\n",
        "#       best_model = search.best_estimator_\n",
        "#       y_pred = best_model.predict(X_test)\n",
        "\n",
        "#       # WMAE\n",
        "#       wmae_score = weighted_mae(y_test.values, y_pred, is_holiday_test)\n",
        "#       print(f\"✅ WMAE: {wmae_score:.2f}\")\n",
        "\n",
        "#       mlflow.log_params(search.best_params_)\n",
        "#       mlflow.log_metric(\"WMAE\", wmae_score)\n",
        "#       # mlflow.xgboost.log_model(best_model, artifact_path=\"xgb_model\")\n",
        "#       joblib.dump(best_model.named_steps[\"model\"], \"xgb_model.pkl\")\n",
        "#       mlflow.log_artifact(\"xgb_model.pkl\")\n",
        "\n",
        "#       joblib.dump(best_model, \"xgb_pipeline.pkl\")\n",
        "#       mlflow.log_artifact(\"xgb_pipeline.pkl\")\n",
        "\n",
        "#       # Save feature importance plot as artifact\n",
        "#       fig, ax = plt.subplots()\n",
        "#       xgb_model = best_model.named_steps[\"model\"]\n",
        "#       importances = xgb_model.feature_importances_\n",
        "#       print(importances)\n",
        "#       ax.bar(range(len(importances)), importances)\n",
        "#       plt.savefig(\"feature_importance.png\")\n",
        "#       mlflow.log_artifact(\"feature_importance.png\")\n",
        "\n",
        "\n",
        "#       return best_model\n",
        "\n",
        "# # ----------------------\n",
        "# # Entry Point\n",
        "# # ----------------------\n",
        "# if __name__ == \"__main__\":\n",
        "#     try:\n",
        "#         with mlflow.start_run(run_name=\"XGBoost_Training\"):\n",
        "#             model = run()\n",
        "#         print(\"✅ Pipeline executed successfully!\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"❌ Pipeline failed: {e}\")\n"
      ],
      "metadata": {
        "id": "mxCeUqv0os8B",
        "collapsed": true
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# =====================================================\n",
        "# BASE TRANSFORMER CLASS\n",
        "# =====================================================\n",
        "class BaseWalmartTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Base class for all Walmart transformers\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit the transformer\"\"\"\n",
        "        self.is_fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform the data\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Transformer not fitted yet. Call fit() first.\")\n",
        "        return X\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Fit and transform in one step\"\"\"\n",
        "        return self.fit(X, y).transform(X)\n",
        "\n",
        "# =====================================================\n",
        "# DATA LOADER CLASS\n",
        "# =====================================================\n",
        "class WalmartDataLoader:\n",
        "    \"\"\"Handles loading and merging of Walmart datasets\"\"\"\n",
        "\n",
        "    def __init__(self, log_mlflow=True):\n",
        "        self.log_mlflow = log_mlflow\n",
        "        self.data_info = {}\n",
        "\n",
        "    def load_data(self, train_path='train.csv', test_path='test.csv',\n",
        "                  stores_path='stores.csv', features_path='features.csv'):\n",
        "        \"\"\"Load and merge all datasets\"\"\"\n",
        "\n",
        "        if self.log_mlflow:\n",
        "            mlflow.start_run(run_name=\"Data_Loading\", nested=True)\n",
        "\n",
        "        try:\n",
        "            # Load datasets\n",
        "            train_df = pd.read_csv(train_path)\n",
        "            test_df = pd.read_csv(test_path)\n",
        "            stores_df = pd.read_csv(stores_path)\n",
        "            features_df = pd.read_csv(features_path)\n",
        "\n",
        "            # Merge training data\n",
        "            merged_train = train_df.merge(features_df, on=['Store', 'Date'], how='inner') \\\n",
        "                                  .merge(stores_df, on='Store', how='inner')\n",
        "\n",
        "            # Store data info\n",
        "            self.data_info = {\n",
        "                \"train_shape\": train_df.shape,\n",
        "                \"test_shape\": test_df.shape,\n",
        "                \"merged_train_shape\": merged_train.shape,\n",
        "                \"missing_values_train\": train_df.isnull().sum().sum(),\n",
        "                \"missing_values_test\": test_df.isnull().sum().sum()\n",
        "            }\n",
        "\n",
        "            if self.log_mlflow:\n",
        "                mlflow.log_params(self.data_info)\n",
        "                mlflow.end_run()\n",
        "\n",
        "            print(f\"Data loaded successfully!\")\n",
        "            print(f\"Training data shape: {merged_train.shape}\")\n",
        "\n",
        "            return merged_train, test_df, stores_df, features_df\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.log_mlflow:\n",
        "                mlflow.end_run()\n",
        "            raise e\n",
        "\n",
        "# =====================================================\n",
        "# DATA CLEANING CLASS\n",
        "# =====================================================\n",
        "class WalmartDataCleaner(BaseWalmartTransformer):\n",
        "    \"\"\"Handles data cleaning and basic preprocessing\"\"\"\n",
        "\n",
        "    def __init__(self, remove_outliers=True, outlier_quantiles=(0.01, 0.99), log_mlflow=True):\n",
        "        super().__init__()\n",
        "        self.remove_outliers = remove_outliers\n",
        "        self.outlier_quantiles = outlier_quantiles\n",
        "        self.log_mlflow = log_mlflow\n",
        "        self.q_low = None\n",
        "        self.q_high = None\n",
        "        self.cleaning_stats = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit the cleaner - compute outlier bounds\"\"\"\n",
        "        if self.log_mlflow:\n",
        "            mlflow.start_run(run_name=\"Data_Cleaning_Fit\", nested=True)\n",
        "\n",
        "        try:\n",
        "            X = X.copy()\n",
        "\n",
        "            # Compute outlier bounds if Weekly_Sales exists\n",
        "            if 'Weekly_Sales' in X.columns and self.remove_outliers:\n",
        "                self.q_low = X[\"Weekly_Sales\"].quantile(self.outlier_quantiles[0])\n",
        "                self.q_high = X[\"Weekly_Sales\"].quantile(self.outlier_quantiles[1])\n",
        "\n",
        "                self.cleaning_stats = {\n",
        "                    \"outlier_lower_bound\": self.q_low,\n",
        "                    \"outlier_upper_bound\": self.q_high,\n",
        "                    \"original_rows\": len(X)\n",
        "                }\n",
        "\n",
        "            if self.log_mlflow:\n",
        "                if self.cleaning_stats:\n",
        "                    mlflow.log_params(self.cleaning_stats)\n",
        "                mlflow.end_run()\n",
        "\n",
        "            return super().fit(X, y)\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.log_mlflow:\n",
        "                mlflow.end_run()\n",
        "            raise e\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform the data - clean and preprocess\"\"\"\n",
        "        super().transform(X)  # Check if fitted\n",
        "\n",
        "        if self.log_mlflow:\n",
        "            mlflow.start_run(run_name=\"Data_Cleaning_Transform\", nested=True)\n",
        "\n",
        "        try:\n",
        "            X = X.copy()\n",
        "            original_rows = len(X)\n",
        "\n",
        "            # Fix IsHoliday column naming issues\n",
        "            if \"IsHoliday_x\" in X.columns:\n",
        "                X[\"IsHoliday\"] = X.pop(\"IsHoliday_x\")\n",
        "            if \"IsHoliday_y\" in X.columns:\n",
        "                X.drop(columns=[\"IsHoliday_y\"], inplace=True)\n",
        "\n",
        "            # Filter positive sales (only if Weekly_Sales exists)\n",
        "            if 'Weekly_Sales' in X.columns:\n",
        "                X = X[X[\"Weekly_Sales\"] > 0]\n",
        "\n",
        "                # Remove outliers if fitted bounds exist\n",
        "                if self.remove_outliers and self.q_low is not None and self.q_high is not None:\n",
        "                    X = X[(X[\"Weekly_Sales\"] >= self.q_low) & (X[\"Weekly_Sales\"] <= self.q_high)]\n",
        "\n",
        "            # Convert Date to datetime and sort\n",
        "            if 'Date' in X.columns:\n",
        "                X[\"Date\"] = pd.to_datetime(X[\"Date\"])\n",
        "                X = X.sort_values(by=\"Date\")\n",
        "\n",
        "            # Fill missing MarkDown values\n",
        "            markdown_cols = [col for col in X.columns if \"MarkDown\" in col]\n",
        "            if markdown_cols:\n",
        "                X[markdown_cols] = X[markdown_cols].fillna(0)\n",
        "\n",
        "            rows_after_cleaning = len(X)\n",
        "\n",
        "            if self.log_mlflow:\n",
        "                mlflow.log_params({\n",
        "                    \"rows_before_cleaning\": original_rows,\n",
        "                    \"rows_after_cleaning\": rows_after_cleaning,\n",
        "                    \"rows_removed\": original_rows - rows_after_cleaning,\n",
        "                    \"markdown_cols_filled\": len(markdown_cols)\n",
        "                })\n",
        "                mlflow.end_run()\n",
        "\n",
        "            print(f\"Data cleaning completed: {original_rows} -> {rows_after_cleaning} rows\")\n",
        "\n",
        "            return X\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.log_mlflow:\n",
        "                mlflow.end_run()\n",
        "            raise e\n",
        "\n",
        "# =====================================================\n",
        "# FEATURE ENGINEERING CLASS\n",
        "# =====================================================\n",
        "class WalmartFeatureEngineer(BaseWalmartTransformer):\n",
        "    \"\"\"Handles feature engineering - lag features, rolling stats, date features\"\"\"\n",
        "\n",
        "    def __init__(self, lag_periods=[1, 2, 4, 52], rolling_windows=[4], log_mlflow=True):\n",
        "        super().__init__()\n",
        "        self.lag_periods = lag_periods\n",
        "        self.rolling_windows = rolling_windows\n",
        "        self.log_mlflow = log_mlflow\n",
        "        self.feature_stats = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit feature engineer - just store parameters\"\"\"\n",
        "        if self.log_mlflow:\n",
        "            mlflow.start_run(run_name=\"Feature_Engineering_Fit\", nested=True)\n",
        "\n",
        "        try:\n",
        "            self.feature_stats = {\n",
        "                \"lag_periods\": self.lag_periods,\n",
        "                \"rolling_windows\": self.rolling_windows,\n",
        "                \"has_weekly_sales\": 'Weekly_Sales' in X.columns,\n",
        "                \"has_date\": 'Date' in X.columns,\n",
        "                \"has_store\": 'Store' in X.columns\n",
        "            }\n",
        "\n",
        "            if self.log_mlflow:\n",
        "                mlflow.log_params(self.feature_stats)\n",
        "                mlflow.end_run()\n",
        "\n",
        "            return super().fit(X, y)\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.log_mlflow:\n",
        "                mlflow.end_run()\n",
        "            raise e\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform data - create features\"\"\"\n",
        "        super().transform(X)\n",
        "\n",
        "        if self.log_mlflow:\n",
        "            mlflow.start_run(run_name=\"Feature_Engineering_Transform\", nested=True)\n",
        "\n",
        "        try:\n",
        "            X = X.copy()\n",
        "            original_features = len(X.columns)\n",
        "\n",
        "            # Create lag features\n",
        "            if 'Weekly_Sales' in X.columns and 'Store' in X.columns:\n",
        "                for lag in self.lag_periods:\n",
        "                    X[f\"lag_{lag}\"] = X.groupby(\"Store\")[\"Weekly_Sales\"].shift(lag)\n",
        "\n",
        "                # Create rolling statistics\n",
        "                for window in self.rolling_windows:\n",
        "                    X[f\"rolling_mean_{window}\"] = X.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(window=window).mean()\n",
        "                    X[f\"rolling_std_{window}\"] = X.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(window=window).std()\n",
        "\n",
        "            # Create date features\n",
        "            if 'Date' in X.columns:\n",
        "                X[\"Year\"] = X[\"Date\"].dt.year\n",
        "                X[\"Month\"] = X[\"Date\"].dt.month\n",
        "                X[\"Week\"] = X[\"Date\"].dt.isocalendar().week\n",
        "                X[\"DayOfWeek\"] = X[\"Date\"].dt.dayofweek\n",
        "                X[\"IsMonthStart\"] = X[\"Date\"].dt.is_month_start.astype(int)\n",
        "                X[\"IsMonthEnd\"] = X[\"Date\"].dt.is_month_end.astype(int)\n",
        "                X[\"Quarter\"] = X[\"Date\"].dt.quarter\n",
        "\n",
        "                # Drop Date column after feature extraction\n",
        "                X.drop(columns=[\"Date\"], inplace=True)\n",
        "\n",
        "            # Drop rows with NaN values (from lag/rolling features)\n",
        "            rows_before_dropna = len(X)\n",
        "            X = X.dropna()\n",
        "            rows_after_dropna = len(X)\n",
        "\n",
        "            new_features = len(X.columns)\n",
        "\n",
        "            if self.log_mlflow:\n",
        "                mlflow.log_params({\n",
        "                    \"original_features\": original_features,\n",
        "                    \"new_features\": new_features,\n",
        "                    \"features_added\": new_features - original_features,\n",
        "                    \"rows_before_dropna\": rows_before_dropna,\n",
        "                    \"rows_after_dropna\": rows_after_dropna,\n",
        "                    \"rows_dropped_due_to_nan\": rows_before_dropna - rows_after_dropna\n",
        "                })\n",
        "                mlflow.end_run()\n",
        "\n",
        "            print(f\"Feature engineering completed: {original_features} -> {new_features} features\")\n",
        "            print(f\"Rows after removing NaNs: {rows_before_dropna} -> {rows_after_dropna}\")\n",
        "\n",
        "            return X\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.log_mlflow:\n",
        "                mlflow.end_run()\n",
        "            raise e\n",
        "\n",
        "# =====================================================\n",
        "# MODEL PIPELINE CLASS\n",
        "# =====================================================\n",
        "class WalmartModelPipeline(BaseWalmartTransformer):\n",
        "    \"\"\"Handles model preprocessing and training\"\"\"\n",
        "\n",
        "    def __init__(self, model=None, log_mlflow=True):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.log_mlflow = log_mlflow\n",
        "        self.preprocessor = None\n",
        "        self.pipeline = None\n",
        "        self.numeric_cols = None\n",
        "        self.categorical_cols = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fit the preprocessing pipeline\"\"\"\n",
        "        if self.log_mlflow:\n",
        "            mlflow.start_run(run_name=\"Model_Pipeline_Fit\", nested=True)\n",
        "\n",
        "        try:\n",
        "            # Identify column types\n",
        "            self.numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "            self.categorical_cols = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "\n",
        "            # Create preprocessor\n",
        "            transformers = []\n",
        "            if self.numeric_cols:\n",
        "                transformers.append((\"num\", SimpleImputer(strategy='mean'), self.numeric_cols))\n",
        "            if self.categorical_cols:\n",
        "                transformers.append((\"cat\", OneHotEncoder(drop='first', handle_unknown='ignore'), self.categorical_cols))\n",
        "\n",
        "            self.preprocessor = ColumnTransformer(transformers=transformers)\n",
        "\n",
        "            # Create default model if none provided\n",
        "            if self.model is None:\n",
        "                self.model = XGBRegressor(\n",
        "                    n_estimators=100,\n",
        "                    max_depth=6,\n",
        "                    learning_rate=0.1,\n",
        "                    random_state=42,\n",
        "                    n_jobs=-1\n",
        "                )\n",
        "\n",
        "            # Create pipeline\n",
        "            self.pipeline = Pipeline([\n",
        "                (\"preprocessor\", self.preprocessor),\n",
        "                (\"model\", self.model)\n",
        "            ])\n",
        "\n",
        "            if self.log_mlflow:\n",
        "                mlflow.log_params({\n",
        "                    \"numeric_cols_count\": len(self.numeric_cols),\n",
        "                    \"categorical_cols_count\": len(self.categorical_cols),\n",
        "                    \"numeric_cols\": self.numeric_cols[:10],  # Log first 10 to avoid too long\n",
        "                    \"categorical_cols\": self.categorical_cols,\n",
        "                    \"model_type\": type(self.model).__name__\n",
        "                })\n",
        "                mlflow.end_run()\n",
        "\n",
        "            return super().fit(X, y)\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.log_mlflow:\n",
        "                mlflow.end_run()\n",
        "            raise e\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform using the fitted pipeline preprocessor\"\"\"\n",
        "        super().transform(X)\n",
        "\n",
        "        if self.pipeline is None:\n",
        "            raise ValueError(\"Pipeline not fitted yet.\")\n",
        "\n",
        "        # Only transform using preprocessor, not the full pipeline\n",
        "        return self.pipeline.named_steps['preprocessor'].transform(X)\n",
        "\n",
        "    def get_pipeline(self):\n",
        "        \"\"\"Get the full pipeline for training\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Pipeline not fitted yet.\")\n",
        "        return self.pipeline\n",
        "\n",
        "# =====================================================\n",
        "# EVALUATION METRICS CLASS\n",
        "# =====================================================\n",
        "class WalmartEvaluator:\n",
        "    \"\"\"Handles model evaluation with WMAE and other metrics\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def weighted_mae(self, y_true, y_pred, is_holiday):\n",
        "        \"\"\"Calculate Weighted Mean Absolute Error\"\"\"\n",
        "        weights = np.where(is_holiday, 5, 1)\n",
        "        return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "    def evaluate_model(self, model, X_test, y_test, is_holiday_test=None):\n",
        "        \"\"\"Evaluate model with multiple metrics\"\"\"\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Standard MAE\n",
        "        mae_score = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "        # WMAE if holiday info available\n",
        "        wmae_score = None\n",
        "        if is_holiday_test is not None:\n",
        "            wmae_score = self.weighted_mae(y_test, y_pred, is_holiday_test)\n",
        "\n",
        "        metrics = {\n",
        "            'mae': mae_score,\n",
        "            'wmae': wmae_score\n",
        "        }\n",
        "\n",
        "        return metrics, y_pred\n",
        "\n",
        "# =====================================================\n",
        "# MAIN PIPELINE CLASS\n",
        "# =====================================================\n",
        "class WalmartSalesPipeline:\n",
        "    \"\"\"Main pipeline orchestrating all components\"\"\"\n",
        "\n",
        "    def __init__(self, log_mlflow=True, experiment_name=\"Walmart_Sales_OOP\"):\n",
        "        self.log_mlflow = log_mlflow\n",
        "        self.experiment_name = experiment_name\n",
        "\n",
        "        # Initialize components\n",
        "        self.data_loader = WalmartDataLoader(log_mlflow=log_mlflow)\n",
        "        self.data_cleaner = WalmartDataCleaner(log_mlflow=log_mlflow)\n",
        "        self.feature_engineer = WalmartFeatureEngineer(log_mlflow=log_mlflow)\n",
        "        self.model_pipeline = WalmartModelPipeline(log_mlflow=log_mlflow)\n",
        "        self.evaluator = WalmartEvaluator()\n",
        "\n",
        "        # Pipeline state\n",
        "        self.is_fitted = False\n",
        "        self.best_model = None\n",
        "        self.best_params = None\n",
        "        self.training_data = None\n",
        "\n",
        "        if self.log_mlflow:\n",
        "            mlflow.set_experiment(self.experiment_name)\n",
        "\n",
        "    def fit(self, train_path='train.csv', test_path='test.csv',\n",
        "            stores_path='stores.csv', features_path='features.csv',\n",
        "            hyperparameter_search=True, cv_folds=3, n_iter=20):\n",
        "        \"\"\"Fit the entire pipeline\"\"\"\n",
        "\n",
        "        if self.log_mlflow:\n",
        "            mlflow.start_run(run_name=\"Walmart_Pipeline_Training\")\n",
        "\n",
        "        try:\n",
        "            print(\"=\"*60)\n",
        "            print(\"WALMART SALES PIPELINE - TRAINING\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            # 1. Load data\n",
        "            print(\"\\n1. Loading data...\")\n",
        "            train_data, test_data, stores_data, features_data = self.data_loader.load_data(\n",
        "                train_path, test_path, stores_path, features_path\n",
        "            )\n",
        "\n",
        "            # 2. Clean data\n",
        "            print(\"\\n2. Cleaning data...\")\n",
        "            self.data_cleaner.fit(train_data)\n",
        "            train_cleaned = self.data_cleaner.transform(train_data)\n",
        "\n",
        "            # 3. Engineer features\n",
        "            print(\"\\n3. Engineering features...\")\n",
        "            self.feature_engineer.fit(train_cleaned)\n",
        "            train_processed = self.feature_engineer.transform(train_cleaned)\n",
        "\n",
        "            # Store processed data\n",
        "            self.training_data = train_processed.copy()\n",
        "\n",
        "            # 4. Prepare features and target\n",
        "            print(\"\\n4. Preparing features and target...\")\n",
        "            X = train_processed.drop(columns=[\"Weekly_Sales\"])\n",
        "            y = train_processed[\"Weekly_Sales\"]\n",
        "\n",
        "            # 5. Split data\n",
        "            print(\"\\n5. Splitting data...\")\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y, test_size=0.2, random_state=42\n",
        "            )\n",
        "\n",
        "            # Extract IsHoliday for WMAE\n",
        "            is_holiday_test = X_test[\"IsHoliday\"].astype(bool).values if \"IsHoliday\" in X_test.columns else np.zeros_like(y_test, dtype=bool)\n",
        "\n",
        "            # 6. Fit model pipeline\n",
        "            print(\"\\n6. Fitting model pipeline...\")\n",
        "            self.model_pipeline.fit(X_train, y_train)\n",
        "            pipeline = self.model_pipeline.get_pipeline()\n",
        "\n",
        "            # 7. Hyperparameter tuning\n",
        "            if hyperparameter_search:\n",
        "                print(\"\\n7. Hyperparameter tuning...\")\n",
        "                param_dist = {\n",
        "                    \"model__n_estimators\": [100, 200, 300],\n",
        "                    \"model__max_depth\": [3, 6, 9],\n",
        "                    \"model__learning_rate\": [0.05, 0.1, 0.2],\n",
        "                    \"model__subsample\": [0.5, 0.7, 1.0],\n",
        "                    \"model__colsample_bytree\": [0.5, 0.7, 1.0]\n",
        "                }\n",
        "\n",
        "                search = RandomizedSearchCV(\n",
        "                    pipeline,\n",
        "                    param_distributions=param_dist,\n",
        "                    n_iter=n_iter,\n",
        "                    cv=cv_folds,\n",
        "                    scoring=\"neg_mean_absolute_error\",\n",
        "                    verbose=1,\n",
        "                    n_jobs=-1,\n",
        "                    random_state=42\n",
        "                )\n",
        "\n",
        "                search.fit(X_train, y_train)\n",
        "                self.best_model = search.best_estimator_\n",
        "                self.best_params = search.best_params_\n",
        "\n",
        "                if self.log_mlflow:\n",
        "                    mlflow.log_params(self.best_params)\n",
        "\n",
        "            else:\n",
        "                print(\"\\n7. Training model without hyperparameter search...\")\n",
        "                pipeline.fit(X_train, y_train)\n",
        "                self.best_model = pipeline\n",
        "\n",
        "            # 8. Evaluate model\n",
        "            print(\"\\n8. Evaluating model...\")\n",
        "            metrics, y_pred = self.evaluator.evaluate_model(\n",
        "                self.best_model, X_test, y_test, is_holiday_test\n",
        "            )\n",
        "\n",
        "            print(f\"✅ MAE: {metrics['mae']:.2f}\")\n",
        "            if metrics['wmae'] is not None:\n",
        "                print(f\"✅ WMAE: {metrics['wmae']:.2f}\")\n",
        "\n",
        "            if self.log_mlflow:\n",
        "                mlflow.log_metrics(metrics)\n",
        "\n",
        "            # 9. Save artifacts\n",
        "            print(\"\\n9. Saving model artifacts...\")\n",
        "            self._save_artifacts()\n",
        "\n",
        "            self.is_fitted = True\n",
        "\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"✅ PIPELINE TRAINING COMPLETED!\")\n",
        "            print(\"=\"*60)\n",
        "\n",
        "            if self.log_mlflow:\n",
        "                mlflow.end_run()\n",
        "\n",
        "            return self.best_model, metrics\n",
        "\n",
        "        except Exception as e:\n",
        "            if self.log_mlflow:\n",
        "                mlflow.end_run()\n",
        "            raise e\n",
        "\n",
        "    def predict(self, new_data):\n",
        "        \"\"\"Make predictions on new data\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Pipeline not fitted yet. Call fit() first.\")\n",
        "\n",
        "        # Apply same preprocessing steps\n",
        "        cleaned_data = self.data_cleaner.transform(new_data)\n",
        "        processed_data = self.feature_engineer.transform(cleaned_data)\n",
        "\n",
        "        # Remove target column if it exists\n",
        "        if 'Weekly_Sales' in processed_data.columns:\n",
        "            processed_data = processed_data.drop(columns=['Weekly_Sales'])\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = self.best_model.predict(processed_data)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def _save_artifacts(self):\n",
        "        \"\"\"Save model artifacts\"\"\"\n",
        "        try:\n",
        "            # Save pipeline\n",
        "            joblib.dump(self.best_model, \"walmart_pipeline.pkl\")\n",
        "            if self.log_mlflow:\n",
        "                mlflow.log_artifact(\"walmart_pipeline.pkl\")\n",
        "\n",
        "            # Save individual components\n",
        "            joblib.dump(self.data_cleaner, \"data_cleaner.pkl\")\n",
        "            joblib.dump(self.feature_engineer, \"feature_engineer.pkl\")\n",
        "            joblib.dump(self.model_pipeline, \"model_pipeline.pkl\")\n",
        "\n",
        "            if self.log_mlflow:\n",
        "                mlflow.log_artifact(\"data_cleaner.pkl\")\n",
        "                mlflow.log_artifact(\"feature_engineer.pkl\")\n",
        "                mlflow.log_artifact(\"model_pipeline.pkl\")\n",
        "\n",
        "            # Feature importance plot\n",
        "            if hasattr(self.best_model.named_steps[\"model\"], 'feature_importances_'):\n",
        "                fig, ax = plt.subplots(figsize=(10, 6))\n",
        "                importances = self.best_model.named_steps[\"model\"].feature_importances_\n",
        "                indices = np.argsort(importances)[::-1][:20]  # Top 20 features\n",
        "\n",
        "                ax.bar(range(len(indices)), importances[indices])\n",
        "                ax.set_title(\"Top 20 Feature Importances\")\n",
        "                ax.set_xlabel(\"Feature Index\")\n",
        "                ax.set_ylabel(\"Importance\")\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(\"feature_importance.png\")\n",
        "\n",
        "                if self.log_mlflow:\n",
        "                    mlflow.log_artifact(\"feature_importance.png\")\n",
        "\n",
        "                plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not save some artifacts: {e}\")\n",
        "\n",
        "# =====================================================\n",
        "# USAGE EXAMPLE\n",
        "# =====================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize and run pipeline\n",
        "    pipeline = WalmartSalesPipeline(\n",
        "        log_mlflow=True,\n",
        "        experiment_name=\"Walmart_Sales_OOP_Pipeline\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Train the pipeline\n",
        "        best_model, metrics = pipeline.fit(\n",
        "            hyperparameter_search=True,\n",
        "            cv_folds=3,\n",
        "            n_iter=10  # Reduced for faster testing\n",
        "        )\n",
        "\n",
        "        print(f\"\\n🎉 Training completed successfully!\")\n",
        "        print(f\"Best parameters: {pipeline.best_params}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Pipeline failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "ZeHzBTnO0wQl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc4f9e7-5d84-4651-e593-56dc1bbe057d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/31 11:54:50 INFO mlflow.tracking.fluent: Experiment with name 'Walmart_Sales_OOP_Pipeline' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "WALMART SALES PIPELINE - TRAINING\n",
            "============================================================\n",
            "\n",
            "1. Loading data...\n",
            "🏃 View run Data_Loading at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/1/runs/7511b1d63cc94b24853fd6bd5a505361\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/1\n",
            "Data loaded successfully!\n",
            "Training data shape: (421570, 17)\n",
            "\n",
            "2. Cleaning data...\n",
            "🏃 View run Data_Cleaning_Fit at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/1/runs/1996f0510a8340b1a7fb4ede7c8e232b\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/1\n",
            "🏃 View run Data_Cleaning_Transform at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/1/runs/121a63d25d704556a76a869fb86ed17b\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/1\n",
            "Data cleaning completed: 421570 -> 413212 rows\n",
            "\n",
            "3. Engineering features...\n",
            "🏃 View run Feature_Engineering_Fit at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/1/runs/043376d865eb41ae9145a9ce8613c610\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/1\n",
            "🏃 View run Feature_Engineering_Transform at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/1/runs/d9bdf3f1f1c7466e85e7dbbb952cc7a5\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/1\n",
            "Feature engineering completed: 16 -> 28 features\n",
            "Rows after removing NaNs: 413212 -> 410870\n",
            "\n",
            "4. Preparing features and target...\n",
            "\n",
            "5. Splitting data...\n",
            "\n",
            "6. Fitting model pipeline...\n",
            "🏃 View run Model_Pipeline_Fit at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/1/runs/4d30f6051d0e4f4e9e2e1ebc85e957e0\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/1\n",
            "\n",
            "7. Hyperparameter tuning...\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "\n",
            "8. Evaluating model...\n",
            "✅ MAE: 1793.99\n",
            "✅ WMAE: 1856.56\n",
            "\n",
            "9. Saving model artifacts...\n",
            "\n",
            "============================================================\n",
            "✅ PIPELINE TRAINING COMPLETED!\n",
            "============================================================\n",
            "🏃 View run Walmart_Pipeline_Training at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/1/runs/8008f1cad2b74f29bc2d836bb5699af7\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/1\n",
            "\n",
            "🎉 Training completed successfully!\n",
            "Best parameters: {'model__subsample': 1.0, 'model__n_estimators': 300, 'model__max_depth': 9, 'model__learning_rate': 0.1, 'model__colsample_bytree': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_uri = \"runs:/8008f1cad2b74f29bc2d836bb5699af7/model_pipeline.pkl\"\n"
      ],
      "metadata": {
        "id": "5xDToj4cLeqI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlflow --quiet"
      ],
      "metadata": {
        "id": "jjAXbuF41dBh",
        "outputId": "50cb7336-1aa7-498a-e4bb-7c8d32b61b0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m680.6/680.6 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import joblib\n",
        "from mlflow.models import ModelSignature, infer_signature\n",
        "\n",
        "# Load the model.pkl manually\n",
        "model = joblib.load(\"mlruns/8008f1cad2b74f29bc2d836bb5699af7/artifacts/model_pipeline.pkl\")  # or from a URL\n",
        "\n",
        "# Optional: infer signature (if you have data)\n",
        "# signature = infer_signature(X_test, model.predict(X_test))\n",
        "\n",
        "# Start a new run and log model properly for registration\n",
        "with mlflow.start_run() as run:\n",
        "    mlflow.sklearn.log_model(model, artifact_path=\"model\")  # or xgboost.log_model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "w5nW4V1Za3xc",
        "outputId": "c6de49a0-c8d3-4655-8c6b-8873e7d55922"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'mlflow-artifacts:/e0cbe91d54c64e1d839ca8a3c7668be3/8008f1cad2b74f29bc2d836bb5699af7/artifacts/model_pipeline.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2699105472.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the model.pkl manually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mlflow-artifacts:/e0cbe91d54c64e1d839ca8a3c7668be3/8008f1cad2b74f29bc2d836bb5699af7/artifacts/model_pipeline.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# or from a URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Optional: infer signature (if you have data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode, ensure_native_byte_order)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_native_byte_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_native_byte_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             with _validate_fileobject_and_memmap(f, filename, mmap_mode) as (\n\u001b[1;32m    737\u001b[0m                 \u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mlflow-artifacts:/e0cbe91d54c64e1d839ca8a3c7668be3/8008f1cad2b74f29bc2d836bb5699af7/artifacts/model_pipeline.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import joblib\n",
        "\n",
        "# This assumes your model.pkl is inside the 'model' artifact folder\n",
        "model_uri = \"mlflow-artifacts:/e0cbe91d54c64e1d839ca8a3c7668be3/8008f1cad2b74f29bc2d836bb5699af7/artifacts/model_pipeline.pkl\"\n",
        "\n",
        "# Download the file locally (MLflow handles remote storage)\n",
        "local_path = mlflow.artifacts.download_artifacts(model_uri)\n",
        "model = joblib.load(local_path)\n",
        "\n",
        "# Log again for registration\n",
        "with mlflow.start_run() as run:\n",
        "    mlflow.sklearn.log_model(model, artifact_path=\"xgb_model\")  # or xgboost.log_model if it's XGBoost\n",
        "    print(\"Re-logged for registration. Run ID:\", run.info.run_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "zolen_S_bE6Q",
        "outputId": "be12158c-b579-4073-f60c-e0837abc8c03"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MlflowException",
          "evalue": "When an mlflow-artifacts URI was supplied, the tracking URI must be a valid http or https URI, but it was currently set to file:///content/mlruns. Perhaps you forgot to set the tracking URI to the running MLflow server. To set the tracking URI, use either of the following methods:\n1. Set the MLFLOW_TRACKING_URI environment variable to the desired tracking URI. `export MLFLOW_TRACKING_URI=http://localhost:5000`\n2. Set the tracking URI programmatically by calling `mlflow.set_tracking_uri`. `mlflow.set_tracking_uri('http://localhost:5000')`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-366835455.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Download the file locally (MLflow handles remote storage)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlocal_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifacts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/artifacts/__init__.py\u001b[0m in \u001b[0;36mdownload_artifacts\u001b[0;34m(artifact_uri, run_id, artifact_path, dst_path, tracking_uri)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0martifact_uri\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         return _download_artifact_from_uri(\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0martifact_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdst_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracking_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtracking_uri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracking/artifact_utils.py\u001b[0m in \u001b[0;36m_download_artifact_from_uri\u001b[0;34m(artifact_uri, output_path, lineage_header_info, tracking_uri)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \"\"\"\n\u001b[1;32m    114\u001b[0m     \u001b[0mroot_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_root_uri_and_artifact_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mrepo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_artifact_repository\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracking_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtracking_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/store/artifact/artifact_repository_registry.py\u001b[0m in \u001b[0;36mget_artifact_repository\u001b[0;34m(artifact_uri, tracking_uri)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mrequirements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \"\"\"\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_artifact_repository_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_artifact_repository\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracking_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/store/artifact/artifact_repository_registry.py\u001b[0m in \u001b[0;36mget_artifact_repository\u001b[0;34m(self, artifact_uri, tracking_uri)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;34mf\"Currently registered schemes are: {list(self._registry.keys())}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             )\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrepository\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracking_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_registered_artifact_repositories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/store/artifact/mlflow_artifacts_repo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, artifact_uri, tracking_uri)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact_uri\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracking_uri\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0meffective_tracking_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracking_uri\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mget_tracking_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meffective_tracking_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracking_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/store/artifact/mlflow_artifacts_repo.py\u001b[0m in \u001b[0;36mresolve_uri\u001b[0;34m(cls, artifact_uri, tracking_uri)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Check that tracking uri is http or https\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0m_validate_uri_scheme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_parse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muri_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# root directory; build simple path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/store/artifact/mlflow_artifacts_repo.py\u001b[0m in \u001b[0;36m_validate_uri_scheme\u001b[0;34m(parsed_uri)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mallowable_schemes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"http\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"https\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowable_schemes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         raise MlflowException(\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;34m\"When an mlflow-artifacts URI was supplied, the tracking URI must be a valid \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;34mf\"http or https URI, but it was currently set to {parsed_uri.geturl()}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMlflowException\u001b[0m: When an mlflow-artifacts URI was supplied, the tracking URI must be a valid http or https URI, but it was currently set to file:///content/mlruns. Perhaps you forgot to set the tracking URI to the running MLflow server. To set the tracking URI, use either of the following methods:\n1. Set the MLFLOW_TRACKING_URI environment variable to the desired tracking URI. `export MLFLOW_TRACKING_URI=http://localhost:5000`\n2. Set the tracking URI programmatically by calling `mlflow.set_tracking_uri`. `mlflow.set_tracking_uri('http://localhost:5000')`"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import joblib\n",
        "# from my_module import WalmartModelPipeline  # <- Make sure this line works\n",
        "\n",
        "\n",
        "mlflow.set_tracking_uri(\"https://dagshub.com/ekvirika/WalmartRecruiting.mlflow\")\n",
        "\n",
        "# Set your run ID and artifact path\n",
        "run_id = \"8008f1cad2b74f29bc2d836bb5699af7\"\n",
        "artifact_path = \"model_pipeline.pkl\"\n",
        "\n",
        "# Download the file to a temporary location\n",
        "local_path = mlflow.artifacts.download_artifacts(\n",
        "    run_id=run_id,\n",
        "    artifact_path=artifact_path\n",
        ")\n",
        "\n",
        "# Load the model from the downloaded path\n",
        "model = joblib.load(local_path)\n",
        "\n",
        "# # Log again as an MLflow model\n",
        "with mlflow.start_run() as run:\n",
        "    mlflow.sklearn.log_model(model, artifact_path=\"xgb_model\")\n",
        "    print(\"Model logged at:\", f\"runs:/{run.info.run_id}/xgb_model\")\n"
      ],
      "metadata": {
        "id": "bNL71HjZ1LRs",
        "outputId": "6e1657c3-a27d-4ed9-eb95-0f7ac0ed3288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388,
          "referenced_widgets": [
            "2685415ee9524bcb904dfc152d0bac10",
            "39687dbee4ae4b1e85cdf150ffd78d84",
            "db87779c504e40e48950d21c0c414b9d",
            "c69364d3a184447e8dfddd2240b90e71",
            "8acacaafb0604a54b4237dc41a4d807f",
            "0c8dbb12f2a444908cde8e2f0513c2b6",
            "0ae6fac694724d1e83dcd811b52e4bc1",
            "5e42bc0940e04566a38f37b249744bb9",
            "a519062cc2d74dc89c3f45498b036c27",
            "d1dbdf809df44f4c9705a7bece29753e",
            "ed5d2f00030e4ce5bb2e8431cdcb7901"
          ]
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2685415ee9524bcb904dfc152d0bac10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "Can't get attribute 'WalmartModelPipeline' on <module '__main__'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2354198443.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Load the model from the downloaded path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# # Log again as an MLflow model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode, ensure_native_byte_order)\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0;31m# it has been written with. Other arrays are coerced to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 \u001b[0;31m# native endianness of the host system.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m                 obj = _unpickle(\n\u001b[0m\u001b[1;32m    750\u001b[0m                     \u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m                     \u001b[0mensure_native_byte_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_native_byte_order\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36m_unpickle\u001b[0;34m(fobj, ensure_native_byte_order, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             warnings.warn(\n",
            "\u001b[0;32m/usr/lib/python3.11/pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pickle.py\u001b[0m in \u001b[0;36mload_stack_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1536\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"STACK_GLOBAL requires str\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1538\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1539\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSTACK_GLOBAL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_stack_global\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pickle.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_getattribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pickle.py\u001b[0m in \u001b[0;36m_getattribute\u001b[0;34m(obj, name)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             raise AttributeError(\"Can't get attribute {!r} on {!r}\"\n\u001b[0m\u001b[1;32m    332\u001b[0m                                  .format(name, obj)) from None\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'WalmartModelPipeline' on <module '__main__'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import mlflow\n",
        "\n",
        "# Optional: use cloudpickle if custom classes are involved\n",
        "model = joblib.load(\"mlflow-artifacts:/e0cbe91d54c64e1d839ca8a3c7668be3/8008f1cad2b74f29bc2d836bb5699af7/artifacts/model_pipeline.pkl\")\n"
      ],
      "metadata": {
        "id": "NzawOR-C47mB",
        "outputId": "4685181a-8c12-4b06-aed1-be506b74e953",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'mlflow-artifacts:/e0cbe91d54c64e1d839ca8a3c7668be3/8008f1cad2b74f29bc2d836bb5699af7/artifacts/model_pipeline.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3002770665.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Optional: use cloudpickle if custom classes are involved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mlflow-artifacts:/e0cbe91d54c64e1d839ca8a3c7668be3/8008f1cad2b74f29bc2d836bb5699af7/artifacts/model_pipeline.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode, ensure_native_byte_order)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_native_byte_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_native_byte_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             with _validate_fileobject_and_memmap(f, filename, mmap_mode) as (\n\u001b[1;32m    737\u001b[0m                 \u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mlflow-artifacts:/e0cbe91d54c64e1d839ca8a3c7668be3/8008f1cad2b74f29bc2d836bb5699af7/artifacts/model_pipeline.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BrMjoK6M-o8u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}