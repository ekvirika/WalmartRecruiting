{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 3816,
          "databundleVersionId": 32105,
          "sourceType": "competition"
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekvirika/WalmartRecruiting/blob/main/notebooks/model_experiment_xgboost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "TTX7B7wlTO2t",
        "outputId": "26822634-c524-4de9-8b7e-61bb13787a62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install wandb torch torchvision pandas numpy matplotlib seaborn scikit-learn\n",
        "\n",
        "# Set up Kaggle API\n",
        "!pip install kaggle"
      ],
      "metadata": {
        "trusted": true,
        "id": "bGrqWCxbTO2t",
        "outputId": "3e47fa01-bba6-423a-e91e-51ef11b9cfa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.7.14)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Upload your kaggle.json to Colab and run:\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "trusted": true,
        "id": "pOX3Df1jTO2t"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Download the dataset\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "!unzip -q walmart-recruiting-store-sales-forecasting.zip"
      ],
      "metadata": {
        "trusted": true,
        "id": "SHblSkKnTO2u",
        "outputId": "fa394793-0712-4d01-c9e2-8ed89dc7056d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 492MB/s]\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q train.csv.zip\n",
        "!unzip -q stores.csv.zip\n",
        "!unzip -q test.csv.zip\n",
        "!unzip -q features.csv.zip"
      ],
      "metadata": {
        "id": "Gt8OnAW9UID4",
        "outputId": "b1deadcc-6fe0-442d-a9a5-cfb29d5005c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open stores.csv.zip, stores.csv.zip.zip or stores.csv.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dagshub mlflow --quiet"
      ],
      "metadata": {
        "id": "xrt_tcN7UpsI",
        "outputId": "7ce9a833-bd26-45fe-a5c5-4fd8bf9017c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.2/261.2 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m680.6/680.6 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 1"
      ],
      "metadata": {
        "id": "aQ00zpw0jjtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler, OneHotEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, mean_absolute_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "import xgboost as xgb\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import mlflow.xgboost\n",
        "from mlflow.models.signature import infer_signature\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "yXmt7oouAKGe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install optuna"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oWDfcRxjBI5v",
        "outputId": "f592d37b-f595-498e-e913-9b63b8ab5c55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.4)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.9.0 optuna-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "stores = pd.read_csv('stores.csv')\n",
        "features = pd.read_csv('features.csv')"
      ],
      "metadata": {
        "id": "tSo5MUqKa29M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# fig, axs = plt.subplots(4, 1, figsize=(15, 12), sharex=True)\n",
        "\n",
        "# # Original Weekly Sales\n",
        "# axs[0].plot(train['Weekly_Sales'], label='Weekly_Sales', color='blue')\n",
        "# axs[0].set_title('Original Weekly Sales')\n",
        "# axs[0].legend()\n",
        "\n",
        "# # Rolling Average with window=4\n",
        "# axs[1].plot(train['Weekly_Sales_rolling_4'], label='Rolling Mean (4 weeks)', color='orange')\n",
        "# axs[1].set_title('Rolling Average - Window 4')\n",
        "# axs[1].legend()\n",
        "\n",
        "# # Rolling Average with window=13\n",
        "# axs[2].plot(train['Weekly_Sales_rolling_13'], label='Rolling Mean (13 weeks)', color='green')\n",
        "# axs[2].set_title('Rolling Average - Window 13')\n",
        "# axs[2].legend()\n",
        "\n",
        "# # Rolling Average with window=52\n",
        "# axs[3].plot(train['Weekly_Sales_rolling_52'], label='Rolling Mean (52 weeks)', color='red')\n",
        "# axs[3].set_title('Rolling Average - Window 52')\n",
        "# axs[3].legend()\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "EFTHg0BYwICJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "from dagshub import dagshub_logger\n",
        "import os\n",
        "\n",
        "# Set tracking URI manually\n",
        "mlflow.set_tracking_uri(\"https://dagshub.com/ekvirika/WalmartRecruiting.mlflow\")\n",
        "\n",
        "# Use your DagsHub credentials\n",
        "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"ekvirika\"\n",
        "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"0adb1004ddd4221395353efea2d8ead625e26197\"\n",
        "\n",
        "# Optional: set registry if you're using model registry\n",
        "mlflow.set_registry_uri(\"https://dagshub.com/ekvirika/WalmartRecruiting.mlflow\")"
      ],
      "metadata": {
        "id": "WHIvxHEiywEA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import mlflow\n",
        "# import mlflow.sklearn\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from xgboost import XGBRegressor\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "# from sklearn.model_selection import RandomizedSearchCV\n",
        "# import matplotlib.pyplot as plt\n",
        "# import joblib\n",
        "\n",
        "\n",
        "\n",
        "# # ----------------------\n",
        "# # Evaluation Metric (WMAE)\n",
        "# # ----------------------\n",
        "# def weighted_mae(y_true, y_pred, is_holiday):\n",
        "#     weights = np.where(is_holiday, 5, 1)\n",
        "#     return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "# # ----------------------\n",
        "# # Data Loader\n",
        "# # ----------------------\n",
        "# def load_data():\n",
        "#     with mlflow.start_run(run_name=\"XGBoost_Data_Loading\", nested=True):\n",
        "#         train_df = pd.read_csv('train.csv')\n",
        "#         test_df = pd.read_csv('test.csv')\n",
        "#         stores_df = pd.read_csv('stores.csv')\n",
        "#         features_df = pd.read_csv('features.csv')\n",
        "\n",
        "#         df = train_df.merge(features_df, on=['Store', 'Date'], how='inner') \\\n",
        "#                      .merge(stores_df, on='Store', how='inner')\n",
        "\n",
        "#         mlflow.log_params({\n",
        "#             \"train_shape\": train_df.shape,\n",
        "#             \"test_shape\": test_df.shape,\n",
        "#             \"missing_values_train\": train_df.isnull().sum().sum(),\n",
        "#             \"missing_values_test\": test_df.isnull().sum().sum()\n",
        "#         })\n",
        "\n",
        "#         return df\n",
        "\n",
        "# # ----------------------\n",
        "# # Preprocessing\n",
        "# # ----------------------\n",
        "# def preprocess(df):\n",
        "#     with mlflow.start_run(run_name=\"XGBoost_Cleaning\", nested=True):\n",
        "\n",
        "\n",
        "#       df = df.copy()\n",
        "\n",
        "#       # Fix column naming issues\n",
        "#       df[\"IsHoliday\"] = df.pop(\"IsHoliday_x\") if \"IsHoliday_x\" in df else df[\"IsHoliday\"]\n",
        "#       df.drop(columns=[\"IsHoliday_y\"], errors='ignore', inplace=True)\n",
        "\n",
        "#       # Filter and sort\n",
        "#       df = df[df[\"Weekly_Sales\"] > 0]\n",
        "#       df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
        "#       df = df.sort_values(by=\"Date\")\n",
        "\n",
        "#       # Remove outliers\n",
        "#       q_low = df[\"Weekly_Sales\"].quantile(0.01)\n",
        "#       q_high = df[\"Weekly_Sales\"].quantile(0.99)\n",
        "#       df = df[(df[\"Weekly_Sales\"] >= q_low) & (df[\"Weekly_Sales\"] <= q_high)]\n",
        "\n",
        "#       # Lag features\n",
        "#       for lag in [1, 2, 4, 52]:\n",
        "#           df[f\"lag_{lag}\"] = df.groupby(\"Store\")[\"Weekly_Sales\"].shift(lag)\n",
        "\n",
        "#       # Rolling statistics\n",
        "#       df[\"rolling_mean_4\"] = df.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(window=4).mean()\n",
        "#       df[\"rolling_std_4\"] = df.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(window=4).std()\n",
        "\n",
        "#       # Fill missing MarkDowns\n",
        "#       markdown_cols = [col for col in df.columns if \"MarkDown\" in col]\n",
        "#       df[markdown_cols] = df[markdown_cols].fillna(0)\n",
        "\n",
        "#       # Date features\n",
        "#       df[\"Year\"] = df[\"Date\"].dt.year\n",
        "#       df[\"Month\"] = df[\"Date\"].dt.month\n",
        "#       df[\"Week\"] = df[\"Date\"].dt.isocalendar().week\n",
        "#       df[\"DayOfWeek\"] = df[\"Date\"].dt.dayofweek\n",
        "#       df[\"IsMonthStart\"] = df[\"Date\"].dt.is_month_start.astype(int)\n",
        "#       df[\"IsMonthEnd\"] = df[\"Date\"].dt.is_month_end.astype(int)\n",
        "#       df[\"Quarter\"] = df[\"Date\"].dt.quarter\n",
        "#       df.drop(columns=[\"Date\"], inplace=True)\n",
        "\n",
        "#       mlflow.log_params({\"droppped_cols\": \"IsHoliday_x, IsHoliday_y, Date\", \"fill_markdown_NaNs\": \"0\"})\n",
        "\n",
        "#       # Drop rows with missing lag/rolling values\n",
        "#       df = df.dropna()\n",
        "\n",
        "#       return df\n",
        "\n",
        "# # ----------------------\n",
        "# # Dynamic Pipeline Builder\n",
        "# # ----------------------\n",
        "# def build_pipeline(X, model=None):\n",
        "#     with mlflow.start_run(run_name=\"XGBoost_Feature_Engineering\", nested=True):\n",
        "\n",
        "#       numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "#       categorical_cols = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "\n",
        "#       preprocessor = ColumnTransformer(transformers=[\n",
        "#           (\"num\", SimpleImputer(strategy='mean'), numeric_cols),\n",
        "#           (\"cat\", OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols)\n",
        "#       ])\n",
        "\n",
        "#       if model is None:\n",
        "#           model = XGBRegressor(\n",
        "#               n_estimators=100,\n",
        "#               max_depth=6,\n",
        "#               learning_rate=0.1,\n",
        "#               random_state=42,\n",
        "#               n_jobs=-1\n",
        "#           )\n",
        "\n",
        "#       pipeline = Pipeline([\n",
        "#           (\"preprocessor\", preprocessor),\n",
        "#           (\"model\", model)\n",
        "#       ])\n",
        "\n",
        "#       mlflow.log_params({\"num_cols\": numeric_cols, \"cat_cols\": categorical_cols,\n",
        "#                         \"simple_imputer\": \"mean\", \"cat_encoder\": \"one_hot\", \"pipeline\": pipeline})\n",
        "\n",
        "#       return pipeline\n",
        "\n",
        "# # ----------------------\n",
        "# # Runner\n",
        "# # ----------------------\n",
        "# def run():\n",
        "#     # with mlflow.start_run(run_name=\"XGBoost_Training\"):\n",
        "\n",
        "#       df = load_data()\n",
        "#       df = preprocess(df)\n",
        "\n",
        "#       # Define features and target\n",
        "#       X = df.drop(columns=[\"Weekly_Sales\"])\n",
        "#       y = df[\"Weekly_Sales\"]\n",
        "\n",
        "#       # Split\n",
        "#       X_train, X_test, y_train, y_test = train_test_split(\n",
        "#           X, y, test_size=0.2, random_state=42\n",
        "#       )\n",
        "\n",
        "#       # Store IsHoliday for WMAE\n",
        "#       is_holiday_test = X_test[\"IsHoliday\"].astype(bool).values if \"IsHoliday\" in X_test.columns else np.zeros_like(y_test)\n",
        "\n",
        "#       # Train\n",
        "#       pipeline = build_pipeline(X_train)\n",
        "\n",
        "#       # Hyperparameter search space\n",
        "#       param_dist = {\n",
        "#           \"model__n_estimators\": [100, 200, 300],\n",
        "#           \"model__max_depth\": [3, 6, 9],\n",
        "#           \"model__learning_rate\": [0.05, 0.1, 0.2],\n",
        "#           \"model__subsample\": [0.5, 0.7, 1.0],\n",
        "#           \"model__colsample_bytree\": [0.5, 0.7, 1.0]\n",
        "#       }\n",
        "\n",
        "#       search = RandomizedSearchCV(\n",
        "#           pipeline,\n",
        "#           param_distributions=param_dist,\n",
        "#           n_iter=20,\n",
        "#           cv=3,\n",
        "#           scoring=\"neg_mean_absolute_error\",  # you could also define custom scorer using WMAE if needed\n",
        "#           verbose=2,\n",
        "#           n_jobs=-1,\n",
        "#           random_state=42\n",
        "#       )\n",
        "\n",
        "#       search.fit(X_train, y_train)\n",
        "\n",
        "#       best_model = search.best_estimator_\n",
        "#       y_pred = best_model.predict(X_test)\n",
        "\n",
        "#       # WMAE\n",
        "#       wmae_score = weighted_mae(y_test.values, y_pred, is_holiday_test)\n",
        "#       print(f\"✅ WMAE: {wmae_score:.2f}\")\n",
        "\n",
        "#       mlflow.log_params(search.best_params_)\n",
        "#       mlflow.log_metric(\"WMAE\", wmae_score)\n",
        "#       # mlflow.xgboost.log_model(best_model, artifact_path=\"xgb_model\")\n",
        "#       joblib.dump(best_model.named_steps[\"model\"], \"xgb_model.pkl\")\n",
        "#       mlflow.log_artifact(\"xgb_model.pkl\")\n",
        "\n",
        "#       joblib.dump(best_model, \"xgb_pipeline.pkl\")\n",
        "#       mlflow.log_artifact(\"xgb_pipeline.pkl\")\n",
        "\n",
        "#       # Save feature importance plot as artifact\n",
        "#       fig, ax = plt.subplots()\n",
        "#       xgb_model = best_model.named_steps[\"model\"]\n",
        "#       importances = xgb_model.feature_importances_\n",
        "#       print(importances)\n",
        "#       ax.bar(range(len(importances)), importances)\n",
        "#       plt.savefig(\"feature_importance.png\")\n",
        "#       mlflow.log_artifact(\"feature_importance.png\")\n",
        "\n",
        "\n",
        "#       return best_model\n",
        "\n",
        "# # ----------------------\n",
        "# # Entry Point\n",
        "# # ----------------------\n",
        "# if __name__ == \"__main__\":\n",
        "#     try:\n",
        "#         with mlflow.start_run(run_name=\"XGBoost_Training\"):\n",
        "#             model = run()\n",
        "#         print(\"✅ Pipeline executed successfully!\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"❌ Pipeline failed: {e}\")\n"
      ],
      "metadata": {
        "id": "mxCeUqv0os8B",
        "collapsed": true
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import mlflow\n",
        "# import mlflow.sklearn\n",
        "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.base import BaseEstimator, TransformerMixin\n",
        "# from xgboost import XGBRegressor\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "# import matplotlib.pyplot as plt\n",
        "# import joblib\n",
        "# import warnings\n",
        "# from google.colab import files\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "# # =====================================================\n",
        "# # BASE TRANSFORMER CLASS\n",
        "# # =====================================================\n",
        "# class BaseWalmartTransformer(BaseEstimator, TransformerMixin):\n",
        "#     \"\"\"Base class for all Walmart transformers\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         self.is_fitted = False\n",
        "\n",
        "#     def fit(self, X, y=None):\n",
        "#         \"\"\"Fit the transformer\"\"\"\n",
        "#         self.is_fitted = True\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         \"\"\"Transform the data\"\"\"\n",
        "#         if not self.is_fitted:\n",
        "#             raise ValueError(\"Transformer not fitted yet. Call fit() first.\")\n",
        "#         return X\n",
        "\n",
        "#     def fit_transform(self, X, y=None):\n",
        "#         \"\"\"Fit and transform in one step\"\"\"\n",
        "#         return self.fit(X, y).transform(X)\n",
        "\n",
        "# # =====================================================\n",
        "# # DATA LOADER CLASS\n",
        "# # =====================================================\n",
        "# class WalmartDataLoader:\n",
        "#     \"\"\"Handles loading and merging of Walmart datasets\"\"\"\n",
        "\n",
        "#     def __init__(self, log_mlflow=True):\n",
        "#         self.log_mlflow = log_mlflow\n",
        "#         self.data_info = {}\n",
        "\n",
        "#     def load_data(self, train_path='train.csv', test_path='test.csv',\n",
        "#                   stores_path='stores.csv', features_path='features.csv'):\n",
        "#         \"\"\"Load and merge all datasets\"\"\"\n",
        "\n",
        "#         if self.log_mlflow:\n",
        "#             mlflow.start_run(run_name=\"Data_Loading\", nested=True)\n",
        "\n",
        "#         try:\n",
        "#             # Load datasets\n",
        "#             train_df = pd.read_csv(train_path)\n",
        "#             test_df = pd.read_csv(test_path)\n",
        "#             stores_df = pd.read_csv(stores_path)\n",
        "#             features_df = pd.read_csv(features_path)\n",
        "\n",
        "#             # Merge training data\n",
        "#             merged_train = train_df.merge(features_df, on=['Store', 'Date'], how='inner') \\\n",
        "#                                   .merge(stores_df, on='Store', how='inner')\n",
        "\n",
        "#             # Store data info\n",
        "#             self.data_info = {\n",
        "#                 \"train_shape\": train_df.shape,\n",
        "#                 \"test_shape\": test_df.shape,\n",
        "#                 \"merged_train_shape\": merged_train.shape,\n",
        "#                 \"missing_values_train\": train_df.isnull().sum().sum(),\n",
        "#                 \"missing_values_test\": test_df.isnull().sum().sum()\n",
        "#             }\n",
        "\n",
        "#             if self.log_mlflow:\n",
        "#                 mlflow.log_params(self.data_info)\n",
        "#                 mlflow.end_run()\n",
        "\n",
        "#             print(f\"Data loaded successfully!\")\n",
        "#             print(f\"Training data shape: {merged_train.shape}\")\n",
        "\n",
        "#             return merged_train, test_df, stores_df, features_df\n",
        "\n",
        "#         except Exception as e:\n",
        "#             if self.log_mlflow:\n",
        "#                 mlflow.end_run()\n",
        "#             raise e\n",
        "\n",
        "# # =====================================================\n",
        "# # DATA CLEANING CLASS\n",
        "# # =====================================================\n",
        "# class WalmartDataCleaner(BaseWalmartTransformer):\n",
        "#     \"\"\"Handles data cleaning and basic preprocessing\"\"\"\n",
        "\n",
        "#     def __init__(self, remove_outliers=True, outlier_quantiles=(0.01, 0.99), log_mlflow=True):\n",
        "#         super().__init__()\n",
        "#         self.remove_outliers = remove_outliers\n",
        "#         self.outlier_quantiles = outlier_quantiles\n",
        "#         self.log_mlflow = log_mlflow\n",
        "#         self.q_low = None\n",
        "#         self.q_high = None\n",
        "#         self.cleaning_stats = {}\n",
        "\n",
        "#     def fit(self, X, y=None):\n",
        "#         \"\"\"Fit the cleaner - compute outlier bounds\"\"\"\n",
        "#         if self.log_mlflow:\n",
        "#             mlflow.start_run(run_name=\"Data_Cleaning_Fit\", nested=True)\n",
        "\n",
        "#         try:\n",
        "#             X = X.copy()\n",
        "\n",
        "#             # Compute outlier bounds if Weekly_Sales exists\n",
        "#             if 'Weekly_Sales' in X.columns and self.remove_outliers:\n",
        "#                 self.q_low = X[\"Weekly_Sales\"].quantile(self.outlier_quantiles[0])\n",
        "#                 self.q_high = X[\"Weekly_Sales\"].quantile(self.outlier_quantiles[1])\n",
        "\n",
        "#                 self.cleaning_stats = {\n",
        "#                     \"outlier_lower_bound\": self.q_low,\n",
        "#                     \"outlier_upper_bound\": self.q_high,\n",
        "#                     \"original_rows\": len(X)\n",
        "#                 }\n",
        "\n",
        "#             if self.log_mlflow:\n",
        "#                 if self.cleaning_stats:\n",
        "#                     mlflow.log_params(self.cleaning_stats)\n",
        "#                 mlflow.end_run()\n",
        "\n",
        "#             return super().fit(X, y)\n",
        "\n",
        "#         except Exception as e:\n",
        "#             if self.log_mlflow:\n",
        "#                 mlflow.end_run()\n",
        "#             raise e\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         \"\"\"Transform the data - clean and preprocess\"\"\"\n",
        "#         super().transform(X)  # Check if fitted\n",
        "\n",
        "#         if self.log_mlflow:\n",
        "#             mlflow.start_run(run_name=\"Data_Cleaning_Transform\", nested=True)\n",
        "\n",
        "#         try:\n",
        "#             X = X.copy()\n",
        "#             original_rows = len(X)\n",
        "\n",
        "#             # Fix IsHoliday column naming issues\n",
        "#             if \"IsHoliday_x\" in X.columns:\n",
        "#                 X[\"IsHoliday\"] = X.pop(\"IsHoliday_x\")\n",
        "#             if \"IsHoliday_y\" in X.columns:\n",
        "#                 X.drop(columns=[\"IsHoliday_y\"], inplace=True)\n",
        "\n",
        "#             # Filter positive sales (only if Weekly_Sales exists)\n",
        "#             if 'Weekly_Sales' in X.columns:\n",
        "#                 X = X[X[\"Weekly_Sales\"] > 0]\n",
        "\n",
        "#                 # Remove outliers if fitted bounds exist\n",
        "#                 if self.remove_outliers and self.q_low is not None and self.q_high is not None:\n",
        "#                     X = X[(X[\"Weekly_Sales\"] >= self.q_low) & (X[\"Weekly_Sales\"] <= self.q_high)]\n",
        "\n",
        "#             # Convert Date to datetime and sort\n",
        "#             if 'Date' in X.columns:\n",
        "#                 X[\"Date\"] = pd.to_datetime(X[\"Date\"])\n",
        "#                 X = X.sort_values(by=\"Date\")\n",
        "\n",
        "#             # Fill missing MarkDown values\n",
        "#             markdown_cols = [col for col in X.columns if \"MarkDown\" in col]\n",
        "#             if markdown_cols:\n",
        "#                 X[markdown_cols] = X[markdown_cols].fillna(0)\n",
        "\n",
        "#             rows_after_cleaning = len(X)\n",
        "\n",
        "#             if self.log_mlflow:\n",
        "#                 mlflow.log_params({\n",
        "#                     \"rows_before_cleaning\": original_rows,\n",
        "#                     \"rows_after_cleaning\": rows_after_cleaning,\n",
        "#                     \"rows_removed\": original_rows - rows_after_cleaning,\n",
        "#                     \"markdown_cols_filled\": len(markdown_cols)\n",
        "#                 })\n",
        "#                 mlflow.end_run()\n",
        "\n",
        "#             print(f\"Data cleaning completed: {original_rows} -> {rows_after_cleaning} rows\")\n",
        "\n",
        "#             return X\n",
        "\n",
        "#         except Exception as e:\n",
        "#             if self.log_mlflow:\n",
        "#                 mlflow.end_run()\n",
        "#             raise e\n",
        "\n",
        "# # =====================================================\n",
        "# # FEATURE ENGINEERING CLASS\n",
        "# # =====================================================\n",
        "# class WalmartFeatureEngineer(BaseWalmartTransformer):\n",
        "#     \"\"\"Handles feature engineering - lag features, rolling stats, date features\"\"\"\n",
        "\n",
        "#     def __init__(self, lag_periods=[1, 2, 4, 52], rolling_windows=[4], log_mlflow=True):\n",
        "#         super().__init__()\n",
        "#         self.lag_periods = lag_periods\n",
        "#         self.rolling_windows = rolling_windows\n",
        "#         self.log_mlflow = log_mlflow\n",
        "#         self.feature_stats = {}\n",
        "\n",
        "#     def fit(self, X, y=None):\n",
        "#         \"\"\"Fit feature engineer - just store parameters\"\"\"\n",
        "#         if self.log_mlflow:\n",
        "#             mlflow.start_run(run_name=\"Feature_Engineering_Fit\", nested=True)\n",
        "\n",
        "#         try:\n",
        "#             self.feature_stats = {\n",
        "#                 \"lag_periods\": self.lag_periods,\n",
        "#                 \"rolling_windows\": self.rolling_windows,\n",
        "#                 \"has_weekly_sales\": 'Weekly_Sales' in X.columns,\n",
        "#                 \"has_date\": 'Date' in X.columns,\n",
        "#                 \"has_store\": 'Store' in X.columns\n",
        "#             }\n",
        "\n",
        "#             if self.log_mlflow:\n",
        "#                 mlflow.log_params(self.feature_stats)\n",
        "#                 mlflow.end_run()\n",
        "\n",
        "#             return super().fit(X, y)\n",
        "\n",
        "#         except Exception as e:\n",
        "#             if self.log_mlflow:\n",
        "#                 mlflow.end_run()\n",
        "#             raise e\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         \"\"\"Transform data - create features\"\"\"\n",
        "#         super().transform(X)\n",
        "\n",
        "#         if self.log_mlflow:\n",
        "#             mlflow.start_run(run_name=\"Feature_Engineering_Transform\", nested=True)\n",
        "\n",
        "#         try:\n",
        "#             X = X.copy()\n",
        "#             original_features = len(X.columns)\n",
        "\n",
        "#             # Create lag features\n",
        "#             if 'Weekly_Sales' in X.columns and 'Store' in X.columns:\n",
        "#                 for lag in self.lag_periods:\n",
        "#                     X[f\"lag_{lag}\"] = X.groupby(\"Store\")[\"Weekly_Sales\"].shift(lag)\n",
        "\n",
        "#                 # Create rolling statistics\n",
        "#                 for window in self.rolling_windows:\n",
        "#                     X[f\"rolling_mean_{window}\"] = X.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(window=window).mean()\n",
        "#                     X[f\"rolling_std_{window}\"] = X.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(window=window).std()\n",
        "\n",
        "#             # Create date features\n",
        "#             if 'Date' in X.columns:\n",
        "#                 X[\"Year\"] = X[\"Date\"].dt.year\n",
        "#                 X[\"Month\"] = X[\"Date\"].dt.month\n",
        "#                 X[\"Week\"] = X[\"Date\"].dt.isocalendar().week\n",
        "#                 X[\"DayOfWeek\"] = X[\"Date\"].dt.dayofweek\n",
        "#                 X[\"IsMonthStart\"] = X[\"Date\"].dt.is_month_start.astype(int)\n",
        "#                 X[\"IsMonthEnd\"] = X[\"Date\"].dt.is_month_end.astype(int)\n",
        "#                 X[\"Quarter\"] = X[\"Date\"].dt.quarter\n",
        "\n",
        "#                 # Drop Date column after feature extraction\n",
        "#                 X.drop(columns=[\"Date\"], inplace=True)\n",
        "\n",
        "#             # Drop rows with NaN values (from lag/rolling features)\n",
        "#             rows_before_dropna = len(X)\n",
        "#             X = X.dropna()\n",
        "#             rows_after_dropna = len(X)\n",
        "\n",
        "#             new_features = len(X.columns)\n",
        "\n",
        "#             if self.log_mlflow:\n",
        "#                 mlflow.log_params({\n",
        "#                     \"original_features\": original_features,\n",
        "#                     \"new_features\": new_features,\n",
        "#                     \"features_added\": new_features - original_features,\n",
        "#                     \"rows_before_dropna\": rows_before_dropna,\n",
        "#                     \"rows_after_dropna\": rows_after_dropna,\n",
        "#                     \"rows_dropped_due_to_nan\": rows_before_dropna - rows_after_dropna\n",
        "#                 })\n",
        "#                 mlflow.end_run()\n",
        "\n",
        "#             print(f\"Feature engineering completed: {original_features} -> {new_features} features\")\n",
        "#             print(f\"Rows after removing NaNs: {rows_before_dropna} -> {rows_after_dropna}\")\n",
        "\n",
        "#             return X\n",
        "\n",
        "#         except Exception as e:\n",
        "#             if self.log_mlflow:\n",
        "#                 mlflow.end_run()\n",
        "#             raise e\n",
        "\n",
        "# # =====================================================\n",
        "# # MODEL PIPELINE CLASS\n",
        "# # =====================================================\n",
        "# class WalmartModelPipeline(BaseWalmartTransformer):\n",
        "#     \"\"\"Handles model preprocessing and training\"\"\"\n",
        "\n",
        "#     def __init__(self, model=None, log_mlflow=True):\n",
        "#         super().__init__()\n",
        "#         self.model = model\n",
        "#         self.log_mlflow = log_mlflow\n",
        "#         self.preprocessor = None\n",
        "#         self.pipeline = None\n",
        "#         self.numeric_cols = None\n",
        "#         self.categorical_cols = None\n",
        "\n",
        "#     def fit(self, X, y=None):\n",
        "#         \"\"\"Fit the preprocessing pipeline\"\"\"\n",
        "#         if self.log_mlflow:\n",
        "#             mlflow.start_run(run_name=\"Model_Pipeline_Fit\", nested=True)\n",
        "\n",
        "#         try:\n",
        "#             # Identify column types\n",
        "#             self.numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "#             self.categorical_cols = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "\n",
        "#             # Create preprocessor\n",
        "#             transformers = []\n",
        "#             if self.numeric_cols:\n",
        "#                 transformers.append((\"num\", SimpleImputer(strategy='mean'), self.numeric_cols))\n",
        "#             if self.categorical_cols:\n",
        "#                 transformers.append((\"cat\", OneHotEncoder(drop='first', handle_unknown='ignore'), self.categorical_cols))\n",
        "\n",
        "#             self.preprocessor = ColumnTransformer(transformers=transformers)\n",
        "\n",
        "#             # Create default model if none provided\n",
        "#             if self.model is None:\n",
        "#                 self.model = XGBRegressor(\n",
        "#                     n_estimators=100,\n",
        "#                     max_depth=6,\n",
        "#                     learning_rate=0.1,\n",
        "#                     random_state=42,\n",
        "#                     n_jobs=-1\n",
        "#                 )\n",
        "\n",
        "#             # Create pipeline\n",
        "#             self.pipeline = Pipeline([\n",
        "#                 (\"preprocessor\", self.preprocessor),\n",
        "#                 (\"model\", self.model)\n",
        "#             ])\n",
        "\n",
        "#             if self.log_mlflow:\n",
        "#                 mlflow.log_params({\n",
        "#                     \"numeric_cols_count\": len(self.numeric_cols),\n",
        "#                     \"categorical_cols_count\": len(self.categorical_cols),\n",
        "#                     \"numeric_cols\": self.numeric_cols[:10],  # Log first 10 to avoid too long\n",
        "#                     \"categorical_cols\": self.categorical_cols,\n",
        "#                     \"model_type\": type(self.model).__name__\n",
        "#                 })\n",
        "#                 mlflow.end_run()\n",
        "\n",
        "#             return super().fit(X, y)\n",
        "\n",
        "#         except Exception as e:\n",
        "#             if self.log_mlflow:\n",
        "#                 mlflow.end_run()\n",
        "#             raise e\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         \"\"\"Transform using the fitted pipeline preprocessor\"\"\"\n",
        "#         super().transform(X)\n",
        "\n",
        "#         if self.pipeline is None:\n",
        "#             raise ValueError(\"Pipeline not fitted yet.\")\n",
        "\n",
        "#         # Only transform using preprocessor, not the full pipeline\n",
        "#         return self.pipeline.named_steps['preprocessor'].transform(X)\n",
        "\n",
        "#     def get_pipeline(self):\n",
        "#         \"\"\"Get the full pipeline for training\"\"\"\n",
        "#         if not self.is_fitted:\n",
        "#             raise ValueError(\"Pipeline not fitted yet.\")\n",
        "#         return self.pipeline\n",
        "\n",
        "# # =====================================================\n",
        "# # EVALUATION METRICS CLASS\n",
        "# # =====================================================\n",
        "# class WalmartEvaluator:\n",
        "#     \"\"\"Handles model evaluation with WMAE and other metrics\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         pass\n",
        "\n",
        "#     def weighted_mae(self, y_true, y_pred, is_holiday):\n",
        "#         \"\"\"Calculate Weighted Mean Absolute Error\"\"\"\n",
        "#         weights = np.where(is_holiday, 5, 1)\n",
        "#         return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "#     def evaluate_model(self, model, X_test, y_test, is_holiday_test=None):\n",
        "#         \"\"\"Evaluate model with multiple metrics\"\"\"\n",
        "#         y_pred = model.predict(X_test)\n",
        "\n",
        "#         # Standard MAE\n",
        "#         mae_score = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "#         # WMAE if holiday info available\n",
        "#         wmae_score = None\n",
        "#         if is_holiday_test is not None:\n",
        "#             wmae_score = self.weighted_mae(y_test, y_pred, is_holiday_test)\n",
        "\n",
        "#         metrics = {\n",
        "#             'mae': mae_score,\n",
        "#             'wmae': wmae_score\n",
        "#         }\n",
        "\n",
        "#         return metrics, y_pred\n",
        "\n",
        "# # =====================================================\n",
        "# # MAIN PIPELINE CLASS\n",
        "# # =====================================================\n",
        "# class WalmartSalesPipeline:\n",
        "#     \"\"\"Main pipeline orchestrating all components\"\"\"\n",
        "\n",
        "#     def __init__(self, log_mlflow=True, experiment_name=\"Walmart_Sales_OOP\"):\n",
        "#         self.log_mlflow = log_mlflow\n",
        "#         self.experiment_name = experiment_name\n",
        "\n",
        "#         # Initialize components\n",
        "#         self.data_loader = WalmartDataLoader(log_mlflow=log_mlflow)\n",
        "#         self.data_cleaner = WalmartDataCleaner(log_mlflow=log_mlflow)\n",
        "#         self.feature_engineer = WalmartFeatureEngineer(log_mlflow=log_mlflow)\n",
        "#         self.model_pipeline = WalmartModelPipeline(log_mlflow=log_mlflow)\n",
        "#         self.evaluator = WalmartEvaluator()\n",
        "\n",
        "#         # Pipeline state\n",
        "#         self.is_fitted = False\n",
        "#         self.best_model = None\n",
        "#         self.best_params = None\n",
        "#         self.training_data = None\n",
        "\n",
        "#         if self.log_mlflow:\n",
        "#             mlflow.set_experiment(self.experiment_name)\n",
        "\n",
        "#     def fit(self, train_path='train.csv', test_path='test.csv',\n",
        "#             stores_path='stores.csv', features_path='features.csv',\n",
        "#             hyperparameter_search=True, cv_folds=3, n_iter=20):\n",
        "#         \"\"\"Fit the entire pipeline\"\"\"\n",
        "\n",
        "#         if self.log_mlflow:\n",
        "#             mlflow.start_run(run_name=\"Walmart_Pipeline_Training\")\n",
        "\n",
        "#         try:\n",
        "#             print(\"=\"*60)\n",
        "#             print(\"WALMART SALES PIPELINE - TRAINING\")\n",
        "#             print(\"=\"*60)\n",
        "\n",
        "#             # 1. Load data\n",
        "#             print(\"\\n1. Loading data...\")\n",
        "#             train_data, test_data, stores_data, features_data = self.data_loader.load_data(\n",
        "#                 train_path, test_path, stores_path, features_path\n",
        "#             )\n",
        "\n",
        "#             # 2. Clean data\n",
        "#             print(\"\\n2. Cleaning data...\")\n",
        "#             self.data_cleaner.fit(train_data)\n",
        "#             train_cleaned = self.data_cleaner.transform(train_data)\n",
        "\n",
        "#             # 3. Engineer features\n",
        "#             print(\"\\n3. Engineering features...\")\n",
        "#             self.feature_engineer.fit(train_cleaned)\n",
        "#             train_processed = self.feature_engineer.transform(train_cleaned)\n",
        "\n",
        "#             # Store processed data\n",
        "#             self.training_data = train_processed.copy()\n",
        "\n",
        "#             # 4. Prepare features and target\n",
        "#             print(\"\\n4. Preparing features and target...\")\n",
        "#             X = train_processed.drop(columns=[\"Weekly_Sales\"])\n",
        "#             y = train_processed[\"Weekly_Sales\"]\n",
        "\n",
        "#             # 5. Split data\n",
        "#             print(\"\\n5. Splitting data...\")\n",
        "#             X_train, X_test, y_train, y_test = train_test_split(\n",
        "#                 X, y, test_size=0.2, random_state=42\n",
        "#             )\n",
        "\n",
        "#             # Extract IsHoliday for WMAE\n",
        "#             is_holiday_test = X_test[\"IsHoliday\"].astype(bool).values if \"IsHoliday\" in X_test.columns else np.zeros_like(y_test, dtype=bool)\n",
        "\n",
        "#             # 6. Fit model pipeline\n",
        "#             print(\"\\n6. Fitting model pipeline...\")\n",
        "#             self.model_pipeline.fit(X_train, y_train)\n",
        "#             pipeline = self.model_pipeline.get_pipeline()\n",
        "\n",
        "#             # 7. Hyperparameter tuning\n",
        "#             if hyperparameter_search:\n",
        "#                 print(\"\\n7. Hyperparameter tuning...\")\n",
        "#                 param_dist = {\n",
        "#                     \"model__n_estimators\": [100, 200, 300],\n",
        "#                     \"model__max_depth\": [3, 6, 9],\n",
        "#                     \"model__learning_rate\": [0.05, 0.1, 0.2],\n",
        "#                     \"model__subsample\": [0.5, 0.7, 1.0],\n",
        "#                     \"model__colsample_bytree\": [0.5, 0.7, 1.0]\n",
        "#                 }\n",
        "\n",
        "#                 search = RandomizedSearchCV(\n",
        "#                     pipeline,\n",
        "#                     param_distributions=param_dist,\n",
        "#                     n_iter=n_iter,\n",
        "#                     cv=cv_folds,\n",
        "#                     scoring=\"neg_mean_absolute_error\",\n",
        "#                     verbose=1,\n",
        "#                     n_jobs=-1,\n",
        "#                     random_state=42\n",
        "#                 )\n",
        "\n",
        "#                 search.fit(X_train, y_train)\n",
        "#                 self.best_model = search.best_estimator_\n",
        "#                 self.best_params = search.best_params_\n",
        "\n",
        "#                 if self.log_mlflow:\n",
        "#                     mlflow.log_params(self.best_params)\n",
        "\n",
        "#             else:\n",
        "#                 print(\"\\n7. Training model without hyperparameter search...\")\n",
        "#                 pipeline.fit(X_train, y_train)\n",
        "#                 self.best_model = pipeline\n",
        "\n",
        "#             # 8. Evaluate model\n",
        "#             print(\"\\n8. Evaluating model...\")\n",
        "#             metrics, y_pred = self.evaluator.evaluate_model(\n",
        "#                 self.best_model, X_test, y_test, is_holiday_test\n",
        "#             )\n",
        "\n",
        "#             print(f\"✅ MAE: {metrics['mae']:.2f}\")\n",
        "#             if metrics['wmae'] is not None:\n",
        "#                 print(f\"✅ WMAE: {metrics['wmae']:.2f}\")\n",
        "\n",
        "#             if self.log_mlflow:\n",
        "#                 mlflow.log_metrics(metrics)\n",
        "\n",
        "#             # 9. Save artifacts\n",
        "#             print(\"\\n9. Saving model artifacts...\")\n",
        "#             self._save_artifacts()\n",
        "\n",
        "#             self.is_fitted = True\n",
        "\n",
        "#             print(\"\\n\" + \"=\"*60)\n",
        "#             print(\"✅ PIPELINE TRAINING COMPLETED!\")\n",
        "#             print(\"=\"*60)\n",
        "\n",
        "#             if self.log_mlflow:\n",
        "#                 mlflow.end_run()\n",
        "\n",
        "#             return self.best_model, metrics\n",
        "\n",
        "#         except Exception as e:\n",
        "#             if self.log_mlflow:\n",
        "#                 mlflow.end_run()\n",
        "#             raise e\n",
        "\n",
        "#     def predict(self, new_data):\n",
        "#         \"\"\"Make predictions on new data\"\"\"\n",
        "#         if not self.is_fitted:\n",
        "#             raise ValueError(\"Pipeline not fitted yet. Call fit() first.\")\n",
        "\n",
        "#         test = pd.read_csv('test.csv')\n",
        "\n",
        "#         # Apply same preprocessing steps\n",
        "#         cleaned_data = self.data_cleaner.transform(new_data)\n",
        "#         processed_data = self.feature_engineer.transform(cleaned_data)\n",
        "\n",
        "#         # Remove target column if it exists\n",
        "#         if 'Weekly_Sales' in processed_data.columns:\n",
        "#             processed_data = processed_data.drop(columns=['Weekly_Sales'])\n",
        "\n",
        "#         # Make predictions\n",
        "#         predictions = self.best_model.predict(processed_data)\n",
        "\n",
        "#         # Predict weekly sales\n",
        "#         y_pred = self.best_model.predict(test)\n",
        "\n",
        "#         test[\"Date\"] = test[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "#         submission = test[[\"Id\"]].copy()\n",
        "#         submission[\"Weekly_Sales\"] = y_pred\n",
        "\n",
        "#         submission.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "#         files.download('submission.csv')\n",
        "\n",
        "#         return predictions\n",
        "\n",
        "#     def _save_artifacts(self):\n",
        "#         \"\"\"Save model artifacts\"\"\"\n",
        "#         try:\n",
        "#             # Save pipeline\n",
        "#             joblib.dump(self.best_model, \"walmart_pipeline.pkl\")\n",
        "#             if self.log_mlflow:\n",
        "#                 mlflow.log_artifact(\"walmart_pipeline.pkl\")\n",
        "\n",
        "#             # Save individual components\n",
        "#             joblib.dump(self.data_cleaner, \"data_cleaner.pkl\")\n",
        "#             joblib.dump(self.feature_engineer, \"feature_engineer.pkl\")\n",
        "#             joblib.dump(self.model_pipeline, \"model_pipeline.pkl\")\n",
        "\n",
        "#             if self.log_mlflow:\n",
        "#                 mlflow.log_artifact(\"data_cleaner.pkl\")\n",
        "#                 mlflow.log_artifact(\"feature_engineer.pkl\")\n",
        "#                 mlflow.log_artifact(\"model_pipeline.pkl\")\n",
        "\n",
        "#             # Feature importance plot\n",
        "#             if hasattr(self.best_model.named_steps[\"model\"], 'feature_importances_'):\n",
        "#                 fig, ax = plt.subplots(figsize=(10, 6))\n",
        "#                 importances = self.best_model.named_steps[\"model\"].feature_importances_\n",
        "#                 indices = np.argsort(importances)[::-1][:20]  # Top 20 features\n",
        "\n",
        "#                 ax.bar(range(len(indices)), importances[indices])\n",
        "#                 ax.set_title(\"Top 20 Feature Importances\")\n",
        "#                 ax.set_xlabel(\"Feature Index\")\n",
        "#                 ax.set_ylabel(\"Importance\")\n",
        "\n",
        "#                 plt.tight_layout()\n",
        "#                 plt.savefig(\"feature_importance.png\")\n",
        "\n",
        "#                 if self.log_mlflow:\n",
        "#                     mlflow.log_artifact(\"feature_importance.png\")\n",
        "\n",
        "#                 plt.close()\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"Warning: Could not save some artifacts: {e}\")\n",
        "\n",
        "# # =====================================================\n",
        "# # USAGE EXAMPLE\n",
        "# # =====================================================\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Initialize and run pipeline\n",
        "#     pipeline = WalmartSalesPipeline(\n",
        "#         log_mlflow=True,\n",
        "#         experiment_name=\"Walmart_Sales_OOP_Pipeline\"\n",
        "#     )\n",
        "\n",
        "#     try:\n",
        "#         # Train the pipeline\n",
        "#         best_model, metrics = pipeline.fit(\n",
        "#             hyperparameter_search=True,\n",
        "#             cv_folds=3,\n",
        "#             n_iter=10  # Reduced for faster testing\n",
        "#         )\n",
        "\n",
        "#         print(f\"\\n🎉 Training completed successfully!\")\n",
        "#         print(f\"Best parameters: {pipeline.best_params}\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"❌ Pipeline failed: {e}\")\n",
        "#         import traceback\n",
        "#         traceback.print_exc()"
      ],
      "metadata": {
        "id": "ZeHzBTnO0wQl",
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import joblib\n",
        "# import mlflow\n",
        "\n",
        "# model_uri = \"runs:/6d5792d20dbf4993b5753eb467020430/mlflow-artifacts:/e0cbe91d54c64e1d839ca8a3c7668be3/6d5792d20dbf4993b5753eb467020430/artifacts/model_pipeline.pkl\"  # e.g., runs:/1234abcd/xgb_model\n",
        "# model = mlflow.sklearn.load_model(model_uri)\n",
        "\n",
        "\n",
        "# # Load your trained model\n",
        "# # model = joblib.load(\"model.pkl\")  # Or use the model directly if already in memory\n",
        "\n",
        "# # Make predictions on test set\n",
        "# preds = model.predict(test)\n",
        "\n",
        "# # Construct the Id column — assumes you have these columns in your test set\n",
        "# X_test_with_id = test.copy()\n",
        "# X_test_with_id[\"Id\"] = X_test_with_id[\"Store\"].astype(str) + \"_\" + \\\n",
        "#                        X_test_with_id[\"Dept\"].astype(str) + \"_\" + \\\n",
        "#                        X_test_with_id[\"Date\"].astype(str)\n",
        "\n",
        "# # Build submission DataFrame\n",
        "# submission = pd.DataFrame({\n",
        "#     \"Id\": X_test_with_id[\"Id\"],\n",
        "#     \"Weekly_Sales\": preds\n",
        "# })\n",
        "\n",
        "# # Save to CSV\n",
        "# submission.to_csv(\"submission.csv\", index=False)\n",
        "# print(\"Submission file saved to 'submission.csv'\")\n"
      ],
      "metadata": {
        "id": "ljFoVofx9UKP",
        "collapsed": true
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "stores = pd.read_csv('stores.csv')\n",
        "features = pd.read_csv('features.csv')"
      ],
      "metadata": {
        "id": "Dn2hbAa6rcaX"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class BaseMerger(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, features_df, stores_df):\n",
        "        self.features_df = features_df.copy()\n",
        "        self.stores_df = stores_df.copy()\n",
        "        self.features_df[\"Date\"] = pd.to_datetime(self.features_df[\"Date\"])\n",
        "        self.stores_df[\"Store\"] = self.stores_df[\"Store\"].astype(int)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[\"Date\"] = pd.to_datetime(X[\"Date\"])\n",
        "        X[\"Store\"] = X[\"Store\"].astype(int)\n",
        "        merged = X.merge(self.features_df, on=[\"Store\", \"Date\"], how=\"left\")\n",
        "        merged = merged.merge(self.stores_df, on=\"Store\", how=\"left\")\n",
        "        return merged\n",
        "\n",
        "class FeatureAdder(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, is_train=True):\n",
        "        self.is_train = is_train\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[\"Date\"] = pd.to_datetime(X[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "        if self.is_train and \"Weekly_Sales\" in X.columns:\n",
        "            for lag in [1, 2, 4, 52]:\n",
        "                X[f\"lag_{lag}\"] = X.groupby(\"Store\")[\"Weekly_Sales\"].shift(lag)\n",
        "            X[\"rolling_mean_4\"] = X.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(4).mean()\n",
        "            X[\"rolling_std_4\"] = X.groupby(\"Store\")[\"Weekly_Sales\"].shift(1).rolling(4).std()\n",
        "\n",
        "        markdown_cols = [col for col in X.columns if \"MarkDown\" in col]\n",
        "        X[markdown_cols] = X[markdown_cols].fillna(0)\n",
        "\n",
        "        X[\"Year\"] = X[\"Date\"].dt.year\n",
        "        X[\"Month\"] = X[\"Date\"].dt.month\n",
        "        X[\"Week\"] = X[\"Date\"].dt.isocalendar().week\n",
        "        X[\"DayOfWeek\"] = X[\"Date\"].dt.dayofweek\n",
        "        X[\"IsMonthStart\"] = X[\"Date\"].dt.is_month_start.astype(int)\n",
        "        X[\"IsMonthEnd\"] = X[\"Date\"].dt.is_month_end.astype(int)\n",
        "        X[\"Quarter\"] = X[\"Date\"].dt.quarter\n",
        "\n",
        "        # ⚠️ Don't drop Date here — it's needed for aggregations!\n",
        "        return X\n",
        "\n",
        "class MissingValueFiller(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, is_train=True):\n",
        "        self.q_low = None\n",
        "        self.q_high = None\n",
        "        self.is_train = is_train\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if self.is_train and \"Weekly_Sales\" in X.columns:\n",
        "            self.q_low = X[\"Weekly_Sales\"].quantile(0.01)\n",
        "            self.q_high = X[\"Weekly_Sales\"].quantile(0.99)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[\"Date\"] = pd.to_datetime(X[\"Date\"])\n",
        "\n",
        "        if self.is_train and \"Weekly_Sales\" in X.columns:\n",
        "            X = X[X[\"Weekly_Sales\"] > 0]\n",
        "            if self.q_low is not None and self.q_high is not None:\n",
        "                X = X[(X[\"Weekly_Sales\"] >= self.q_low) & (X[\"Weekly_Sales\"] <= self.q_high)]\n",
        "\n",
        "        return X\n",
        "\n",
        "\n",
        "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.fill_values = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "        for col in numeric_cols:\n",
        "            if X[col].isnull().any():\n",
        "                self.fill_values[col] = X[col].mean()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        for col, fill_value in self.fill_values.items():\n",
        "            if col in X.columns:\n",
        "                X[col] = X[col].fillna(fill_value)\n",
        "\n",
        "        # if \"IsHoliday\" in X.columns:\n",
        "        #     X[\"IsHoliday\"] = X[\"IsHoliday\"].astype(int)\n",
        "\n",
        "        # X = X.dropna()  # Now okay, since lags are filled\n",
        "\n",
        "        if \"IsHoliday\" in X.columns:\n",
        "            X[\"IsHoliday\"] = X[\"IsHoliday\"].fillna(0).astype(int)  # Fill missing with 0 (not holiday)\n",
        "\n",
        "        if \"Type\" in X.columns:\n",
        "            X[\"Type\"] = X[\"Type\"].map({\"A\": 3, \"B\": 2, \"C\": 1}).fillna(0)\n",
        "\n",
        "        # Fill missing values for numeric columns as before\n",
        "        for col, fill_value in self.fill_values.items():\n",
        "            if col in X.columns:\n",
        "                X[col] = X[col].fillna(fill_value)\n",
        "\n",
        "        # Now drop rows only if they still have missing values in important columns (exclude 'IsHoliday')\n",
        "        X = X.dropna(subset=[col for col in X.columns if col != \"IsHoliday\"])\n",
        "\n",
        "        return X\n"
      ],
      "metadata": {
        "id": "qjHnaV5vC1Sh"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is your train.csv with 'Store' and 'Weekly_Sales'\n",
        "train_dict = {}\n",
        "for store_id, group in train.groupby(\"Store\"):\n",
        "    # You can define custom proportions here if needed\n",
        "    train_dict[store_id] = (group.copy(), 1.0)\n"
      ],
      "metadata": {
        "id": "ccRxmwvAYGcZ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import make_scorer, mean_absolute_percentage_error"
      ],
      "metadata": {
        "id": "1vTaw_H1Xzg7"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # def train_store_models_xgb(train_weeks):\n",
        "# #     mape_array = []\n",
        "# #     store_models = {}\n",
        "\n",
        "# #     for store in train_dict.keys():\n",
        "# #         df, prop = train_dict[store]\n",
        "\n",
        "# #         # Pipeline\n",
        "# #         df = BaseMerger(features, stores).fit_transform(df)\n",
        "# #         df = FeatureAdder(is_train=True).fit_transform(df)\n",
        "# #         df = MissingValueFiller(is_train=True).fit_transform(df)\n",
        "# #         df = CategoricalEncoder().fit_transform(df)\n",
        "\n",
        "# #         df = df.sort_values(\"Date\")\n",
        "# #         train_df = df.iloc[:train_weeks]\n",
        "# #         val_df = df.iloc[train_weeks:]\n",
        "\n",
        "# #         X_train = train_df.drop(columns=[\"Weekly_Sales\", \"Date\"])\n",
        "# #         y_train = train_df[\"Weekly_Sales\"]\n",
        "\n",
        "# #         X_val = val_df.drop(columns=[\"Weekly_Sales\", \"Date\"])\n",
        "# #         y_val = val_df[\"Weekly_Sales\"]\n",
        "\n",
        "# #         model = XGBRegressor(n_estimators=300, learning_rate=0.1, max_depth=9)\n",
        "# #         model.fit(X_train, y_train)\n",
        "\n",
        "# #         preds = model.predict(X_val)\n",
        "# #         mape = np.mean(np.abs((y_val - preds) / y_val)) * 100\n",
        "# #         mape_array.append(mape)\n",
        "\n",
        "# #         store_models[store] = (model, prop)\n",
        "\n",
        "# #     return mape_array, store_models\n",
        "\n",
        "\n",
        "# def train_store_models_xgb(train_weeks):\n",
        "#     mape_array = []\n",
        "#     wmae_array = []\n",
        "#     store_models = {}\n",
        "\n",
        "#     for store in train_dict.keys():\n",
        "#         df, prop = train_dict[store]\n",
        "\n",
        "#         # Pipeline\n",
        "#         df = BaseMerger(features, stores).fit_transform(df)\n",
        "#         df = FeatureAdder(is_train=True).fit_transform(df)\n",
        "#         df = MissingValueFiller(is_train=True).fit_transform(df)\n",
        "#         df = CategoricalEncoder().fit_transform(df)\n",
        "\n",
        "#         df = df.sort_values(\"Date\")\n",
        "#         train_df = df.iloc[:train_weeks]\n",
        "#         val_df = df.iloc[train_weeks:]\n",
        "\n",
        "#         X_train = train_df.drop(columns=[\"Weekly_Sales\", \"Date\"])\n",
        "#         y_train = train_df[\"Weekly_Sales\"]\n",
        "\n",
        "#         X_val = val_df.drop(columns=[\"Weekly_Sales\", \"Date\"])\n",
        "#         y_val = val_df[\"Weekly_Sales\"]\n",
        "\n",
        "#         model = XGBRegressor(n_estimators=300, learning_rate=0.1, max_depth=9)\n",
        "#         model.fit(X_train, y_train)\n",
        "\n",
        "#         preds = model.predict(X_val)\n",
        "\n",
        "#         # MAPE\n",
        "#         mape = np.mean(np.abs((y_val - preds) / y_val)) * 100\n",
        "#         mape_array.append(mape)\n",
        "\n",
        "#         # WMAE (Holiday weight = 5, otherwise 1)\n",
        "#         weights = np.where(val_df[\"IsHoliday\"] == True, 5, 1)\n",
        "#         wmae = np.sum(weights * np.abs(y_val - preds)) / np.sum(weights)\n",
        "#         wmae_array.append(wmae)\n",
        "\n",
        "#         store_models[store] = (model, prop)\n",
        "\n",
        "#     return mape_array, wmae_array, store_models\n",
        "\n",
        "\n",
        "import mlflow\n",
        "import mlflow.xgboost\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "def train_store_models_xgb(train_weeks):\n",
        "    mape_array = []\n",
        "    wmae_array = []\n",
        "    store_models = {}\n",
        "\n",
        "    for store in train_dict.keys():\n",
        "        df, prop = train_dict[store]\n",
        "\n",
        "        df = df.sort_values(\"Date\")\n",
        "\n",
        "        # Pipeline\n",
        "        df = BaseMerger(features, stores).fit_transform(df)\n",
        "        df = FeatureAdder(is_train=True).fit_transform(df)\n",
        "        df = MissingValueFiller(is_train=True).fit_transform(df)\n",
        "        df = CategoricalEncoder().fit_transform(df)\n",
        "\n",
        "        train_df = df.iloc[:train_weeks]\n",
        "        val_df = df.iloc[train_weeks:]\n",
        "\n",
        "        X_train = train_df.drop(columns=[\"Weekly_Sales\", \"Date\"])\n",
        "        y_train = train_df[\"Weekly_Sales\"]\n",
        "\n",
        "        # is_holiday_val = val_df[\"IsHoliday\"]  # <-- Extract before dropping\n",
        "\n",
        "        X_val = val_df.drop(columns=[\"Weekly_Sales\", \"Date\"])\n",
        "        y_val = val_df[\"Weekly_Sales\"]\n",
        "\n",
        "        model = XGBRegressor(n_estimators=300, learning_rate=0.1, max_depth=9)\n",
        "\n",
        "        # Start MLflow run\n",
        "        with mlflow.start_run(run_name=f\"store_{store}\"):\n",
        "            mlflow.log_params({\n",
        "                \"store\": store,\n",
        "                \"n_estimators\": 300,\n",
        "                \"learning_rate\": 0.1,\n",
        "                \"max_depth\": 9,\n",
        "            })\n",
        "\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            preds = model.predict(X_val)\n",
        "\n",
        "            # MAPE\n",
        "            mape = np.mean(np.abs((y_val - preds) / y_val)) * 100\n",
        "            mape_array.append(mape)\n",
        "\n",
        "            # WMAE\n",
        "            if \"IsHoliday\" in val_df.columns:\n",
        "                weights = np.where(val_df[\"IsHoliday\"] == 1, 5, 1)\n",
        "                print(weights)\n",
        "            else:\n",
        "                weights = np.ones(len(val_df))  # fallback equal weights\n",
        "\n",
        "            wmae = np.sum(weights * np.abs(y_val - preds)) / np.sum(weights)\n",
        "            wmae_array.append(wmae)\n",
        "\n",
        "            mlflow.log_metrics({\n",
        "                \"MAPE\": mape,\n",
        "                \"WMAE\": wmae\n",
        "            })\n",
        "\n",
        "            # Log model\n",
        "            # mlflow.xgboost.log_model(model, artifact_path=\"model\")\n",
        "\n",
        "            store_models[store] = (model, prop)\n",
        "\n",
        "    return mape_array, wmae_array, store_models\n"
      ],
      "metadata": {
        "id": "C6lqyCjJXOWQ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_weeks = 120\n",
        "mape_array, wmae_array, store_models = train_store_models_xgb(train_weeks)"
      ],
      "metadata": {
        "id": "DHKO6R-8oMq8",
        "outputId": "63bbfc9e-4112-45e2-9f8c-d13a1bdab928",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run store_1 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/db6d716509f64d749191a288001bef7e\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_2 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/c8001f5502c94caf94cbb7e3f015ec5a\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_3 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/8af286fdf5204e98bb5bf353bf6363b4\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_4 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/d8900454884240879d9984cd95b2525c\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_5 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/92f34dda084b405abfd075f9076839ba\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_6 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/0f1a7cb07f064f5883491cf5fe6a9062\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_7 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/e67033464ecf464fa0455bcc3f1207ee\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_8 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/106596d86de5484883cbe343181af875\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_9 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/8ee218243d624e1bb73e6f61773e76cb\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_10 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/30861ea97afa42cbb8bdfe7b9fe610d1\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_11 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/25ddac3188b240ddbe7c286c43d07a4b\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_12 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/376baaccc8b743eea41d432dfa97d386\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_13 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/89d2d0f209284b659ee7c980d84db1d1\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_14 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/ee7aaff22e2d41c8a5bcf8d0e482746b\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_15 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/b79aa6ff48954c309cd9652a20d18d40\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_16 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/0f706ae4f881476692e93ca272fe7c94\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_17 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/da4b2d04b6a1442ea03f50923c899adf\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_18 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/eeb822b22bb644a3b5cdbaa65a47a106\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_19 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/f1521a7be8164eb58dc1133e656362e3\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_20 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/46fea9318f764eb8a31e0ca3e1a508d6\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_21 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/90b54420632f403fbd08f226d26fb4d9\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_22 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/dfdb1c784da84816b106f81f1caf314b\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_23 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/5d6d1eaab2f447088285d890abd24d61\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_24 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/743216fc219a4ae78905c8af345ca068\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_25 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/912f48219ecb48beac6c71a8679e4bd7\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_26 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/51fd599d1fbd4c0893a02c64441c5bfc\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_27 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/a94e9496fab047279c12a683ecd3e3fe\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_28 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/258f4fce772a4425a2eaff85de53fcfc\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_29 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/32ee0bead1564b5cbfc5fba30ace2a6c\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_30 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/3866ea6393c0436a8be5c2d0271950d5\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_31 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/f2c9f68a72994bfe992b38815bb12e76\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_32 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/cc0f9d720fb74d1fa77dc05bbb1442d7\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_33 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/3cf29869cd784fce831aae4dbeebf036\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_34 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/4826717c8d504b2e86fc6aaf8bae582e\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_35 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/76ce0ac9c06e4c7ab8d166abba71b67e\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_36 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/8e2895f6045a4947bfb133c8fbe00d12\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_37 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/b057d12e503c4fbebef5338b2b0bede1\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_38 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/b1df4443465b42ce95fe84cf4b89da3c\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_39 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/b86c2ccef9574f92bb661de903b54bc7\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_40 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/96f8de19bdd44b27a07e32380eb6435a\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_41 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/c4d37de4ba0345659944bd70f5c24168\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_42 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/ccb30b1b615d40f68606665a0bc69686\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_43 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/1da71f9117ba41199c7bf7e18b71ab6d\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_44 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/17ebcd19c31a44359cb0e4782d828251\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n",
            "🏃 View run store_45 at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0/runs/b6ba70b709bd43c88ea6da4486afdf89\n",
            "🧪 View experiment at: https://dagshub.com/ekvirika/WalmartRecruiting.mlflow/#/experiments/0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission = []\n",
        "\n",
        "for store, (model, prop) in store_models.items():\n",
        "    df, _ = train_dict[store]\n",
        "\n",
        "    df = df.sort_values(\"Date\")\n",
        "\n",
        "    # Preprocess test data\n",
        "    df = BaseMerger(features, stores).fit_transform(df)\n",
        "    df = FeatureAdder(is_train=True).fit_transform(df)\n",
        "    df = MissingValueFiller(is_train=True).fit_transform(df)\n",
        "    df = CategoricalEncoder().fit_transform(df)\n",
        "\n",
        "    val_df = df.iloc[train_weeks:]\n",
        "\n",
        "    X_val = val_df.drop(columns=[\"Weekly_Sales\", \"Date\"])\n",
        "    dates = val_df[\"Date\"]\n",
        "\n",
        "    preds = model.predict(X_val)\n",
        "\n",
        "    for i, row in val_df.iterrows():\n",
        "        dept = prop[\"Dept\"] if isinstance(prop, dict) and \"Dept\" in prop else 1\n",
        "        prediction_date = row[\"Date\"]\n",
        "        submission.append({\n",
        "            \"Id\": f\"{store}_{dept}_{prediction_date.strftime('%Y-%m-%d')}\",\n",
        "            \"Weekly_Sales\": preds[i - val_df.index[0]]\n",
        "        })\n",
        "\n",
        "submission_df = pd.DataFrame(submission)\n",
        "submission_df.to_csv(\"submission_xgb.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "XJ1Voyz5Yl8h",
        "outputId": "051b0bfc-391e-4aa9-cb3b-0a4e1719bab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 9918 is out of bounds for axis 0 with size 9918",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1030519905.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         submission.append({\n\u001b[1;32m     25\u001b[0m             \u001b[0;34m\"Id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{store}_{dept}_{prediction_date.strftime('%Y-%m-%d')}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;34m\"Weekly_Sales\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         })\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 9918 is out of bounds for axis 0 with size 9918"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the real test.csv file\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Process and predict per store as you did for validation\n",
        "for store, (model, prop) in store_models.items():\n",
        "    test_store = test[test[\"Store\"] == store].copy()\n",
        "\n",
        "    test_store = BaseMerger(features, stores).fit_transform(test_store)\n",
        "    test_store = FeatureAdder(is_train=True).fit_transform(test_store)\n",
        "    test_store = MissingValueFiller(is_train=True).fit_transform(test_store)\n",
        "    test_store = CategoricalEncoder().fit_transform(test_store)\n",
        "\n",
        "    X_test = test_store.drop(columns=[\"Date\"])\n",
        "    dates = test_store[\"Date\"]\n",
        "    depts = test_store[\"Dept\"]\n",
        "\n",
        "    preds = model.predict(X_test)\n",
        "\n",
        "    for i in range(len(test_store)):\n",
        "        submission.append({\n",
        "            \"Id\": f\"{store}_{depts.iloc[i]}_{dates.iloc[i].strftime('%Y-%m-%d')}\",\n",
        "            \"Weekly_Sales\": preds[i]\n",
        "        })\n"
      ],
      "metadata": {
        "id": "sR2h4SwO1B8h",
        "outputId": "2cdd5fff-cdcd-4f25-fde8-591effadff3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "feature_names mismatch: ['Store', 'Dept', 'IsHoliday_x', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday_y', 'Type', 'Size', 'lag_1', 'lag_2', 'lag_4', 'lag_52', 'rolling_mean_4', 'rolling_std_4', 'Year', 'Month', 'Week', 'DayOfWeek', 'IsMonthStart', 'IsMonthEnd', 'Quarter'] ['Store', 'Dept', 'IsHoliday_x', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday_y', 'Type', 'Size', 'Year', 'Month', 'Week', 'DayOfWeek', 'IsMonthStart', 'IsMonthEnd', 'Quarter']\nexpected lag_4, lag_1, lag_2, lag_52, rolling_std_4, rolling_mean_4 in input data",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1800653157.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdepts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_store\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dept\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_store\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_use_inplace_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m                     predts = self.get_booster().inplace_predict(\n\u001b[0m\u001b[1;32m   1328\u001b[0m                         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m                         \u001b[0miteration_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miteration_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_transform_pandas_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_categorical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_validate_features\u001b[0;34m(self, feature_names)\u001b[0m\n\u001b[1;32m   3241\u001b[0m                 )\n\u001b[1;32m   3242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3243\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3245\u001b[0m     def get_split_value_histogram(\n",
            "\u001b[0;31mValueError\u001b[0m: feature_names mismatch: ['Store', 'Dept', 'IsHoliday_x', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday_y', 'Type', 'Size', 'lag_1', 'lag_2', 'lag_4', 'lag_52', 'rolling_mean_4', 'rolling_std_4', 'Year', 'Month', 'Week', 'DayOfWeek', 'IsMonthStart', 'IsMonthEnd', 'Quarter'] ['Store', 'Dept', 'IsHoliday_x', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday_y', 'Type', 'Size', 'Year', 'Month', 'Week', 'DayOfWeek', 'IsMonthStart', 'IsMonthEnd', 'Quarter']\nexpected lag_4, lag_1, lag_2, lag_52, rolling_std_4, rolling_mean_4 in input data"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('submission_xgb.csv')"
      ],
      "metadata": {
        "id": "ZOk5F4OLZ-yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "70PJjO5pxAHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6QMmQHkfAjQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vR4UtJAXAjNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from xgboost import XGBRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import GroupKFold, TimeSeriesSplit\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Enhanced preprocessing classes for XGBoost\n",
        "class BaseMerger(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, features_df, stores_df):\n",
        "        self.features_df = features_df.copy()\n",
        "        self.stores_df = stores_df.copy()\n",
        "        self.features_df[\"Date\"] = pd.to_datetime(self.features_df[\"Date\"])\n",
        "        self.stores_df[\"Store\"] = self.stores_df[\"Store\"].astype(int)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[\"Date\"] = pd.to_datetime(X[\"Date\"])\n",
        "        X[\"Store\"] = X[\"Store\"].astype(int)\n",
        "        merged = X.merge(self.features_df, on=[\"Store\", \"Date\"], how=\"left\")\n",
        "        merged = merged.merge(self.stores_df, on=\"Store\", how=\"left\")\n",
        "        return merged\n",
        "\n",
        "class FeatureAdder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, is_train=True):\n",
        "        self.is_train = is_train\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[\"Date\"] = pd.to_datetime(X[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "        # Department-level lag features (crucial for department predictions)\n",
        "        if self.is_train and \"Weekly_Sales\" in X.columns:\n",
        "            # Lag features grouped by Store and Dept\n",
        "            for lag in [1, 2, 4, 8, 12, 52]:\n",
        "                X[f\"lag_{lag}\"] = X.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(lag)\n",
        "\n",
        "            # Rolling statistics\n",
        "            X[\"rolling_mean_4\"] = X.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(1).rolling(4).mean()\n",
        "            X[\"rolling_std_4\"] = X.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(1).rolling(4).std()\n",
        "            X[\"rolling_mean_12\"] = X.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(1).rolling(12).mean()\n",
        "            X[\"rolling_max_4\"] = X.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(1).rolling(4).max()\n",
        "            X[\"rolling_min_4\"] = X.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(1).rolling(4).min()\n",
        "\n",
        "            # Store-level aggregations (helpful for new departments)\n",
        "            X[\"store_mean_sales\"] = X.groupby([\"Store\", \"Date\"])[\"Weekly_Sales\"].transform(\"mean\")\n",
        "            X[\"store_total_sales\"] = X.groupby([\"Store\", \"Date\"])[\"Weekly_Sales\"].transform(\"sum\")\n",
        "\n",
        "            # Department-level aggregations across stores\n",
        "            X[\"dept_mean_sales\"] = X.groupby([\"Dept\", \"Date\"])[\"Weekly_Sales\"].transform(\"mean\")\n",
        "\n",
        "        # Handle markdown columns\n",
        "        markdown_cols = [col for col in X.columns if \"MarkDown\" in col]\n",
        "        X[markdown_cols] = X[markdown_cols].fillna(0)\n",
        "\n",
        "        # Additional markdown features\n",
        "        if len(markdown_cols) > 0:\n",
        "            X[\"total_markdown\"] = X[markdown_cols].sum(axis=1)\n",
        "            X[\"markdown_count\"] = (X[markdown_cols] > 0).sum(axis=1)\n",
        "\n",
        "        # Enhanced date features\n",
        "        X[\"Year\"] = X[\"Date\"].dt.year\n",
        "        X[\"Month\"] = X[\"Date\"].dt.month\n",
        "        X[\"Week\"] = X[\"Date\"].dt.isocalendar().week\n",
        "        X[\"DayOfWeek\"] = X[\"Date\"].dt.dayofweek\n",
        "        X[\"IsMonthStart\"] = X[\"Date\"].dt.is_month_start.astype(int)\n",
        "        X[\"IsMonthEnd\"] = X[\"Date\"].dt.is_month_end.astype(int)\n",
        "        X[\"Quarter\"] = X[\"Date\"].dt.quarter\n",
        "        X[\"WeekOfMonth\"] = (X[\"Date\"].dt.day - 1) // 7 + 1\n",
        "        X[\"DaysFromYear2010\"] = (X[\"Date\"] - pd.Timestamp('2010-01-01')).dt.days\n",
        "\n",
        "        # Seasonal features\n",
        "        X[\"sin_week\"] = np.sin(2 * np.pi * X[\"Week\"] / 52)\n",
        "        X[\"cos_week\"] = np.cos(2 * np.pi * X[\"Week\"] / 52)\n",
        "        X[\"sin_month\"] = np.sin(2 * np.pi * X[\"Month\"] / 12)\n",
        "        X[\"cos_month\"] = np.cos(2 * np.pi * X[\"Month\"] / 12)\n",
        "\n",
        "        return X\n",
        "\n",
        "class MissingValueFiller(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, is_train=True):\n",
        "        self.q_low = None\n",
        "        self.q_high = None\n",
        "        self.is_train = is_train\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if self.is_train and \"Weekly_Sales\" in X.columns:\n",
        "            self.q_low = X[\"Weekly_Sales\"].quantile(0.005)  # More conservative outlier removal\n",
        "            self.q_high = X[\"Weekly_Sales\"].quantile(0.995)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[\"Date\"] = pd.to_datetime(X[\"Date\"])\n",
        "\n",
        "        if self.is_train and \"Weekly_Sales\" in X.columns:\n",
        "            # Remove negative sales\n",
        "            X = X[X[\"Weekly_Sales\"] > 0]\n",
        "            # Remove extreme outliers\n",
        "            if self.q_low is not None and self.q_high is not None:\n",
        "                X = X[(X[\"Weekly_Sales\"] >= self.q_low) & (X[\"Weekly_Sales\"] <= self.q_high)]\n",
        "\n",
        "        return X\n",
        "\n",
        "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.fill_values = {}\n",
        "        self.store_means = {}\n",
        "        self.dept_means = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Calculate mean fill values\n",
        "        numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
        "        for col in numeric_cols:\n",
        "            if X[col].isnull().any():\n",
        "                self.fill_values[col] = X[col].mean()\n",
        "\n",
        "        # Calculate store and department level means for better imputation\n",
        "        if \"Weekly_Sales\" in X.columns:\n",
        "            self.store_means = X.groupby(\"Store\")[\"Weekly_Sales\"].mean().to_dict()\n",
        "            self.dept_means = X.groupby(\"Dept\")[\"Weekly_Sales\"].mean().to_dict()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Handle IsHoliday\n",
        "        if \"IsHoliday\" in X.columns:\n",
        "            X[\"IsHoliday\"] = X[\"IsHoliday\"].fillna(0).astype(int)\n",
        "\n",
        "        # Handle Type with better encoding\n",
        "        if \"Type\" in X.columns:\n",
        "            X[\"Type\"] = X[\"Type\"].map({\"A\": 3, \"B\": 2, \"C\": 1}).fillna(0)\n",
        "\n",
        "        # Fill missing values for numeric columns\n",
        "        for col, fill_value in self.fill_values.items():\n",
        "            if col in X.columns:\n",
        "                X[col] = X[col].fillna(fill_value)\n",
        "\n",
        "        # Smarter imputation for lag features using store/dept means\n",
        "        lag_cols = [col for col in X.columns if 'lag_' in col or 'rolling_' in col]\n",
        "        for col in lag_cols:\n",
        "            if col in X.columns and X[col].isnull().any():\n",
        "                # Fill with store-specific means where possible\n",
        "                for store in X['Store'].unique():\n",
        "                    if store in self.store_means:\n",
        "                        mask = (X['Store'] == store) & X[col].isnull()\n",
        "                        X.loc[mask, col] = self.store_means[store]\n",
        "\n",
        "        # Final cleanup - drop rows with remaining missing values\n",
        "        important_cols = [col for col in X.columns if col not in [\"IsHoliday\", \"Date\"]]\n",
        "        X = X.dropna(subset=important_cols)\n",
        "\n",
        "        return X\n",
        "\n",
        "# XGBoost Model Wrapper for Store-Department combinations\n",
        "class WalmartXGBoostModel:\n",
        "    def __init__(self, store_id, dept_id, params=None):\n",
        "        self.store_id = store_id\n",
        "        self.dept_id = dept_id\n",
        "        self.model = None\n",
        "        self.feature_cols = None\n",
        "\n",
        "        # Default XGBoost parameters optimized for retail sales\n",
        "        self.params = {\n",
        "            'n_estimators': 500,\n",
        "            'learning_rate': 0.1,\n",
        "            'max_depth': 6,\n",
        "            'min_child_weight': 3,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'gamma': 0.1,\n",
        "            'reg_alpha': 0.1,\n",
        "            'reg_lambda': 1.0,\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1,\n",
        "            'verbosity': 0\n",
        "        }\n",
        "\n",
        "        if params:\n",
        "            self.params.update(params)\n",
        "\n",
        "    def fit(self, X, y, eval_set=None, early_stopping_rounds=50):\n",
        "        \"\"\"\n",
        "        Fit XGBoost model\n",
        "        \"\"\"\n",
        "        if len(X) < 10:  # Need minimum data points\n",
        "            return None\n",
        "\n",
        "        # Store feature columns\n",
        "        self.feature_cols = [col for col in X.columns if col not in ['Date', 'Weekly_Sales']]\n",
        "\n",
        "        # Initialize XGBoost\n",
        "        self.model = XGBRegressor(**self.params)\n",
        "\n",
        "        try:\n",
        "            # Fit model\n",
        "            if eval_set is not None:\n",
        "                self.model.fit(\n",
        "                    X[self.feature_cols], y,\n",
        "                    eval_set=eval_set,\n",
        "                    early_stopping_rounds=early_stopping_rounds,\n",
        "                    verbose=False\n",
        "                )\n",
        "            else:\n",
        "                self.model.fit(X[self.feature_cols], y)\n",
        "\n",
        "            return self\n",
        "        except Exception as e:\n",
        "            print(f\"Error fitting Store {self.store_id}, Dept {self.dept_id}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions using XGBoost model\n",
        "        \"\"\"\n",
        "        if self.model is None or self.feature_cols is None:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            predictions = self.model.predict(X[self.feature_cols])\n",
        "            return np.maximum(predictions, 0)  # Ensure non-negative\n",
        "        except Exception as e:\n",
        "            print(f\"Error predicting Store {self.store_id}, Dept {self.dept_id}: {str(e)}\")\n",
        "            return np.zeros(len(X))\n",
        "\n",
        "    def get_feature_importance(self):\n",
        "        \"\"\"\n",
        "        Get feature importance from trained model\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            return None\n",
        "\n",
        "        importance = self.model.feature_importances_\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': self.feature_cols,\n",
        "            'importance': importance\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        return feature_importance\n",
        "\n",
        "# Function to create store-department training data\n",
        "def create_store_dept_data(train_df):\n",
        "    \"\"\"\n",
        "    Create store-department level training data\n",
        "    \"\"\"\n",
        "    store_dept_data = {}\n",
        "    total_rows = len(train_df)\n",
        "\n",
        "    for store_id in train_df['Store'].unique():\n",
        "        store_data = train_df[train_df['Store'] == store_id].copy()\n",
        "        store_dept_data[store_id] = {}\n",
        "\n",
        "        for dept_id in store_data['Dept'].unique():\n",
        "            dept_data = store_data[store_data['Dept'] == dept_id].copy()\n",
        "            dept_data = dept_data.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "            proportion = len(dept_data) / total_rows\n",
        "            store_dept_data[store_id][dept_id] = (dept_data, proportion)\n",
        "\n",
        "        print(f\"Store {store_id}: {len(store_data['Dept'].unique())} departments, {len(store_data)} total records\")\n",
        "\n",
        "    return store_dept_data\n",
        "\n",
        "# Training function with cross-validation\n",
        "def train_xgboost_models_with_cv(store_dept_data, features, stores, cv_folds=3):\n",
        "    \"\"\"\n",
        "    Train XGBoost models with cross-validation for each store-department combination\n",
        "    \"\"\"\n",
        "    models = {}\n",
        "    cv_scores = []\n",
        "    successful_models = 0\n",
        "    total_combinations = 0\n",
        "\n",
        "    for store_id in store_dept_data.keys():\n",
        "        print(f\"\\n=== Training Store {store_id} ===\")\n",
        "        models[store_id] = {}\n",
        "\n",
        "        for dept_id in store_dept_data[store_id].keys():\n",
        "            total_combinations += 1\n",
        "            df, proportion = store_dept_data[store_id][dept_id]\n",
        "\n",
        "            print(f\"Training Store {store_id}, Dept {dept_id} ({len(df)} records)\")\n",
        "\n",
        "            try:\n",
        "                # Apply preprocessing pipeline\n",
        "                df = BaseMerger(features, stores).fit_transform(df)\n",
        "                df = FeatureAdder(is_train=True).fit_transform(df)\n",
        "                df = MissingValueFiller(is_train=True).fit_transform(df)\n",
        "                df = CategoricalEncoder().fit_transform(df)\n",
        "\n",
        "                # Sort by date\n",
        "                df = df.sort_values(\"Date\").reset_index(drop=True)\n",
        "\n",
        "                if len(df) < 20:  # Need minimum data for reliable training\n",
        "                    print(f\"Skipping Store {store_id}, Dept {dept_id} - insufficient data ({len(df)} records)\")\n",
        "                    continue\n",
        "\n",
        "                # Prepare features and target\n",
        "                feature_cols = [col for col in df.columns if col not in ['Date', 'Weekly_Sales']]\n",
        "                X = df[feature_cols]\n",
        "                y = df['Weekly_Sales']\n",
        "\n",
        "                # Time series cross-validation\n",
        "                tscv = TimeSeriesSplit(n_splits=min(cv_folds, len(df)//10))\n",
        "                fold_scores = []\n",
        "\n",
        "                best_model = None\n",
        "                best_score = float('inf')\n",
        "\n",
        "                for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
        "                    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "                    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "                    # Train model\n",
        "                    model = WalmartXGBoostModel(store_id, dept_id)\n",
        "                    fitted_model = model.fit(\n",
        "                        X_train, y_train,\n",
        "                        eval_set=[(X_val[model.feature_cols], y_val)],\n",
        "                        early_stopping_rounds=25\n",
        "                    )\n",
        "\n",
        "                    if fitted_model is not None:\n",
        "                        # Validate\n",
        "                        val_pred = fitted_model.predict(X_val)\n",
        "                        if val_pred is not None:\n",
        "                            # WMAE scoring (competition metric)\n",
        "                            is_holiday = X_val['IsHoliday'].values if 'IsHoliday' in X_val.columns else np.zeros(len(X_val))\n",
        "                            weights = np.where(is_holiday == 1, 5, 1)\n",
        "                            wmae = np.sum(weights * np.abs(y_val - val_pred)) / np.sum(weights)\n",
        "                            fold_scores.append(wmae)\n",
        "\n",
        "                            if wmae < best_score:\n",
        "                                best_score = wmae\n",
        "                                best_model = fitted_model\n",
        "\n",
        "                if best_model is not None and fold_scores:\n",
        "                    avg_score = np.mean(fold_scores)\n",
        "                    cv_scores.append(avg_score)\n",
        "\n",
        "                    # Retrain on full data\n",
        "                    final_model = WalmartXGBoostModel(store_id, dept_id)\n",
        "                    final_fitted = final_model.fit(X, y)\n",
        "\n",
        "                    if final_fitted is not None:\n",
        "                        models[store_id][dept_id] = {\n",
        "                            'model': final_fitted,\n",
        "                            'cv_score': avg_score,\n",
        "                            'proportion': proportion,\n",
        "                            'training_size': len(df)\n",
        "                        }\n",
        "                        successful_models += 1\n",
        "                        print(f\"✓ Store {store_id}, Dept {dept_id} - CV WMAE: {avg_score:.2f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Error training Store {store_id}, Dept {dept_id}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    print(f\"\\n=== Training Summary ===\")\n",
        "    print(f\"Successfully trained: {successful_models}/{total_combinations} models\")\n",
        "    if cv_scores:\n",
        "        print(f\"Average CV WMAE: {np.mean(cv_scores):.2f} (+/- {np.std(cv_scores):.2f})\")\n",
        "\n",
        "    return models, cv_scores\n",
        "\n",
        "# Generate predictions for test set\n",
        "def generate_xgboost_submission(models, test_df, features, stores):\n",
        "    \"\"\"\n",
        "    Generate predictions for test set in Walmart competition format\n",
        "    \"\"\"\n",
        "    print(\"Generating test predictions...\")\n",
        "\n",
        "    # Apply preprocessing to test data\n",
        "    test_processed = test_df.copy()\n",
        "    test_processed = BaseMerger(features, stores).fit_transform(test_processed)\n",
        "    test_processed = FeatureAdder(is_train=False).fit_transform(test_processed)\n",
        "    test_processed = CategoricalEncoder().fit_transform(test_processed)\n",
        "\n",
        "    submission_data = []\n",
        "    predictions_made = 0\n",
        "    fallback_predictions = 0\n",
        "\n",
        "    # Group by store and department\n",
        "    for (store_id, dept_id), group in test_processed.groupby(['Store', 'Dept']):\n",
        "        group = group.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "        # Check if we have a trained model\n",
        "        if (store_id in models and dept_id in models[store_id]):\n",
        "            model_info = models[store_id][dept_id]\n",
        "            model = model_info['model']\n",
        "\n",
        "            # Get predictions\n",
        "            predictions = model.predict(group)\n",
        "\n",
        "            if predictions is not None:\n",
        "                predictions_made += len(predictions)\n",
        "            else:\n",
        "                predictions = np.full(len(group), 1000)  # Fallback\n",
        "                fallback_predictions += len(predictions)\n",
        "        else:\n",
        "            # Fallback prediction strategy\n",
        "            # Use department average from similar stores or overall average\n",
        "            predictions = np.full(len(group), 1000)  # Simple fallback\n",
        "            fallback_predictions += len(predictions)\n",
        "\n",
        "        # Create submission entries\n",
        "        for i, (_, row) in enumerate(group.iterrows()):\n",
        "            submission_id = f\"{int(row['Store'])}_{int(row['Dept'])}_{row['Date'].strftime('%Y-%m-%d')}\"\n",
        "            submission_data.append({\n",
        "                'Id': submission_id,\n",
        "                'Weekly_Sales': max(0, predictions[i])  # Ensure non-negative\n",
        "            })\n",
        "\n",
        "    print(f\"Model predictions: {predictions_made}\")\n",
        "    print(f\"Fallback predictions: {fallback_predictions}\")\n",
        "    print(f\"Total predictions: {len(submission_data)}\")\n",
        "\n",
        "    return pd.DataFrame(submission_data)\n",
        "\n",
        "# Feature importance analysis\n",
        "def analyze_feature_importance(models, top_n=20):\n",
        "    \"\"\"\n",
        "    Analyze feature importance across all models\n",
        "    \"\"\"\n",
        "    all_importances = []\n",
        "\n",
        "    for store_id in models.keys():\n",
        "        for dept_id in models[store_id].keys():\n",
        "            model_info = models[store_id][dept_id]\n",
        "            importance = model_info['model'].get_feature_importance()\n",
        "\n",
        "            if importance is not None:\n",
        "                importance['store'] = store_id\n",
        "                importance['dept'] = dept_id\n",
        "                all_importances.append(importance)\n",
        "\n",
        "    if all_importances:\n",
        "        combined_importance = pd.concat(all_importances, ignore_index=True)\n",
        "\n",
        "        # Calculate average importance across all models\n",
        "        avg_importance = combined_importance.groupby('feature')['importance'].agg(['mean', 'std', 'count']).reset_index()\n",
        "        avg_importance = avg_importance.sort_values('mean', ascending=False)\n",
        "\n",
        "        print(f\"\\nTop {top_n} Most Important Features:\")\n",
        "        print(avg_importance.head(top_n))\n",
        "\n",
        "        # Plot feature importance\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        top_features = avg_importance.head(top_n)\n",
        "        plt.barh(range(len(top_features)), top_features['mean'], xerr=top_features['std'])\n",
        "        plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "        plt.xlabel('Average Feature Importance')\n",
        "        plt.title(f'Top {top_n} Feature Importances Across All Models')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return avg_importance\n",
        "\n",
        "    return None\n",
        "\n",
        "# Complete pipeline\n",
        "def run_walmart_xgboost_pipeline(train_df, features, stores, test_df, cv_folds=3):\n",
        "    \"\"\"\n",
        "    Complete XGBoost pipeline for Walmart competition\n",
        "    \"\"\"\n",
        "    print(\"Starting Walmart XGBoost Pipeline...\")\n",
        "\n",
        "    # Step 1: Create store-department data\n",
        "    print(\"\\nStep 1: Creating store-department training data...\")\n",
        "    store_dept_data = create_store_dept_data(train_df)\n",
        "\n",
        "    # Step 2: Train models with cross-validation\n",
        "    print(\"\\nStep 2: Training XGBoost models with cross-validation...\")\n",
        "    models, cv_scores = train_xgboost_models_with_cv(\n",
        "        store_dept_data, features, stores, cv_folds\n",
        "    )\n",
        "\n",
        "    # Step 3: Analyze feature importance\n",
        "    print(\"\\nStep 3: Analyzing feature importance...\")\n",
        "    feature_importance = analyze_feature_importance(models)\n",
        "\n",
        "    # Step 4: Generate test predictions\n",
        "    print(\"\\nStep 4: Generating test predictions...\")\n",
        "    submission_df = generate_xgboost_submission(models, test_df, features, stores)\n",
        "\n",
        "    return models, submission_df, feature_importance\n",
        "\n",
        "models, submission_df, feature_importance = run_walmart_xgboost_pipeline(\n",
        "    train_df=train,\n",
        "    features=features,\n",
        "    stores=stores,\n",
        "    test_df=test,\n",
        "    cv_folds=3\n",
        ")\n",
        "\n",
        "# Save submission\n",
        "submission_df.to_csv('walmart_xgboost_submission.csv', index=False)\n",
        "print(f\"Submission saved! Shape: {submission_df.shape}\")\n",
        "print(\"Sample predictions:\")\n",
        "print(submission_df.head(10))"
      ],
      "metadata": {
        "id": "ACehkmeyAjLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('walmart_xgboost_submission.csv')"
      ],
      "metadata": {
        "id": "PoAqazbcAq5l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}