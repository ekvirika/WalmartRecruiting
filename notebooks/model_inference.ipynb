{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47e2c16",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Model Inference - Walmart Store Sales Forecasting\n",
    "# =================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import wandb\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================\n",
    "# Load Best Model from Model Registry\n",
    "# ============================================================\n",
    "\n",
    "def load_best_model_from_registry():\n",
    "    \"\"\"Load the best model from wandb model registry\"\"\"\n",
    "    \n",
    "    # Initialize wandb\n",
    "    run = wandb.init(\n",
    "        project=\"Walmart Recruiting - Store Sales Forecasting\",\n",
    "        job_type=\"inference\"\n",
    "    )\n",
    "    \n",
    "    # Download model artifact\n",
    "    artifact = run.use_artifact('nbeats_final_model:latest', type='model')\n",
    "    artifact_dir = artifact.download()\n",
    "    \n",
    "    # Load the model\n",
    "    model_path = f\"{artifact_dir}/nbeats_final_model.pkl\"\n",
    "    model = joblib.load(model_path)\n",
    "    \n",
    "    print(f\"Model loaded from: {model_path}\")\n",
    "    return model\n",
    "\n",
    "def load_local_model(model_path='nbeats_final_model.pkl'):\n",
    "    \"\"\"Load model from local file (fallback option)\"\"\"\n",
    "    model = joblib.load(model_path)\n",
    "    print(f\"Model loaded from: {model_path}\")\n",
    "    return model\n",
    "\n",
    "# ============================================================\n",
    "# Data Preprocessing for Test Set\n",
    "# ============================================================\n",
    "\n",
    "class TestDataProcessor:\n",
    "    \"\"\"Process test data for inference\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def load_test_data(self, stores_path, features_path, test_path):\n",
    "        \"\"\"Load test data files\"\"\"\n",
    "        self.stores = pd.read_csv(stores_path)\n",
    "        self.features = pd.read_csv(features_path)\n",
    "        self.test = pd.read_csv(test_path)\n",
    "        return self\n",
    "    \n",
    "    def preprocess_test_data(self, merge_features=True, merge_stores=True):\n",
    "        \"\"\"Preprocess test data to match training format\"\"\"\n",
    "        df = self.test.copy()\n",
    "        \n",
    "        # Convert Date to datetime\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        \n",
    "        # Merge with stores data\n",
    "        if merge_stores and hasattr(self, 'stores'):\n",
    "            df = df.merge(self.stores, on='Store', how='left')\n",
    "        \n",
    "        # Merge with features data\n",
    "        if merge_features and hasattr(self, 'features'):\n",
    "            df = df.merge(self.features, on=['Store', 'Date'], how='left')\n",
    "            \n",
    "        # Fill missing values\n",
    "        df = df.fillna(0)\n",
    "        \n",
    "        # Sort by Store, Dept, Date\n",
    "        df = df.sort_values(['Store', 'Dept', 'Date'])\n",
    "        \n",
    "        # Create unique_id for N-BEATS (combination of Store and Dept)\n",
    "        df['unique_id'] = df['Store'].astype(str) + '_' + df['Dept'].astype(str)\n",
    "        \n",
    "        # Rename columns for N-BEATS format\n",
    "        df = df.rename(columns={'Date': 'ds'})\n",
    "        \n",
    "        # Select relevant columns (no 'y' column in test data)\n",
    "        columns_to_keep = ['unique_id', 'ds', 'Id']  # Keep Id for submission\n",
    "        if 'IsHoliday' in df.columns:\n",
    "            columns_to_keep.append('IsHoliday')\n",
    "        \n",
    "        # Add any other features that were used during training\n",
    "        available_cols = df.columns.tolist()\n",
    "        for col in available_cols:\n",
    "            if col not in columns_to_keep and col not in ['Store', 'Dept']:\n",
    "                columns_to_keep.append(col)\n",
    "        \n",
    "        df = df[columns_to_keep]\n",
    "        \n",
    "        self.processed_test_data = df\n",
    "        return df\n",
    "\n",
    "# ============================================================\n",
    "# Prediction Pipeline\n",
    "# ============================================================\n",
    "\n",
    "def create_submission_predictions(model, test_data):\n",
    "    \"\"\"Create predictions for submission\"\"\"\n",
    "    \n",
    "    print(\"Making predictions on test data...\")\n",
    "    \n",
    "    # For N-BEATS, we need historical data to make future predictions\n",
    "    # Since test data contains future dates, we'll need to handle this carefully\n",
    "    \n",
    "    # Get unique store-department combinations\n",
    "    unique_ids = test_data['unique_id'].unique()\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    for unique_id in unique_ids:\n",
    "        # Get data for this store-department combination\n",
    "        subset = test_data[test_data['unique_id'] == unique_id].copy()\n",
    "        \n",
    "        # Sort by date\n",
    "        subset = subset.sort_values('ds')\n",
    "        \n",
    "        try:\n",
    "            # Make predictions using the model\n",
    "            # Note: This assumes the model can handle the prediction format\n",
    "            predictions = model.predict(subset)\n",
    "            \n",
    "            # Create prediction dataframe\n",
    "            pred_df = subset[['Id']].copy()\n",
    "            pred_df['Weekly_Sales'] = predictions\n",
    "            \n",
    "            all_predictions.append(pred_df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting for {unique_id}: {e}\")\n",
    "            # Use fallback prediction (e.g., historical average)\n",
    "            pred_df = subset[['Id']].copy()\n",
    "            pred_df['Weekly_Sales'] = 0  # or some fallback value\n",
    "            all_predictions.append(pred_df)\n",
    "    \n",
    "    # Combine all predictions\n",
    "    final_predictions = pd.concat(all_predictions, ignore_index=True)\n",
    "    \n",
    "    return final_predictions\n",
    "\n",
    "def create_kaggle_submission(predictions, output_file='submission.csv'):\n",
    "    \"\"\"Create Kaggle submission file\"\"\"\n",
    "    \n",
    "    # Ensure we have the required columns\n",
    "    submission = predictions[['Id', 'Weekly_Sales']].copy()\n",
    "    \n",
    "    # Sort by Id\n",
    "    submission = submission.sort_values('Id')\n",
    "    \n",
    "    # Save to CSV\n",
    "    submission.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Submission file saved as: {output_file}\")\n",
    "    print(f\"Submission shape: {submission.shape}\")\n",
    "    print(f\"Sample predictions:\")\n",
    "    print(submission.head())\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# ============================================================\n",
    "# Alternative Prediction Method for N-BEATS\n",
    "# ============================================================\n",
    "\n",
    "def predict_with_nbeats_direct(test_data, train_data_path='data/train.csv'):\n",
    "    \"\"\"\n",
    "    Alternative method: Use N-BEATS with historical data to predict future\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load training data to get historical context\n",
    "    train_data = pd.read_csv(train_data_path)\n",
    "    \n",
    "    # Preprocess training data\n",
    "    train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
    "    train_data['unique_id'] = train_data['Store'].astype(str) + '_' + train_data['Dept'].astype(str)\n",
    "    train_data = train_data.rename(columns={'Date': 'ds', 'Weekly_Sales': 'y'})\n",
    "    \n",
    "    # Get the date range for predictions\n",
    "    test_start_date = test_data['ds'].min()\n",
    "    test_end_date = test_data['ds'].max()\n",
    "    \n",
    "    print(f\"Predicting from {test_start_date} to {test_end_date}\")\n",
    "    \n",
    "    # For each unique store-department combination\n",
    "    unique_ids = test_data['unique_id'].unique()\n",
    "    all_predictions = []\n",
    "    \n",
    "    from neuralforecast import NeuralForecast\n",
    "    from neuralforecast.models import NBEATS\n",
    "    \n",
    "    # Create N-BEATS model with best parameters\n",
    "    model = NBEATS(\n",
    "        max_steps=100,  # Reduced for inference\n",
    "        h=53,  # Prediction horizon\n",
    "        input_size=52,\n",
    "        batch_size=256,\n",
    "        learning_rate=1e-3,\n",
    "        random_seed=42,\n",
    "        enable_progress_bar=False\n",
    "    )\n",
    "    \n",
    "    nf = NeuralForecast(models=[model], freq='W')\n",
    "    \n",
    "    for i, unique_id in enumerate(unique_ids):\n",
    "        print(f\"Processing {unique_id} ({i+1}/{len(unique_ids)})\")\n",
    "        \n",
    "        try:\n",
    "            # Get historical data for this store-department\n",
    "            hist_data = train_data[train_data['unique_id'] == unique_id].copy()\n",
    "            hist_data = hist_data[['unique_id', 'ds', 'y']].sort_values('ds')\n",
    "            \n",
    "            if len(hist_data) < 52:  # Need minimum history\n",
    "                print(f\"Insufficient history for {unique_id}, using fallback\")\n",
    "                test_subset = test_data[test_data['unique_id'] == unique_id]\n",
    "                pred_df = test_subset[['Id']].copy()\n",
    "                pred_df['Weekly_Sales'] = hist_data['y'].mean() if len(hist_data) > 0 else 0\n",
    "                all_predictions.append(pred_df)\n",
    "                continue\n",
    "            \n",
    "            # Fit model on historical data\n",
    "            nf.fit(hist_data)\n",
    "            \n",
    "            # Make predictions\n",
    "            forecasts = nf.predict()\n",
    "            \n",
    "            # Map predictions to test data\n",
    "            test_subset = test_data[test_data['unique_id'] == unique_id].copy()\n",
    "            test_subset = test_subset.sort_values('ds')\n",
    "            \n",
    "            # Take first len(test_subset) predictions\n",
    "            if len(forecasts) >= len(test_subset):\n",
    "                predictions = forecasts['NBEATS'].iloc[:len(test_subset)].values\n",
    "            else:\n",
    "                # Pad with last prediction if needed\n",
    "                predictions = forecasts['NBEATS'].values\n",
    "                predictions = np.pad(predictions, (0, len(test_subset) - len(predictions)), \n",
    "                                   mode='constant', constant_values=predictions[-1])\n",
    "            \n",
    "            pred_df = test_subset[['Id']].copy()\n",
    "            pred_df['Weekly_Sales'] = predictions\n",
    "            all_predictions.append(pred_df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {unique_id}: {e}\")\n",
    "            # Fallback prediction\n",
    "            test_subset = test_data[test_data['unique_id'] == unique_id]\n",
    "            pred_df = test_subset[['Id']].copy()\n",
    "            pred_df['Weekly_Sales'] = 0\n",
    "            all_predictions.append(pred_df)\n",
    "    \n",
    "    # Combine all predictions\n",
    "    final_predictions = pd.concat(all_predictions, ignore_index=True)\n",
    "    return final_predictions\n",
    "\n",
    "# ============================================================\n",
    "# Main Inference Pipeline\n",
    "# ============================================================\n",
    "\n",
    "def main_inference():\n",
    "    \"\"\"Main inference pipeline\"\"\"\n",
    "    \n",
    "    print(\"Starting inference pipeline...\")\n",
    "    \n",
    "    # Load test data\n",
    "    print(\"Loading test data...\")\n",
    "    processor = TestDataProcessor()\n",
    "    processor.load_test_data(\n",
    "        stores_path='data/stores.csv',\n",
    "        features_path='data/features.csv',\n",
    "        test_path='data/test.csv'\n",
    "    )\n",
    "    \n",
    "    # Preprocess test data\n",
    "    test_data = processor.preprocess_test_data()\n",
    "    print(f\"Test data shape: {test_data.shape}\")\n",
    "    \n",
    "    try:\n",
    "        # Method 1: Try to load model from registry\n",
    "        print(\"Attempting to load model from wandb registry...\")\n",
    "        model = load_best_model_from_registry()\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = create_submission_predictions(model, test_data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading from registry: {e}\")\n",
    "        print(\"Using alternative prediction method...\")\n",
    "        \n",
    "        # Method 2: Direct N-BEATS prediction\n",
    "        predictions = predict_with_nbeats_direct(test_data)\n",
    "    \n",
    "    # Create submission file\n",
    "    submission = create_kaggle_submission(predictions)\n",
    "    \n",
    "    # Log submission info\n",
    "    wandb.init(\n",
    "        project=\"Walmart Recruiting - Store Sales Forecasting\",\n",
    "        job_type=\"inference\"\n",
    "    )\n",
    "    \n",
    "    wandb.log({\n",
    "        'submission_size': len(submission),\n",
    "        'prediction_mean': submission['Weekly_Sales'].mean(),\n",
    "        'prediction_std': submission['Weekly_Sales'].std(),\n",
    "        'prediction_min': submission['Weekly_Sales'].min(),\n",
    "        'prediction_max': submission['Weekly_Sales'].max()\n",
    "    })\n",
    "    \n",
    "    # Save submission as wandb artifact\n",
    "    artifact = wandb.Artifact(\n",
    "        name=\"kaggle_submission\",\n",
    "        type=\"prediction\"\n",
    "    )\n",
    "    artifact.add_file(\"submission.csv\")\n",
    "    wandb.log_artifact(artifact)\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    print(\"Inference completed successfully!\")\n",
    "    return submission\n",
    "\n",
    "# ============================================================\n",
    "# Model Performance Analysis\n",
    "# ============================================================\n",
    "\n",
    "def analyze_predictions(submission):\n",
    "    \"\"\"Analyze the predictions for insights\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PREDICTION ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"Total predictions: {len(submission):,}\")\n",
    "    print(f\"Mean prediction: ${submission['Weekly_Sales'].mean():,.2f}\")\n",
    "    print(f\"Median prediction: ${submission['Weekly_Sales'].median():,.2f}\")\n",
    "    print(f\"Std deviation: ${submission['Weekly_Sales'].std():,.2f}\")\n",
    "    print(f\"Min prediction: ${submission['Weekly_Sales'].min():,.2f}\")\n",
    "    print(f\"Max prediction: ${submission['Weekly_Sales'].max():,.2f}\")\n",
    "    \n",
    "    # Check for negative predictions\n",
    "    negative_preds = submission[submission['Weekly_Sales'] < 0]\n",
    "    if len(negative_preds) > 0:\n",
    "        print(f\"\\nWarning: {len(negative_preds)} negative predictions found!\")\n",
    "        print(\"Consider post-processing to ensure non-negative sales\")\n",
    "    \n",
    "    # Distribution analysis\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(submission['Weekly_Sales'], bins=50, alpha=0.7)\n",
    "    plt.title('Distribution of Predictions')\n",
    "    plt.xlabel('Weekly Sales ($)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.boxplot(submission['Weekly_Sales'])\n",
    "    plt.title('Prediction Box Plot')\n",
    "    plt.ylabel('Weekly Sales ($)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return submission\n",
    "\n",
    "def post_process_predictions(submission, min_sales=0, max_sales=None):\n",
    "    \"\"\"Post-process predictions to ensure realistic values\"\"\"\n",
    "    \n",
    "    print(\"Post-processing predictions...\")\n",
    "    \n",
    "    original_predictions = submission.copy()\n",
    "    \n",
    "    # Clip negative values\n",
    "    if min_sales is not None:\n",
    "        clipped_negative = submission['Weekly_Sales'] < min_sales\n",
    "        submission.loc[clipped_negative, 'Weekly_Sales'] = min_sales\n",
    "        print(f\"Clipped {clipped_negative.sum()} predictions to minimum value {min_sales}\")\n",
    "    \n",
    "    # Clip extremely high values if specified\n",
    "    if max_sales is not None:\n",
    "        clipped_high = submission['Weekly_Sales'] > max_sales\n",
    "        submission.loc[clipped_high, 'Weekly_Sales'] = max_sales\n",
    "        print(f\"Clipped {clipped_high.sum()} predictions to maximum value {max_sales}\")\n",
    "    \n",
    "    # Log changes\n",
    "    changes = (original_predictions['Weekly_Sales'] != submission['Weekly_Sales']).sum()\n",
    "    print(f\"Total predictions modified: {changes}\")\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# ============================================================\n",
    "# Ensemble Predictions (if multiple models available)\n",
    "# ============================================================\n",
    "\n",
    "def ensemble_predictions(model_predictions_list, weights=None):\n",
    "    \"\"\"Combine predictions from multiple models\"\"\"\n",
    "    \n",
    "    if weights is None:\n",
    "        weights = [1.0] * len(model_predictions_list)\n",
    "    \n",
    "    if len(weights) != len(model_predictions_list):\n",
    "        raise ValueError(\"Number of weights must match number of prediction sets\")\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights = np.array(weights)\n",
    "    weights = weights / weights.sum()\n",
    "    \n",
    "    # Combine predictions\n",
    "    ensemble_pred = np.zeros_like(model_predictions_list[0]['Weekly_Sales'])\n",
    "    \n",
    "    for i, (predictions, weight) in enumerate(zip(model_predictions_list, weights)):\n",
    "        ensemble_pred += weight * predictions['Weekly_Sales'].values\n",
    "    \n",
    "    # Create final submission\n",
    "    final_submission = model_predictions_list[0][['Id']].copy()\n",
    "    final_submission['Weekly_Sales'] = ensemble_pred\n",
    "    \n",
    "    print(f\"Ensemble created from {len(model_predictions_list)} models\")\n",
    "    print(f\"Weights: {weights}\")\n",
    "    \n",
    "    return final_submission\n",
    "\n",
    "# ============================================================\n",
    "# Validation on Historical Data\n",
    "# ============================================================\n",
    "\n",
    "def validate_on_historical_data(model, validation_period_weeks=8):\n",
    "    \"\"\"Validate model on recent historical data\"\"\"\n",
    "    \n",
    "    print(f\"Validating on last {validation_period_weeks} weeks of historical data...\")\n",
    "    \n",
    "    # Load full training data\n",
    "    train_data = pd.read_csv('data/train.csv')\n",
    "    train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
    "    \n",
    "    # Split into train and validation\n",
    "    max_date = train_data['Date'].max()\n",
    "    validation_start = max_date - pd.Timedelta(weeks=validation_period_weeks)\n",
    "    \n",
    "    val_data = train_data[train_data['Date'] >= validation_start].copy()\n",
    "    hist_data = train_data[train_data['Date'] < validation_start].copy()\n",
    "    \n",
    "    print(f\"Historical data: {len(hist_data):,} records\")\n",
    "    print(f\"Validation data: {len(val_data):,} records\")\n",
    "    \n",
    "    # Prepare validation data for prediction\n",
    "    val_data['unique_id'] = val_data['Store'].astype(str) + '_' + val_data['Dept'].astype(str)\n",
    "    val_data = val_data.rename(columns={'Date': 'ds'})\n",
    "    \n",
    "    # Make predictions (simplified version)\n",
    "    try:\n",
    "        predictions = model.predict(val_data.drop(columns=['Weekly_Sales']))\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "        \n",
    "        mae = mean_absolute_error(val_data['Weekly_Sales'], predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(val_data['Weekly_Sales'], predictions))\n",
    "        \n",
    "        # Calculate WMAE if holiday info available\n",
    "        if 'IsHoliday' in val_data.columns:\n",
    "            weights = np.where(val_data['IsHoliday'], 5, 1)\n",
    "            weighted_mae = np.sum(weights * np.abs(val_data['Weekly_Sales'] - predictions)) / np.sum(weights)\n",
    "            print(f\"Validation WMAE: {weighted_mae:.2f}\")\n",
    "        \n",
    "        print(f\"Validation MAE: {mae:.2f}\")\n",
    "        print(f\"Validation RMSE: {rmse:.2f}\")\n",
    "        \n",
    "        return {\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'wmae': weighted_mae if 'IsHoliday' in val_data.columns else None\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Validation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# ============================================================\n",
    "# Complete Inference Workflow\n",
    "# ============================================================\n",
    "\n",
    "def complete_inference_workflow():\n",
    "    \"\"\"Complete end-to-end inference workflow\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"WALMART SALES FORECASTING - INFERENCE PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Main inference\n",
    "    submission = main_inference()\n",
    "    \n",
    "    # Step 2: Analyze predictions\n",
    "    submission = analyze_predictions(submission)\n",
    "    \n",
    "    # Step 3: Post-process predictions\n",
    "    submission = post_process_predictions(submission, min_sales=0)\n",
    "    \n",
    "    # Step 4: Save final submission\n",
    "    final_submission = create_kaggle_submission(submission, 'final_submission.csv')\n",
    "    \n",
    "    # Step 5: Create summary report\n",
    "    create_inference_report(final_submission)\n",
    "    \n",
    "    return final_submission\n",
    "\n",
    "def create_inference_report(submission):\n",
    "    \"\"\"Create a summary report of the inference process\"\"\"\n",
    "    \n",
    "    report = f\"\"\"\n",
    "# N-BEATS Model Inference Report\n",
    "## Walmart Store Sales Forecasting\n",
    "\n",
    "### Model Information\n",
    "- **Model Type**: N-BEATS (Neural Basis Expansion Analysis for Time Series)\n",
    "- **Framework**: PyTorch + NeuralForecast\n",
    "- **Prediction Horizon**: 53 weeks\n",
    "- **Input Window**: 52 weeks\n",
    "\n",
    "### Prediction Summary\n",
    "- **Total Predictions**: {len(submission):,}\n",
    "- **Mean Weekly Sales**: ${submission['Weekly_Sales'].mean():,.2f}\n",
    "- **Median Weekly Sales**: ${submission['Weekly_Sales'].median():,.2f}\n",
    "- **Standard Deviation**: ${submission['Weekly_Sales'].std():,.2f}\n",
    "- **Min Prediction**: ${submission['Weekly_Sales'].min():,.2f}\n",
    "- **Max Prediction**: ${submission['Weekly_Sales'].max():,.2f}\n",
    "\n",
    "### Data Quality Checks\n",
    "- **Negative Predictions**: {(submission['Weekly_Sales'] < 0).sum()}\n",
    "- **Zero Predictions**: {(submission['Weekly_Sales'] == 0).sum()}\n",
    "- **Missing Values**: {submission['Weekly_Sales'].isnull().sum()}\n",
    "\n",
    "### Files Generated\n",
    "- `final_submission.csv`: Kaggle submission file\n",
    "- `prediction_analysis.png`: Prediction distribution plots\n",
    "- `inference_report.md`: This report\n",
    "\n",
    "### Notes\n",
    "- Predictions are based on historical patterns learned by the N-BEATS model\n",
    "- Model was trained on historical sales data with weekly frequency\n",
    "- Holiday effects are implicitly captured through the training data\n",
    "- Post-processing applied to ensure non-negative sales values\n",
    "\n",
    "Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "    \n",
    "    with open('inference_report.md', 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(\"Inference report saved as: inference_report.md\")\n",
    "\n",
    "# ============================================================\n",
    "# Usage Examples\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run complete inference workflow\n",
    "    final_submission = complete_inference_workflow()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INFERENCE COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Final submission shape: {final_submission.shape}\")\n",
    "    print(\"Files generated:\")\n",
    "    print(\"- final_submission.csv\")\n",
    "    print(\"- prediction_analysis.png\") \n",
    "    print(\"- inference_report.md\")\n",
    "    print(\"\\nUpload 'final_submission.csv' to Kaggle for evaluation.\")\n",
    "\n",
    "# ============================================================\n",
    "# Additional Utility Functions\n",
    "# ============================================================\n",
    "\n",
    "def compare_with_baseline(submission, baseline_file=None):\n",
    "    \"\"\"Compare predictions with a baseline model\"\"\"\n",
    "    \n",
    "    if baseline_file is None:\n",
    "        # Create simple baseline (historical average)\n",
    "        train_data = pd.read_csv('data/train.csv')\n",
    "        baseline_mean = train_data['Weekly_Sales'].mean()\n",
    "        \n",
    "        baseline_submission = submission[['Id']].copy()\n",
    "        baseline_submission['Weekly_Sales'] = baseline_mean\n",
    "        \n",
    "        print(f\"Baseline (historical mean): ${baseline_mean:.2f}\")\n",
    "    else:\n",
    "        baseline_submission = pd.read_csv(baseline_file)\n",
    "    \n",
    "    # Compare predictions\n",
    "    diff = submission['Weekly_Sales'] - baseline_submission['Weekly_Sales']\n",
    "    \n",
    "    print(f\"\\nComparison with Baseline:\")\n",
    "    print(f\"Mean difference: ${diff.mean():.2f}\")\n",
    "    print(f\"Predictions higher than baseline: {(diff > 0).sum():,} ({(diff > 0).mean()*100:.1f}%)\")\n",
    "    print(f\"Predictions lower than baseline: {(diff < 0).sum():,} ({(diff < 0).mean()*100:.1f}%)\")\n",
    "    \n",
    "    return diff\n",
    "\n",
    "def export_predictions_by_store(submission, test_data_path='data/test.csv'):\n",
    "    \"\"\"Export predictions grouped by store for analysis\"\"\"\n",
    "    \n",
    "    # Load test data to get store information\n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    \n",
    "    # Merge with predictions\n",
    "    detailed_predictions = test_data.merge(submission, on='Id', how='left')\n",
    "    \n",
    "    # Group by store\n",
    "    store_summary = detailed_predictions.groupby('Store').agg({\n",
    "        'Weekly_Sales': ['count', 'mean', 'std', 'sum'],\n",
    "        'Dept': 'nunique'\n",
    "    }).round(2)\n",
    "    \n",
    "    store_summary.columns = ['Predictions_Count', 'Mean_Sales', 'Std_Sales', 'Total_Sales', 'Dept_Count']\n",
    "    store_summary = store_summary.reset_index()\n",
    "    \n",
    "    # Save store-level summary\n",
    "    store_summary.to_csv('predictions_by_store.csv', index=False)\n",
    "    print(\"Store-level predictions saved as: predictions_by_store.csv\")\n",
    "    \n",
    "    return store_summary"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
